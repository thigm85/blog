{
  
    
        "post0": {
            "title": "Chapter 2 - Organizing Data with Datasets",
            "content": "Setting up a dataset . It is possible to upload a dataset either privately or publicly. | It is possible to use &quot;Import a GitHub repository&quot; option to import a experimental library not yet available on Kaggle Notebooks. | . Gathering data . Interesting interview from Larxel: On creating datasets: All in all, the process that I recommend starts with setting your purpose, breaking it down into objectives and topics, formulating questions to fulfil these topics, surveying possible sources of data, selecting and gathering, pre-processing, documenting, publishing, maintaining and supporting, and finally, improvement actions. | . | On learning on Kaggle: Absorbing all the knowledge at the end of a competition | Replication of winning solutions in finished competitions | . | . | . Working with datasets . The easiest way to work with Kaggle datasets is by creating a notebook from the dataset webpage. | . Using Kaggle datasets in Google Colab . This section contains a step-by-step to download Kaggle Datasets into Colab. . Download Kaggle API from your Kaggle account. Place it ~/.kaggle/kaggle.json | Create folder Kaggle on your GDrive and upload .json there. | Mount GDrive to your colab from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . | Provide path to .json config . import os # content/gdrive/My Drive/Kaggle is the path where kaggle.json is # present in the Google Drive os.environ[&#39;KAGGLE_CONFIG_DIR&#39;] = &quot;/content/gdrive/My Drive/Kaggle&quot; # change the working directory %cd /content/gdrive/My Drive/Kaggle . | Go to the dataset page and use the copy API command. | | .",
            "url": "https://thigm85.github.io/blog/book/kaggle/data%20science/2022/06/23/kaggle-book-chapter-2.html",
            "relUrl": "/book/kaggle/data%20science/2022/06/23/kaggle-book-chapter-2.html",
            "date": " • Jun 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Chapter 1 - Introducing Kaggle and other data science competition",
            "content": "Kaggle public API docs. | Kaggle API Github repo. | Tip: Interact with others on the discussion forum when enrolled on a competition to share and learn. | Common Task Framework (CTF): Great for advancing state of the art solutions. Well defined metrics and quality data | Competition | Sharing between competitors | Compute-resource availability | . | What can go wrong in a competition: Leakeage from the data: data contain informatio of the target not available in real-time. | Probing from the leaderboard: Use the leaderboard to metric to tune your solution. Example: https://www.kaggle.com/c/dont-overfit-ii/discussion/91766 | . | Overfitting and consequent leaderboard shake-up: cases with huge gap between the training set and the public test set Technique to measure discrepancies between training set and test set: https://www.kaggle.com/code/tunguz/adversarial-ieee/notebook | . | Private sharing | . | Jeremy Howard on how to set you up for success on Kaggle: https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1 | .",
            "url": "https://thigm85.github.io/blog/book/kaggle/data%20science/2022/06/21/kaggle-book-chapter-1.html",
            "relUrl": "/book/kaggle/data%20science/2022/06/21/kaggle-book-chapter-1.html",
            "date": " • Jun 21, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Load tabular data with TensorFlow",
            "content": "Required packages . import pandas as pd import numpy as np import tensorflow as tf . In-memory data - numeric features . Load data with pandas . For small datasets, we can load them into memory using a pandas DataFrame. . abalone_train = pd.read_csv( &quot;https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv&quot;, names=[&quot;Length&quot;, &quot;Diameter&quot;, &quot;Height&quot;, &quot;Whole weight&quot;, &quot;Shucked weight&quot;, &quot;Viscera weight&quot;, &quot;Shell weight&quot;, &quot;Age&quot;]) abalone_train.head() . Length Diameter Height Whole weight Shucked weight Viscera weight Shell weight Age . 0 0.435 | 0.335 | 0.110 | 0.334 | 0.1355 | 0.0775 | 0.0965 | 7 | . 1 0.585 | 0.450 | 0.125 | 0.874 | 0.3545 | 0.2075 | 0.2250 | 6 | . 2 0.655 | 0.510 | 0.160 | 1.092 | 0.3960 | 0.2825 | 0.3700 | 14 | . 3 0.545 | 0.425 | 0.125 | 0.768 | 0.2940 | 0.1495 | 0.2600 | 16 | . 4 0.545 | 0.420 | 0.130 | 0.879 | 0.3740 | 0.1695 | 0.2300 | 13 | . abalone_train.dtypes . Length float64 Diameter float64 Height float64 Whole weight float64 Shucked weight float64 Viscera weight float64 Shell weight float64 Age int64 dtype: object . Separate label and features . abalone_features = abalone_train.copy() abalone_label = abalone_features.pop(&quot;Age&quot;) . abalone_features.head() . Length Diameter Height Whole weight Shucked weight Viscera weight Shell weight . 0 0.435 | 0.335 | 0.110 | 0.334 | 0.1355 | 0.0775 | 0.0965 | . 1 0.585 | 0.450 | 0.125 | 0.874 | 0.3545 | 0.2075 | 0.2250 | . 2 0.655 | 0.510 | 0.160 | 1.092 | 0.3960 | 0.2825 | 0.3700 | . 3 0.545 | 0.425 | 0.125 | 0.768 | 0.2940 | 0.1495 | 0.2600 | . 4 0.545 | 0.420 | 0.130 | 0.879 | 0.3740 | 0.1695 | 0.2300 | . abalone_label.head() . 0 7 1 6 2 14 3 16 4 13 Name: Age, dtype: int64 . Numeric features as numpy array . X and y can be used to fit a model. . X, y = np.array(abalone_features), abalone_label . X . array([[0.435 , 0.335 , 0.11 , ..., 0.1355, 0.0775, 0.0965], [0.585 , 0.45 , 0.125 , ..., 0.3545, 0.2075, 0.225 ], [0.655 , 0.51 , 0.16 , ..., 0.396 , 0.2825, 0.37 ], ..., [0.53 , 0.42 , 0.13 , ..., 0.3745, 0.167 , 0.249 ], [0.395 , 0.315 , 0.105 , ..., 0.1185, 0.091 , 0.1195], [0.45 , 0.355 , 0.12 , ..., 0.1145, 0.0665, 0.16 ]]) . y . 0 7 1 6 2 14 3 16 4 13 .. 3315 15 3316 10 3317 11 3318 16 3319 19 Name: Age, Length: 3320, dtype: int64 . In-memory data - mixed data types . Load data with pandas . titanic = pd.read_csv(&quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;) titanic.head() . survived sex age n_siblings_spouses parch fare class deck embark_town alone . 0 0 | male | 22.0 | 1 | 0 | 7.2500 | Third | unknown | Southampton | n | . 1 1 | female | 38.0 | 1 | 0 | 71.2833 | First | C | Cherbourg | n | . 2 1 | female | 26.0 | 0 | 0 | 7.9250 | Third | unknown | Southampton | y | . 3 1 | female | 35.0 | 1 | 0 | 53.1000 | First | C | Southampton | n | . 4 0 | male | 28.0 | 0 | 0 | 8.4583 | Third | unknown | Queenstown | y | . titanic.dtypes . survived int64 sex object age float64 n_siblings_spouses int64 parch int64 fare float64 class object deck object embark_town object alone object dtype: object . titanic_features = titanic.copy() titanic_label = titanic.pop(&quot;survived&quot;) . Pre-process mixed type data . Create a pre-processing model that can be used as part of a larger model. The pre-processing model can for example concatenate and normalize all numeric features of type float64 and apply one-hot encoding to the categorical features of type object. See for example the pre-processing contained here. . titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat) . Once the pre-processing layer is setup we can use a dict of features to the model as input to the pre-processing layer: . Parse pandas DataFrame to use as input . titanic_features_dict = {name: np.array(value) for name, value in titanic_features.items()} . Show first row of the dict: . {name: value[:1] for name, value in titanic_features_dict.items()} . {&#39;survived&#39;: array([0]), &#39;sex&#39;: array([&#39;male&#39;], dtype=object), &#39;age&#39;: array([22.]), &#39;n_siblings_spouses&#39;: array([1]), &#39;parch&#39;: array([0]), &#39;fare&#39;: array([7.25]), &#39;class&#39;: array([&#39;Third&#39;], dtype=object), &#39;deck&#39;: array([&#39;unknown&#39;], dtype=object), &#39;embark_town&#39;: array([&#39;Southampton&#39;], dtype=object), &#39;alone&#39;: array([&#39;n&#39;], dtype=object)} . tf.data.Dataset from in-memory data through from_tensor_slices . features_ds = tf.data.Dataset.from_tensor_slices(titanic_features_dict) . Check first example: . for example in features_ds: for name, value in example.items(): print(&quot;{}: {}&quot;.format(name, value)) break . survived: 0 sex: b&#39;male&#39; age: 22.0 n_siblings_spouses: 1 parch: 0 fare: 7.25 class: b&#39;Third&#39; deck: b&#39;unknown&#39; embark_town: b&#39;Southampton&#39; alone: b&#39;n&#39; . The from_tensor_slice can handle any structure of nested dictionaries and tuples. . titanic_ds = tf.data.Dataset.from_tensor_slices((titanic_features_dict, titanic_label)) . for feature, label in titanic_ds: for name, value in feature.items(): print(&quot;{}: {}&quot;.format(name, value)) print(&quot;Target: {}&quot;.format(label)) break . survived: 0 sex: b&#39;male&#39; age: 22.0 n_siblings_spouses: 1 parch: 0 fare: 7.25 class: b&#39;Third&#39; deck: b&#39;unknown&#39; embark_town: b&#39;Southampton&#39; alone: b&#39;n&#39; Target: 0 . To train a model using this Dataset, you&#39;ll need to at least shuffle and batch the data. . titanic_batches = titanic_ds.shuffle(len(titanic_label)).batch(32) . titanic_batches can be used in fit functions instead of X and y. . Create tf.data.Dataset from CSV file . Uncompressed file . We can create a tf.data.Dataset directly from a .csv file in case our data does not fit into memory. . titanic_file_path = tf.keras.utils.get_file( &quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot; ) . Download the dataset file: . titanic_file_path . &#39;/Users/tmartins/.keras/datasets/train.csv&#39; . Create a tf.data.Dataset from the .csv file above: . titanic_csv_ds = tf.data.experimental.make_csv_dataset( titanic_file_path, batch_size=5, # Artificially small to make examples easier to show. label_name=&#39;survived&#39;, num_epochs=1, ignore_errors=True ) . Take the first batch of data from the tf.data.Dataset: . for batch, label in titanic_csv_ds.take(1): for key, value in batch.items(): print(f&quot;{key:20s}: {value}&quot;) print() print(f&quot;{&#39;label&#39;:20s}: {label}&quot;) . sex : [b&#39;male&#39; b&#39;female&#39; b&#39;male&#39; b&#39;female&#39; b&#39;female&#39;] age : [43. 36. 50. 9. 44.] n_siblings_spouses : [0 1 1 3 0] parch : [0 0 0 2 0] fare : [ 8.05 17.4 55.9 27.9 27.7208] class : [b&#39;Third&#39; b&#39;Third&#39; b&#39;First&#39; b&#39;Third&#39; b&#39;First&#39;] deck : [b&#39;unknown&#39; b&#39;unknown&#39; b&#39;E&#39; b&#39;unknown&#39; b&#39;B&#39;] embark_town : [b&#39;Southampton&#39; b&#39;Southampton&#39; b&#39;Southampton&#39; b&#39;Southampton&#39; b&#39;Cherbourg&#39;] alone : [b&#39;y&#39; b&#39;n&#39; b&#39;n&#39; b&#39;n&#39; b&#39;y&#39;] label : [0 1 0 0 1] . Compressed file . Download compressed file: . traffic_volume_csv_gz = tf.keras.utils.get_file( &#39;Metro_Interstate_Traffic_Volume.csv.gz&#39;, &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz&quot;, cache_dir=&#39;.&#39;, cache_subdir=&#39;traffic&#39; ) . Setup compression_type argument to `&quot;GZIP&quot;``. . traffic_volume_csv_gz_ds = tf.data.experimental.make_csv_dataset( traffic_volume_csv_gz, batch_size=256, label_name=&#39;traffic_volume&#39;, num_epochs=1, compression_type=&quot;GZIP&quot; ) . Take a peep at the first 5 values of each feature and label of the first batch. . for batch, label in traffic_volume_csv_gz_ds.take(1): for key, value in batch.items(): print(f&quot;{key:20s}: {value[:5]}&quot;) print() print(f&quot;{&#39;label&#39;:20s}: {label[:5]}&quot;) . holiday : [b&#39;None&#39; b&#39;None&#39; b&#39;None&#39; b&#39;None&#39; b&#39;None&#39;] temp : [296.68 294.37 275.68 270.68 275.74] rain_1h : [0. 0. 0. 0. 0.] snow_1h : [0. 0. 0. 0. 0.] clouds_all : [ 0 0 90 75 1] weather_main : [b&#39;Clear&#39; b&#39;Clear&#39; b&#39;Clouds&#39; b&#39;Clouds&#39; b&#39;Clear&#39;] weather_description : [b&#39;Sky is Clear&#39; b&#39;Sky is Clear&#39; b&#39;overcast clouds&#39; b&#39;broken clouds&#39; b&#39;sky is clear&#39;] date_time : [b&#39;2013-08-12 20:00:00&#39; b&#39;2013-08-08 13:00:00&#39; b&#39;2013-04-10 07:00:00&#39; b&#39;2013-01-08 04:00:00&#39; b&#39;2013-10-22 10:00:00&#39;] label : [2805 5296 6812 758 4384] . List of files . Download a list of files: . fonts_zip = tf.keras.utils.get_file( &#39;fonts.zip&#39;, &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00417/fonts.zip&quot;, cache_dir=&#39;.&#39;, cache_subdir=&#39;fonts&#39;, extract=True ) . List files downloaded: . import pathlib font_csvs = sorted(str(p) for p in pathlib.Path(&#39;fonts&#39;).glob(&quot;*.csv&quot;)) font_csvs[:10] . [&#39;fonts/AGENCY.csv&#39;, &#39;fonts/ARIAL.csv&#39;, &#39;fonts/BAITI.csv&#39;, &#39;fonts/BANKGOTHIC.csv&#39;, &#39;fonts/BASKERVILLE.csv&#39;, &#39;fonts/BAUHAUS.csv&#39;, &#39;fonts/BELL.csv&#39;, &#39;fonts/BERLIN.csv&#39;, &#39;fonts/BERNARD.csv&#39;, &#39;fonts/BITSTREAMVERA.csv&#39;] . len(font_csvs) . 153 . Create a tf.data.Dataset from a list of files: . fonts_ds = tf.data.experimental.make_csv_dataset( file_pattern = &quot;fonts/*.csv&quot;, batch_size=10, num_epochs=1, num_parallel_reads=20, shuffle_buffer_size=10000 ) . Print features: . for features in fonts_ds.take(1): for i, (name, value) in enumerate(features.items()): if i&gt;15: break print(f&quot;{name:20s}: {value}&quot;) print(&#39;...&#39;) print(f&quot;[total: {len(features)} features]&quot;) . font : [b&#39;BERNARD&#39; b&#39;JAVANESE&#39; b&#39;ONYX&#39; b&#39;MINGLIU&#39; b&#39;ELEPHANT&#39; b&#39;BANKGOTHIC&#39; b&#39;HANDPRINT&#39; b&#39;COMMERCIALSCRIPT&#39; b&#39;BERNARD&#39; b&#39;HARLOW&#39;] fontVariant : [b&#39;BERNARD MT CONDENSED&#39; b&#39;JAVANESE TEXT&#39; b&#39;ONYX&#39; b&#39;MINGLIU_HKSCS-EXTB&#39; b&#39;ELEPHANT&#39; b&#39;BANKGOTHIC MD BT&#39; b&#39;scanned&#39; b&#39;COMMERCIALSCRIPT BT&#39; b&#39;BERNARD MT CONDENSED&#39; b&#39;HARLOW SOLID ITALIC&#39;] m_label : [176 219 103 195 72 186 54 100 68 97] strength : [0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4] italic : [0 1 1 1 0 0 0 1 0 0] orientation : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] m_top : [33 42 46 21 37 43 0 38 33 54] m_left : [23 32 18 19 23 24 0 24 20 23] originalH : [21 63 48 57 49 17 20 39 53 25] originalW : [21 52 34 44 61 24 20 51 34 30] h : [20 20 20 20 20 20 20 20 20 20] w : [20 20 20 20 20 20 20 20 20 20] r0c0 : [ 1 1 1 1 105 1 1 1 255 1] r0c1 : [ 1 1 1 1 178 86 1 1 255 1] r0c2 : [ 1 1 1 1 255 255 1 1 255 1] r0c3 : [ 1 1 1 1 255 255 1 1 255 1] ... [total: 412 features] .",
            "url": "https://thigm85.github.io/blog/tensorflow/tabular_data/loading%20data/2022/06/14/tensorflow-load-tabular-data.html",
            "relUrl": "/tensorflow/tabular_data/loading%20data/2022/06/14/tensorflow-load-tabular-data.html",
            "date": " • Jun 14, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Chapter 2 - The mathematical building blocks of Neural Networks",
            "content": "%config Completer.use_jedi = False . Required packages . import numpy as np import tensorflow as tf . A first look at a Neural Network . MNIST dataset . Task: classify grayscale images of handwritten digits (28 × 28 pixels) into their 10 categories (0 through 9). . from tensorflow.keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 2s 0us/step 11501568/11490434 [==============================] - 2s 0us/step . Training data: . train_images.shape . (60000, 28, 28) . len(train_labels) . 60000 . train_labels . array([5, 0, 4, ..., 5, 6, 8], dtype=uint8) . Test data: . test_images.shape . (10000, 28, 28) . len(test_labels) . 10000 . test_labels . array([7, 2, 1, ..., 4, 5, 6], dtype=uint8) . Define and compile the model . Define a basic multi-layer network . model = tf.keras.Sequential( [ tf.keras.layers.Dense(512, activation=&quot;relu&quot;), tf.keras.layers.Dense(10, activation=&quot;softmax&quot;) ] ) . Compile the model by specifying the optimization algorithm, the loss function and the metrics to track: . model.compile( optimizer=&quot;rmsprop&quot;, loss=&quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;] ) . Pre-process the data as expected by the model . Transform the features from an array (60000, 28, 28) with values between [0, 255] to a flat array of size (60000, 28 * 28) of values [0,1]. . train_images.shape . (60000, 28, 28) . train_images.dtype . dtype(&#39;uint8&#39;) . train_images = train_images.reshape((60000, 28*28)) train_images = train_images.astype(&quot;float32&quot;) / 255 . train_images.shape . (60000, 784) . train_images.dtype . dtype(&#39;float32&#39;) . test_images.shape . (10000, 28, 28) . test_images.dtype . dtype(&#39;uint8&#39;) . test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype(&quot;float32&quot;) / 255 . test_images.shape . (10000, 784) . test_images.dtype . dtype(&#39;float32&#39;) . Fit the model . model.fit(train_images, train_labels, epochs=20, batch_size=128) . Epoch 1/20 469/469 [==============================] - 2s 4ms/step - loss: 0.2423 - accuracy: 0.9297 Epoch 2/20 469/469 [==============================] - 2s 4ms/step - loss: 0.2323 - accuracy: 0.9331 Epoch 3/20 469/469 [==============================] - 2s 4ms/step - loss: 0.2227 - accuracy: 0.9359 Epoch 4/20 469/469 [==============================] - 2s 4ms/step - loss: 0.2133 - accuracy: 0.9384 Epoch 5/20 469/469 [==============================] - 2s 4ms/step - loss: 0.2044 - accuracy: 0.9410 Epoch 6/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1957 - accuracy: 0.9437 Epoch 7/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1881 - accuracy: 0.9460 Epoch 8/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1805 - accuracy: 0.9483 Epoch 9/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1732 - accuracy: 0.9505 Epoch 10/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1665 - accuracy: 0.9524 Epoch 11/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1603 - accuracy: 0.9538 Epoch 12/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1541 - accuracy: 0.9560 Epoch 13/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1487 - accuracy: 0.9576 Epoch 14/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1433 - accuracy: 0.9593 Epoch 15/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1384 - accuracy: 0.9604 Epoch 16/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1337 - accuracy: 0.9622 Epoch 17/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1290 - accuracy: 0.9634 Epoch 18/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1247 - accuracy: 0.9642 Epoch 19/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1208 - accuracy: 0.9658 Epoch 20/20 469/469 [==============================] - 2s 4ms/step - loss: 0.1173 - accuracy: 0.9669 . &lt;keras.callbacks.History at 0x14a481ee0&gt; . Predict with the model . Select the first 10 images of the test set. . test_digits = test_images[0:10] test_digits.shape . (10, 784) . Compute predictions for the first 10 images: . predictions = model.predict(test_digits) . Class probabilities for the first test images: . predictions[0] . array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32) . Pick the class with the highest probability: . predictions[0].argmax() . 7 . Check what is the true label of the first test image: . test_labels[0] . 7 . Evaluate the model on test data . test_loss, test_acc = model.evaluate(test_images, test_labels) . 313/313 [==============================] - 0s 1ms/step - loss: 18.4538 - accuracy: 0.9514 . print(f&quot;Test accuracy: {test_acc:.4} nTest loss: {test_loss:.6}&quot;) . Test accuracy: 0.9514 Test loss: 18.4538 . Data representation for neural networks: Tensors . Tensors are the basic data structures used in Machine Learning. Tensor are multi-dimmentional arrays. In the context of tensors, a dimensional is also called an axis. . In deep learning, you&#39;ll generally manipulate tensors with ranks 0 to 4, although you may go up to 5 if you process video data. . Scalars (rank-0 tensors, 0D tensor) . x = np.array(12) . x.ndim . 0 . Vectors (rank-1 tensors, 1D tensor) . x = np.array([12, 3, 6, 14, 7]) . x.ndim . 1 . Matrices (rank-2 tensors, 2D tensors) . x = np.array( [ [5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2] ] ) . x.ndim . 2 . Rank-3 and higher rank tensors . If you pack such matrices in a new array, you obtain a rank-3 tensor (or 3D tensor) . x = np.array( [ [ [5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2] ], [ [5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2] ], [ [5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2] ] ] ) . x.ndim . 3 . Tensor key attributes . Number of axis | train_images.ndim . 2 . Shape | train_images.shape . (60000, 784) . Data type | train_images.dtype . dtype(&#39;float32&#39;) . Manipulating tensors in NumPy . from tensorflow.keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() . Slicing arrays. The following three statements are equivalent. It selects 90 images, from indices 10 to 99: . my_slice = train_images[10:100] my_slice.shape . (90, 28, 28) . my_slice = train_images[10:100, :, :] my_slice.shape . (90, 28, 28) . my_slice = train_images[10:100, 0:28, 0:28] my_slice.shape . (90, 28, 28) . Select 14x14 pixels in the bottom-right corner of all images: . my_slice = train_images[:, 14:, 14:] my_slice.shape . (60000, 14, 14) . Select 14x14 pixels from the middle of all images: . my_slice = train_images[:, 7:-7, 7:-7] my_slice.shape . (60000, 14, 14) . Real-world example of data tensors . Vector data: 2D-tensor (number_samples, features) | Timeseries or sequence data: 3D-tensor (number_samples, timesteps, features) | Images: 4D-tensor (number_samples, height, width, channels) | Video: 5D-tensor (number_samples, frames, height, width, channels) | . The gears of neural networks: Tensor operations . Layer as a function . The dense layer with a rectified linear unit activation function . tf.keras.layers.Dense(512, activation=&quot;relu&quot;) . &lt;keras.layers.core.dense.Dense at 0x14a487310&gt; . The layer above can be interepreted as the following function: . output = max(dot(input, W) + b, 0) . Element-wise operations . Element-wise operations are carried out by optimized NumPy code. . Broadcasting . When possible, and if there’s no ambiguity, the smaller tensor will be broadcast to match the shape of the larger tensor. . X = np.random.random((32, 10)) y = np.random.random((10,)) . result = X + y result.shape . (32, 10) . x = np.random.random((64, 3, 32, 10)) y = np.random.random((32, 10)) . z = np.maximum(x, y) z.shape . (64, 3, 32, 10) . Tensor-product (dot-product) . Dot-product between vectors | . The dot-product between two vectors is a scalar. . $$x . y = sum _{i=1}^{n} x_i * y_i$$ . Dot-product between a matrix and a vector. | . The dot-product between a matrix X and a vector y is a vector whose element $i$ is the dot-product of the vector y and the $i$-th row of X. . Higher dimension dot-products | . We can take higher dimension dot-product, as long as the last dimension of the first tensor match the first dimension of the second tensor. . (a, b, c, d) . (d,) -&gt; (a,b,c) . (a, b, c, d) . (d, e) -&gt; (a, b, c, e) . Tensor reshaping . Reshaping a tensor means rearranging its rows and columns to match a target shape. Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor. . x = np.array([[0., 1.], [2., 3.], [4., 5.]]) x.shape . (3, 2) . x = x.reshape((6, 1)) x . array([[0.], [1.], [2.], [3.], [4.], [5.]]) . x = x.reshape((2, 3)) x . array([[0., 1., 2.], [3., 4., 5.]]) . Transpose | . x = np.zeros((300, 20)) x = np.transpose(x) x.shape . (20, 300) . Geometric interpretation of tensor operations . In general, elementary geometric operations such as translation, rotation, scaling and so on can be expressed as tensor operations. . Translation: Translating a 2D object can be implemented as the sum of two vectors. | . $$ begin{bmatrix} text{Horizontal factor} text{Vertical factor} end{bmatrix} + begin{bmatrix} x y end{bmatrix} $$ Rotation: A counterclockwise rotation of a 2D vector by an angle $ theta$ can be achieved via a dot-product with a 2 x 2 matrix. | . $$ begin{bmatrix} cos( theta) &amp; -sin( theta) sin( theta) &amp; cos( theta) end{bmatrix} cdot begin{bmatrix} x y end{bmatrix} $$ Scaling: scaling of a 2D object can also be accomplished by a dot-product. | . $$ begin{bmatrix} factor_x &amp; 0 0 &amp; factor_y end{bmatrix} cdot begin{bmatrix} x y end{bmatrix} $$ Linear transform: A dot-product with an arbitrary matrix implements a linear transform. Rotation and Scaling are examples of linear transforms. | . $$W cdot x$$ . Affine transform: It is a combination of a linear transform and a translation. | . $$W cdot x + b$$ . Importance of activation functions: A sequence of affine transforms is equivalent to an affine transform. So a sequence of Dense layers without activation function would still be equivalent to a single Dense layer. | . We just saw that a tensor operation is equivalent to a geometric transformation. Since a neural network is a series of tensor operations, we can say that a neural network is a very complex geometric transformation in a high-dimensional space, implemented via a series of simple steps. . The engine of neural networks: Gradient-based optimization . Assume our network is represented by output = relu(dot(input, W)+b). W and b are the parameters of the model and are initially randomly initialized. . By training the neural network, we will gradually adapt W and b with the objective of minimizing a loss function between the model prediction y_pred and the observed data y_true. . A training loop involves repeating the following steps until the loss seems sufficiently low: . Draw a batch of training samples, x, and corresponding targets, y_true. | Run the model on x (a step called the forward pass) to obtain predictions, y_pred. | Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true. | Update all weights of the model in a way that slightly reduces the loss on this batch. | Step 4 is carried out by gradient descent, which requires that the loss function be differentiable with respect to the model learnable parameters. . What is a derivative? . Geometric explanation of the derivative of a continuous and smooth function. The derivative represents the local slope of the curve of the function. . Derivative of a tensor operation: The gradient . Gradients are just the generalization of the concept of derivatives to functions that take tensors as inputs. . Assuming a model y = f(W), where W is a tensor with model coefficients, grad(loss, W0) can be interpreted as the tensor describing the direction of steepest ascent of loss_value = f(W) around W0. We can reduce loss_value = f(W) by moving W inn the oposite direction from the gradient: W_1 = W_0 - step * grad(loss, W0). step is a small scaling factor that is needed because the gradient is a local approximation of the curvature of the function. . Stochastic gradient descent . Mini-batch stochastic gradient descent draws random batches of data and apply one step of gradient descent to decrease the loss function by a little bit. The process is repeated until convergence. . There exists variations of SGD, such as momentum SDG that uses not only the current gradient but also previous gradient values. . Chaining derivatives: The backpropagation algorithm . Chain rule exemplified: | . def fghj(x): x1 = j(x) x2 = h(x1) x3 = g(x2) y = f(x3) return y grad(y, x) == grad(y, x3) * grad(x3, x2) * grad(x2, x1) * grad(x1, x) . Automatic differentiation with computation graphs | . A computation graph is a directed acyclic graph of tensor operations. The chain rule says that you can obtain the derivative of a node with the respect of another node by multiplying the derivatives for each edge along the path linking the two nodes. . Backpropagation | . Backpropagation is simply the application of the chain rule to a computation graph. Backpropagation starts with the final loss value and works backward from the top layers to the bottom layers, computing the contribution that each parameter had in the loss value. . TensorFlow gradient tape | . It&#39;s a Python scope that will &quot;record&quot; the tensor operations that run inside it, in the form of a computation graph (sometimes called a “tape”). . import tensorflow as tf # gradient wrt a scalar variable x = tf.Variable(0.) with tf.GradientTape() as tape: y = 2 * x + 3 grad_of_y_wrt_x = tape.gradient(y, x) . grad_of_y_wrt_x . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt; . # gradient wrt a tensor variable x = tf.Variable(tf.random.uniform((2, 2))) with tf.GradientTape() as tape: y = 2 * x + 3 grad_of_y_wrt_x = tape.gradient(y, x) . grad_of_y_wrt_x . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[2., 2.], [2., 2.]], dtype=float32)&gt; . # gradient wrt a list of variables W = tf.Variable(tf.random.uniform((2, 2))) b = tf.Variable(tf.zeros((2,))) x = tf.random.uniform((2, 2)) with tf.GradientTape() as tape: y = tf.matmul(x, W) + b grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b]) . grad_of_y_wrt_W_and_b . [&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1.2138343 , 1.2138343 ], [0.50162864, 0.50162864]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 2.], dtype=float32)&gt;] . Reimplementing our first example from scratch in TensorFlow . A simple dense class . output = activation(dot(W,input)+b) . import tensorflow as tf class NaiveDense: def __init__(self, input_size, output_size, activation): self.activation = activation w_shape = (input_size, output_size) w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1) self.W = tf.Variable(w_initial_value) b_shape = (output_size,) b_initial_value = tf.zeros(b_shape) self.b = tf.Variable(b_initial_value) def __call__(self, inputs): return self.activation(tf.matmul(inputs, self.W) + self.b) @property def weights(self): return [self.W, self.b] . A simple sequential class . class NaiveSequential: def __init__(self, layers): self.layers = layers def __call__(self, inputs): x = inputs for layer in self.layers: x = layer(x) return x @property def weights(self): weights = [] for layer in self.layers: weights += layer.weights return weights . Instantiate the model . model = NaiveSequential([ NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu), NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax) ]) assert len(model.weights) == 4 . A batch generator . import math class BatchGenerator: def __init__(self, images, labels, batch_size=128): assert len(images) == len(labels) self.index = 0 self.images = images self.labels = labels self.batch_size = batch_size self.num_batches = math.ceil(len(images) / batch_size) def next(self): images = self.images[self.index : self.index + self.batch_size] labels = self.labels[self.index : self.index + self.batch_size] self.index += self.batch_size return images, labels . Running one training step . def one_training_step(model, images_batch, labels_batch): with tf.GradientTape() as tape: predictions = model(images_batch) per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy( labels_batch, predictions ) average_loss = tf.reduce_mean(per_sample_losses) gradients = tape.gradient(average_loss, model.weights) update_weights(gradients, model.weights) return average_loss . Manual implementation of the update step: . learning_rate = 1e-3 def update_weights(gradients, weights): for g, w in zip(gradients, weights): w.assign_sub(g * learning_rate) . In practice, you would almost never implement a weight update step like this by hand. Instead, you would use an Optimizer instance from Keras, like this: . from tensorflow.keras import optimizers optimizer = optimizers.SGD(learning_rate=1e-3) def update_weights(gradients, weights): optimizer.apply_gradients(zip(gradients, weights)) . The full training loop . def fit(model, images, labels, epochs, batch_size=128): for epoch_counter in range(epochs): print(f&quot;Epoch {epoch_counter}&quot;) batch_generator = BatchGenerator(images, labels) for batch_counter in range(batch_generator.num_batches): images_batch, labels_batch = batch_generator.next() loss = one_training_step(model, images_batch, labels_batch) if batch_counter % 100 == 0: print(f&quot;loss at batch {batch_counter}: {loss:.2f}&quot;) . from tensorflow.keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() train_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype(&quot;float32&quot;) / 255 test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype(&quot;float32&quot;) / 255 fit(model, train_images, train_labels, epochs=10, batch_size=128) . Epoch 0 loss at batch 0: 3.42 loss at batch 100: 2.25 loss at batch 200: 2.20 loss at batch 300: 2.08 loss at batch 400: 2.19 Epoch 1 loss at batch 0: 1.91 loss at batch 100: 1.89 loss at batch 200: 1.83 loss at batch 300: 1.69 loss at batch 400: 1.79 Epoch 2 loss at batch 0: 1.58 loss at batch 100: 1.59 loss at batch 200: 1.50 loss at batch 300: 1.41 loss at batch 400: 1.47 Epoch 3 loss at batch 0: 1.32 loss at batch 100: 1.34 loss at batch 200: 1.24 loss at batch 300: 1.19 loss at batch 400: 1.24 Epoch 4 loss at batch 0: 1.12 loss at batch 100: 1.16 loss at batch 200: 1.04 loss at batch 300: 1.03 loss at batch 400: 1.08 Epoch 5 loss at batch 0: 0.98 loss at batch 100: 1.02 loss at batch 200: 0.91 loss at batch 300: 0.91 loss at batch 400: 0.96 Epoch 6 loss at batch 0: 0.87 loss at batch 100: 0.91 loss at batch 200: 0.80 loss at batch 300: 0.82 loss at batch 400: 0.88 Epoch 7 loss at batch 0: 0.79 loss at batch 100: 0.83 loss at batch 200: 0.73 loss at batch 300: 0.75 loss at batch 400: 0.81 Epoch 8 loss at batch 0: 0.73 loss at batch 100: 0.76 loss at batch 200: 0.66 loss at batch 300: 0.70 loss at batch 400: 0.76 Epoch 9 loss at batch 0: 0.68 loss at batch 100: 0.70 loss at batch 200: 0.62 loss at batch 300: 0.65 loss at batch 400: 0.72 . Evaluating the model . import numpy as np predictions = model(test_images) predictions = predictions.numpy() predicted_labels = np.argmax(predictions, axis=1) matches = predicted_labels == test_labels print(f&quot;accuracy: {matches.mean():.2f}&quot;) . accuracy: 0.82 .",
            "url": "https://thigm85.github.io/blog/deep_learning/python/tensorflow/book/2022/06/14/dl-for-python-chapter-2.html",
            "relUrl": "/deep_learning/python/tensorflow/book/2022/06/14/dl-for-python-chapter-2.html",
            "date": " • Jun 14, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Do not modify SQLite table while sequentially reading data",
            "content": "I recently created an infinite loop by accident when trying to modify the content of rows that were being sequentially read from a SQLite table. I want to replicate the issue here with a minimal example. . Create and connect to a SQLite database. . import os from sqlite3 import connect db_connection = connect(os.environ[&quot;DB_PATH&quot;]) . Create a test table. . create_table_statement = &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS test( id TEXT NOT NULL PRIMARY KEY, value INTEGER );&quot;&quot;&quot; cursor = db_connection.cursor() cursor.execute(create_table_statement) . &lt;sqlite3.Cursor at 0x10d6ec420&gt; . Check if the table was created. . cursor.execute(&quot;SELECT name FROM sqlite_master WHERE type=&#39;table&#39;;&quot;) assert cursor.fetchall()[0][0] == &quot;test&quot; . Insert sample data. . inser_sql = &quot;INSERT OR REPLACE INTO test (id,value) VALUES (?,?)&quot; data = [(&quot;1&quot;, 1), (&quot;2&quot;, 2), (&quot;3&quot;, 3)] for d in data: cursor.execute(inser_sql, d) db_connection.commit() . Check if the sample data were sucessfully inserted. . from pandas import read_sql_query read_sql_query(&quot;SELECT * FROM test&quot;, db_connection) . id value . 0 1 | 1 | . 1 2 | 2 | . 2 3 | 3 | . Loop from the data. Note that the loop here is finite. . for d in cursor.execute(&quot;SELECT * FROM test&quot;): print(d) . (&#39;1&#39;, 1) (&#39;2&#39;, 2) (&#39;3&#39;, 3) . Modify the table while looping through its data. This leads to an infinite loop and should be avoided. . for idx, d in enumerate(cursor.execute(&quot;SELECT * FROM test&quot;)): new_cursor = db_connection.cursor() new_cursor.execute(inser_sql, (d[0], d[1]+1)) db_connection.commit() print(d) if idx == 7: break . (&#39;1&#39;, 1) (&#39;2&#39;, 2) (&#39;3&#39;, 3) (&#39;1&#39;, 2) (&#39;2&#39;, 3) (&#39;3&#39;, 4) (&#39;1&#39;, 3) (&#39;2&#39;, 4) . Clean up environment . os.remove(os.environ[&quot;DB_PATH&quot;]) .",
            "url": "https://thigm85.github.io/blog/sql/sqlite/2021/12/16/modify-while-reading-sqlite-table-leads-to-infinite-loop.html",
            "relUrl": "/sql/sqlite/2021/12/16/modify-while-reading-sqlite-table-leads-to-infinite-loop.html",
            "date": " • Dec 16, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Send and retrieve encoded media to Vespa",
            "content": "Create and deploy the Vespa app . Create the simplest possible app required by our experiment, containing fields for image and video. . from vespa.package import ApplicationPackage, Field app_package = ApplicationPackage(name=&quot;media&quot;) app_package.schema.add_fields( Field(name=&quot;image&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;]), Field(name=&quot;video&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;]) ) . Locally deploy in a Docker container: . from vespa.deployment import VespaDocker vespa_docker = VespaDocker() app = vespa_docker.deploy(app_package) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. Waiting for application status. Waiting for application status. Finished deployment. . Sample image and video . Sample image: . from PIL import Image image_file_path = &quot;data/2021-12-01-convert-media-to-string/dogs.jpg&quot; image = Image.open(image_file_path) image . Sample video: . from IPython.display import Video video_file_path = &quot;data/2021-12-01-convert-media-to-string/archery.mp4&quot; Video(video_file_path, embed=True) . Your browser does not support the video tag. Send encoded data . Function to encode media data to a string . import uu from io import BytesIO def encode_media_to_string(media_file_path): encoded_media_byte_array = BytesIO() uu.encode(media_file_path, encoded_media_byte_array) return encoded_media_byte_array.getvalue().decode(&quot;ascii&quot;) . Send encoded image and video to the vespa app: . send_response = app.feed_data_point( data_id=0, fields={ &quot;image&quot;: encode_media_to_string(image_file_path), &quot;video&quot;: encode_media_to_string(video_file_path) }, schema=&quot;media&quot; ) send_response.status_code . 200 . Retrieve and decode data . Function to decode string to media. . def decode_string_to_media(value): bytes_string = BytesIO(value.encode(&quot;ascii&quot;)) bytes_media = BytesIO() uu.decode(bytes_string, bytes_media) return bytes_media . Retrieve data point contained encoded image and video data. . get_response = app.get_data(data_id=0, schema=&quot;media&quot;) . Decoded image: . decoded_image = decode_string_to_media(get_response.json[&quot;fields&quot;][&quot;image&quot;]) Image.open(decoded_image) . Decoded video: . decoded_video = decode_string_to_media(get_response.json[&quot;fields&quot;][&quot;video&quot;]) Video(decoded_video.getvalue(), embed=True, mimetype=&quot;video/mp4&quot;) . Your browser does not support the video tag.",
            "url": "https://thigm85.github.io/blog/image%20data/video%20data/encoding/decoding/vespa/2021/12/03/send-retrieve-encoded-media-to-vespa.html",
            "relUrl": "/image%20data/video%20data/encoding/decoding/vespa/2021/12/03/send-retrieve-encoded-media-to-vespa.html",
            "date": " • Dec 3, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Encode and decode images and videos with strings",
            "content": "Encode media to string: . import uu from io import BytesIO def encode_media_to_string(media_file_path): encoded_media_byte_array = BytesIO() uu.encode(media_file_path, encoded_media_byte_array) return encoded_media_byte_array.getvalue().decode(&quot;ascii&quot;) . Decode string to media: . def decode_string_to_media(value): bytes_string = BytesIO(value.encode(&quot;ascii&quot;)) bytes_media = BytesIO() uu.decode(bytes_string, bytes_media) return bytes_media . Image . We can test if the encode/decode functions above work for the image below. . from PIL import Image image_file_path = &quot;data/2021-12-01-convert-media-to-string/dogs.jpg&quot; image = Image.open(image_file_path) image . We are able to recover the same image by decoding the encoded image. . image_text = encode_media_to_string(image_file_path) image_bytes = decode_string_to_media(image_text) Image.open(image_bytes) . Video . Now, we want to check if we can recover the video below. . from IPython.display import Video video_file_path = &quot;data/2021-12-01-convert-media-to-string/archery.mp4&quot; Video(video_file_path, embed=True) . Your browser does not support the video tag. Both the video and the audio are recovered when decoding the encoded video data. . video_text = encode_media_to_string(video_file_path) video_bytes = decode_string_to_media(video_text) Video(video_bytes.getvalue(), embed=True, mimetype=&quot;video/mp4&quot;) . Your browser does not support the video tag.",
            "url": "https://thigm85.github.io/blog/image%20data/video%20data/encoding/decoding/2021/12/01/convert-media-to-string.html",
            "relUrl": "/image%20data/video%20data/encoding/decoding/2021/12/01/convert-media-to-string.html",
            "date": " • Dec 1, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Exploring the UCF 101 Action Dataset",
            "content": "From the UCF 101 dataset website: UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. We downloaded a zipped file containing 13320 trimmed videos, each including one action, and a text file containing the list of action classes and their numerical index. . List files available . Unzip the .avi files into a VIDEO_DIR folder. . import glob video_files = glob.glob(os.path.join(os.environ[&quot;VIDEO_DIR&quot;], &quot;*.avi&quot;)) . video_files[0:2] . [&#39;/Users/tmartins/projects/data/ucf_101/UCF101/v_CricketBowling_g01_c06.avi&#39;, &#39;/Users/tmartins/projects/data/ucf_101/UCF101/v_BreastStroke_g06_c01.avi&#39;] . Convert from .avi to .mp4 . There is better support for .mp4 files, so we will convert the .avi files to .mp4 using ffmpeg. The code below requires that your machine have ffmpeg installed. . import subprocess def convert_from_avi_to_mp4(file_name): outputfile = file_name.lower().replace(&quot;.avi&quot;, &quot;.mp4&quot;) subprocess.call([&#39;ffmpeg&#39;, &#39;-i&#39;, file_name, outputfile]) . The code below takes quite a while and could be sped up by using multi-processing: . for file_name in video_files: convert_from_avi_to_mp4(file_name) . Display .mp4 files . Once all the files are converted we can list all the .mp4 files . mp4_video_files = glob.glob(os.path.join(os.environ[&quot;VIDEO_DIR&quot;], &quot;*.mp4&quot;)) mp4_video_files[0:2] . [&#39;/Users/tmartins/projects/data/ucf_101/UCF101/v_knitting_g17_c03.mp4&#39;, &#39;/Users/tmartins/projects/data/ucf_101/UCF101/v_floorgymnastics_g25_c05.mp4&#39;] . Select one of the videos at random for illustration: . from IPython.display import Video from random import choice, seed seed(457) video_path = choice(mp4_video_files) video_path . &#39;/Users/tmartins/projects/data/ucf_101/UCF101/v_soccerjuggling_g01_c02.mp4&#39; . Display the selected video: . Video(video_path, embed=True) . Your browser does not support the video tag. Generate image frames from video . A video is a sequence of image frames. In this section we want to create a function that generates image frames at a given rate of frames per second and then select a specific number of equally spaced image frames to be kept for further processing. . import numpy as np def sample_images(images, number_to_sample): &quot;&quot;&quot; Sample equally spaced frames from a list of image frames :param images: a list of image frames. :param number_to_sample: int representing the number os frames to sample. :return: a numpy array containing the sample of frames. &quot;&quot;&quot; if number_to_sample &lt; len(images): idx = np.round(np.linspace(0, len(images) - 1, number_to_sample)).astype(int) return np.array(images)[idx] else: return np.array(images) . import imageio def extract_images(video_path, number_frames): &quot;&quot;&quot; Extract equally spaced frames from a video. :param video_path: Full .mp4 video path. :param number_frames: Number of frames to sample. :return: a numpy array containing the sample of frames. &quot;&quot;&quot; reader = imageio.get_reader( video_path, fps=1 ) frames = [] for i, im in enumerate(reader): frames.append(im) return sample_images(frames, number_frames) . Generate 4 equally spaced frames from the video displayed above. . images = extract_images(video_path, 4) . Plot the extracted frames: . import matplotlib.pyplot as plt def plot_images(images): fig = plt.figure(figsize=(10, 10)) for idx, image in enumerate(images): sub = fig.add_subplot(2,2,idx+1) imgplot = plt.imshow(image) plt.tight_layout() . plot_images(images) .",
            "url": "https://thigm85.github.io/blog/ucf%20101/video%20data/image%20frames/2021/11/17/explore-ucf101-dataset.html",
            "relUrl": "/ucf%20101/video%20data/image%20frames/2021/11/17/explore-ucf101-dataset.html",
            "date": " • Nov 17, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Simple baseline for YouTube 8M video-level features with TensorFlow",
            "content": "The goal of this blog post is to show how to use the TensorFlow API to create a multi-label logistic classification model that takes multiple inputs. The focus is not on the results as we will use just a sample dataset, but on the API itself. This post builds on a previous blog post that shows how to create a TensorFlow Dataset for the YouTube 8M video-level dataset. . Requirements . This code works with tensorflow 2.6.0. . import os import numpy as np import tensorflow as tf from tensorflow import keras . print(tf.__version__) . 2.6.0 . Load parsed dataset . The parsed dataset created in the previous blog post was saved using tf.data.experimental.save: . tf.data.experimental.save(parsed_dataset, os.path.join(data_folder, &quot;dataset&quot;)) . Load the parsed dataset: . parsed_dataset = tf.data.experimental.load(os.path.join(os.environ[&quot;DATA_FOLDER&quot;], &quot;dataset&quot;)) . for parsed_record in parsed_dataset.take(1): print(repr(parsed_record)) . {&#39;id&#39;: &lt;tf.Tensor: shape=(1,), dtype=string, numpy=array([b&#39;eACj&#39;], dtype=object)&gt;, &#39;labels&#39;: &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([180, 304])&gt;, &#39;mean_rgb&#39;: &lt;tf.Tensor: shape=(1024,), dtype=float32, numpy= array([ 0.34214902, 1.0072957 , -0.28980112, ..., -0.38452676, 0.07256398, -0.9404775 ], dtype=float32)&gt;, &#39;mean_audio&#39;: &lt;tf.Tensor: shape=(128,), dtype=float32, numpy= array([-1.5312055 , -1.0285152 , 0.15257615, -1.3953794 , -0.5539142 , 1.066028 , -1.8354464 , 0.3552817 , -0.7087098 , 0.95269775, -0.35108703, -1.0913819 , -0.43328798, -0.13257357, 0.9500226 , 1.6974918 , 1.8891319 , -0.3803924 , -1.9713941 , 1.7584128 , -0.551239 , 0.13044512, -0.04392789, -1.3871107 , -1.3588997 , -0.08746034, 0.98711026, 0.00665731, -0.3661653 , -0.92649364, 0.11269166, 1.5400211 , 0.5915486 , -1.6733549 , -0.5325128 , -0.9271016 , -1.7089834 , 0.76628643, -1.054659 , 0.4481834 , -0.21100494, 0.12168999, -0.22766402, -1.0156257 , -1.2115217 , 0.42374197, 0.5706336 , 0.06538964, 0.33071873, -0.04344149, 0.15525132, -1.0446879 , -0.78811395, -0.4171153 , -0.52485204, 0.4324971 , -0.6081474 , 0.45110175, -0.13913992, -0.4041042 , 0.2465722 , 0.34263542, -1.3624262 , -0.04867025, 0.42751154, 0.3208692 , 0.12728354, -1.0325279 , 1.2633833 , -0.4146833 , -1.2371792 , 0.17993592, -1.0372703 , -0.7702389 , -1.0303392 , -0.83468634, 1.5831887 , -0.43401757, 0.1333635 , 1.0003645 , 0.72445637, 0.50229496, -0.67599964, 0.96339846, -0.14984064, 1.27834 , 1.491503 , -0.4544462 , -0.04380629, -0.7911539 , -0.16260853, -0.12308885, 0.5622433 , -0.909713 , 0.7098645 , -1.4420735 , 0.30895248, -1.7302632 , -0.14376068, 1.0689464 , -0.13062799, 0.5123877 , 0.6601305 , 1.0648121 , 0.99878377, 0.43930665, -0.05961416, -0.0680045 , -0.37261006, -1.1954707 , -0.9128745 , 0.6335003 , -1.6354159 , 0.00629251, 1.1883566 , 1.6427722 , 0.48028553, 0.5267363 , -0.80732656, -0.8823532 , -1.0776412 , 1.2457514 , 0.06478164, 0.05408093, -0.54844224, 0.1208388 , -0.78628993, 0.5823071 ], dtype=float32)&gt;} . Logistic regression . Builds a multi-label logistic classification model that takes the image and audio vectors as input. . Number of classes . According to the YouTube 8M video-level dataset there are 3862 classes. We can check if our sample data has at most 3862 different labels. It is a good opportunity to use the tf.Dataset.reduce method. . def tf_reduce_unique_values(old, new): concat_tensor = tf.concat([old, new[&quot;labels&quot;]], axis = 0) y, _ = tf.unique(concat_tensor) return y unique_labels = tf.sort(parsed_dataset.reduce( np.array([], dtype=np.int64), tf_reduce_unique_values )) . unique_labels . &lt;tf.Tensor: shape=(3687,), dtype=int64, numpy=array([ 0, 1, 2, ..., 3859, 3860, 3861])&gt; . assert unique_labels[-1] &lt;= 3861 # The dataset has a total of 3862 classes . We can then define the number of classes to be 3862: . number_classes = 3862 . Define the model . Use keras functional API to define a multiple inputs model: . mean_rgb = keras.Input(name=&quot;mean_rgb&quot;, shape=(1024,)) mean_audio = keras.Input(name=&quot;mean_audio&quot;, shape=(128,)) x = keras.layers.concatenate([mean_rgb, mean_audio]) x = keras.layers.Dense(activation=&quot;sigmoid&quot;, units=number_classes)(x) model = keras.Model(inputs=[mean_rgb, mean_audio], outputs=[x]) . Compile the model . Since each video can belong to more than one class, we need to build a multi-label classification model. We can then use the binary crossentropy loss function and the binary accuracy metric for reasons discussed in this blog post. . model.compile( optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.BinaryCrossentropy(), metrics=[keras.metrics.BinaryAccuracy()], ) . Prepare the dataset . The keras training API accepts a tf.Dataset as input but it expects a tuple containing (features, labels). We need then to preprocess our parsed_dataset to turn it into a train_dataset with appropriate output format. We also need to transform the labels from a list of integers to a multi-hot encoding as desccribed in this blog post. . def training_preprocessing(data, number_classes): features = {&quot;mean_rgb&quot;: data[&quot;mean_rgb&quot;], &quot;mean_audio&quot;: data[&quot;mean_audio&quot;]} one_hot = tf.one_hot(indices=data[&quot;labels&quot;], depth=number_classes) label = tf.reduce_max(one_hot, axis = 0) return (features, label) . train_dataset = parsed_dataset.map(lambda x: training_preprocessing(x, number_classes=number_classes)) . for data in train_dataset.take(1): print(repr(data)) . ({&#39;mean_rgb&#39;: &lt;tf.Tensor: shape=(1024,), dtype=float32, numpy= array([ 0.34214902, 1.0072957 , -0.28980112, ..., -0.38452676, 0.07256398, -0.9404775 ], dtype=float32)&gt;, &#39;mean_audio&#39;: &lt;tf.Tensor: shape=(128,), dtype=float32, numpy= array([-1.5312055 , -1.0285152 , 0.15257615, -1.3953794 , -0.5539142 , 1.066028 , -1.8354464 , 0.3552817 , -0.7087098 , 0.95269775, -0.35108703, -1.0913819 , -0.43328798, -0.13257357, 0.9500226 , 1.6974918 , 1.8891319 , -0.3803924 , -1.9713941 , 1.7584128 , -0.551239 , 0.13044512, -0.04392789, -1.3871107 , -1.3588997 , -0.08746034, 0.98711026, 0.00665731, -0.3661653 , -0.92649364, 0.11269166, 1.5400211 , 0.5915486 , -1.6733549 , -0.5325128 , -0.9271016 , -1.7089834 , 0.76628643, -1.054659 , 0.4481834 , -0.21100494, 0.12168999, -0.22766402, -1.0156257 , -1.2115217 , 0.42374197, 0.5706336 , 0.06538964, 0.33071873, -0.04344149, 0.15525132, -1.0446879 , -0.78811395, -0.4171153 , -0.52485204, 0.4324971 , -0.6081474 , 0.45110175, -0.13913992, -0.4041042 , 0.2465722 , 0.34263542, -1.3624262 , -0.04867025, 0.42751154, 0.3208692 , 0.12728354, -1.0325279 , 1.2633833 , -0.4146833 , -1.2371792 , 0.17993592, -1.0372703 , -0.7702389 , -1.0303392 , -0.83468634, 1.5831887 , -0.43401757, 0.1333635 , 1.0003645 , 0.72445637, 0.50229496, -0.67599964, 0.96339846, -0.14984064, 1.27834 , 1.491503 , -0.4544462 , -0.04380629, -0.7911539 , -0.16260853, -0.12308885, 0.5622433 , -0.909713 , 0.7098645 , -1.4420735 , 0.30895248, -1.7302632 , -0.14376068, 1.0689464 , -0.13062799, 0.5123877 , 0.6601305 , 1.0648121 , 0.99878377, 0.43930665, -0.05961416, -0.0680045 , -0.37261006, -1.1954707 , -0.9128745 , 0.6335003 , -1.6354159 , 0.00629251, 1.1883566 , 1.6427722 , 0.48028553, 0.5267363 , -0.80732656, -0.8823532 , -1.0776412 , 1.2457514 , 0.06478164, 0.05408093, -0.54844224, 0.1208388 , -0.78628993, 0.5823071 ], dtype=float32)&gt;}, &lt;tf.Tensor: shape=(3862,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)&gt;) . Fit the model . We can then use the fit method with the train_dataset that we created above. . model.fit(train_dataset.batch(32), epochs=3) . Epoch 1/3 1294/1294 [==============================] - 55s 42ms/step - loss: 0.4018 - binary_accuracy: 0.8924 Epoch 2/3 1294/1294 [==============================] - 55s 43ms/step - loss: 0.1287 - binary_accuracy: 0.9953 Epoch 3/3 1294/1294 [==============================] - 54s 42ms/step - loss: 0.0399 - binary_accuracy: 0.9994 . &lt;keras.callbacks.History at 0x17eb4c640&gt; .",
            "url": "https://thigm85.github.io/blog/youtube%208m/video%20data/tensorflow/training/2021/11/12/youtube-8m-video-level-baseline.html",
            "relUrl": "/youtube%208m/video%20data/tensorflow/training/2021/11/12/youtube-8m-video-level-baseline.html",
            "date": " • Nov 12, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Compare pre-trained CLIP models for text-image retrieval",
            "content": "CLIP model . There are multiple CLIP model variations . import clip clip.available_models() . [&#39;RN50&#39;, &#39;RN101&#39;, &#39;RN50x4&#39;, &#39;RN50x16&#39;, &#39;ViT-B/32&#39;, &#39;ViT-B/16&#39;] . Custom PyTorch ImageFeedDataset . Create a PyTorch dataset that loads an image, create a CLIP-based embedding and output the data into a pyvespa-compatible feed format to make it easier to feed the entire dataset into the search app that will be created below. . import glob import ntpath import torch from torch.utils.data import Dataset from PIL import Image def translate_model_names_to_valid_vespa_field_names(model_name): return model_name.replace(&quot;/&quot;, &quot;_&quot;).replace(&quot;-&quot;, &quot;_&quot;).lower() class ImageFeedDataset(Dataset): def __init__(self, img_dir, model_name): valid_vespa_model_name = translate_model_names_to_valid_vespa_field_names(model_name) self.model, self.preprocess = clip.load(model_name) self.img_dir = img_dir self.image_file_names = glob.glob(os.path.join(img_dir, &quot;*.jpg&quot;)) self.image_embedding_name = valid_vespa_model_name + &quot;_image&quot; def _from_image_to_vector(self, x): with torch.no_grad(): image_features = self.model.encode_image(self.preprocess(x).unsqueeze(0)).float() image_features /= image_features.norm(dim=-1, keepdim=True) return image_features def __len__(self): return len(self.image_file_names) def __getitem__(self, idx): image_file_name = self.image_file_names[idx] image = Image.open(image_file_name) image = self._from_image_to_vector(image) image_base_name = ntpath.basename(image_file_name) return { &quot;id&quot;: image_base_name.split(&quot;.jpg&quot;)[0], &quot;fields&quot;: { &quot;image_file_name&quot;: image_base_name, self.image_embedding_name: {&quot;values&quot;: image.tolist()[0]} }, &quot;create&quot;: True } def get_embedding_size(self): return len(self.__getitem__(0)[&quot;fields&quot;][image_dataset.image_embedding_name][&quot;values&quot;]) . Text to embedding mapping . We need a text processor to map text to embedding when querying our search app . import clip import torch class TextProcessor(object): def __init__(self, model_name): self.model, _ = clip.load(model_name) self.model_name = model_name def embed(self, text): text_tokens = clip.tokenize(text) with torch.no_grad(): text_features = self.model.encode_text(text_tokens).float() text_features /= text_features.norm(dim=-1, keepdim=True) return text_features.tolist()[0] . Create Vespa app . Compute embedding sizes . We need to know embedding sizes when creating our search app. Note that each model variation has a different embedding size. . model_info = {} for model_name in clip.available_models(): image_dataset = ImageFeedDataset( img_dir=os.environ[&quot;IMG_DIR&quot;], # Folder containing image files model_name=model_name # CLIP model name used to convert image into vector ) embedding_size = image_dataset.get_embedding_size() model_info[translate_model_names_to_valid_vespa_field_names(model_name)] = embedding_size print(model_info) . {&#39;rn50&#39;: 1024, &#39;rn101&#39;: 512, &#39;rn50x4&#39;: 640, &#39;rn50x16&#39;: 768, &#39;vit_b_32&#39;: 512, &#39;vit_b_16&#39;: 512} . Create application package based on CLIP models . from vespa.package import ApplicationPackage, Field, HNSW, RankProfile, QueryTypeField def create_text_image_app(model_info): &quot;&quot;&quot; Create text to image search app based on a variety of CLIP models :param model_info: dict containing (vespa compatible) model names as keys and embedding size as values. Check `clip.available_models()` to check which models are available. :return: A Vespa application package. &quot;&quot;&quot; app_package = ApplicationPackage(name=&quot;image_search&quot;) app_package.schema.add_fields( Field( name=&quot;image_file_name&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;] ), ) for model_name, embedding_size in model_info.items(): app_package.schema.add_fields( Field( name=model_name + &quot;_image&quot;, type=&quot;tensor&lt;float&gt;(x[{}])&quot;.format(embedding_size), indexing=[&quot;attribute&quot;, &quot;index&quot;], ann=HNSW( distance_metric=&quot;euclidean&quot;, max_links_per_node=16, neighbors_to_explore_at_insert=500 ) ) ) app_package.schema.add_rank_profile( RankProfile( name=model_name + &quot;_similarity&quot;, inherits=&quot;default&quot;, first_phase=&quot;closeness({})&quot;.format(model_name + &quot;_image&quot;) ) ) app_package.query_profile_type.add_fields( QueryTypeField( name=&quot;ranking.features.query({})&quot;.format(model_name + &quot;_text&quot;), type=&quot;tensor&lt;float&gt;(x[{}])&quot;.format(embedding_size) ) ) return app_package . app_package = create_text_image_app(model_info) . Deploy to Vespa Cloud . from vespa.deployment import VespaCloud vespa_cloud = VespaCloud( tenant=&quot;vespa-team&quot;, application=&quot;pyvespa-integration&quot;, key_location=os.environ[&quot;USER_KEY_PATH&quot;], application_package=app_package, ) app = vespa_cloud.deploy( instance=&quot;clip-image-search&quot;, disk_folder=os.environ[&quot;DISK_FOLDER&quot;] ) . Compute and feed image embeddings . import time from aiohttp.client_exceptions import ClientConnectorError from asyncio import TimeoutError from torch.utils.data import DataLoader # This is for demo purpose as this step should be run outside a notebook on a multi-processing environment. for model_name in clip.available_models(): image_dataset = ImageFeedDataset( img_dir=os.environ[&quot;IMG_DIR&quot;], # Folder containing image files model_name=model_name # CLIP model name used to convert image into vector ) dataloader = DataLoader(image_dataset, batch_size=128, shuffle=False, collate_fn=lambda x: x) for idx, batch in enumerate(dataloader): responses = None while responses is None: try: responses = app.update_batch(batch=batch) except (ClientConnectorError, TimeoutError): time.sleep(3) print(&quot;Model name: {}. Iteration: {}/{}&quot;.format(model_name, idx, len(dataloader))) print(&quot;Status code summary: {}&quot;.format(Counter([x.status_code for x in responses]))) . Evaluate . Define search evaluation metrics: . from vespa.evaluation import MatchRatio, Recall, ReciprocalRank eval_metrics = [ MatchRatio(), Recall(at=5), Recall(at=100), ReciprocalRank(at=5), ReciprocalRank(at=100) ] . Create a functions that takes query and returns the body of a query request based on the Vespa Query Language. . from vespa.query import QueryModel def create_vespa_query(query, text_processor): valid_vespa_model_name = translate_model_names_to_valid_vespa_field_names(text_processor.model_name) image_field_name = valid_vespa_model_name + &quot;_image&quot; text_field_name = valid_vespa_model_name + &quot;_text&quot; ranking_name = valid_vespa_model_name + &quot;_similarity&quot; return { &#39;yql&#39;: &#39;select * from sources * where ([{{&quot;targetNumHits&quot;:100}}]nearestNeighbor({},{}));&#39;.format( image_field_name, text_field_name ), &#39;hits&#39;: 100, &#39;ranking.features.query({})&#39;.format(text_field_name): text_processor.embed(query), &#39;ranking.profile&#39;: ranking_name, &#39;timeout&#39;: 10 } def create_body_function(model_name): text_processor = TextProcessor(model_name=model_name) return lambda x: create_vespa_query(x, text_processor=text_processor) . Create one QueryModel for each of the CLIP models . query_models = [] for model_name in clip.available_models(): query_models.append( QueryModel( name=model_name, body_function=create_body_function(model_name) ) ) . Load labeled data. . from pandas import read_csv labeled_data = read_csv(&quot;/Users/tmartins/projects/data/flickr8k/labeled_data.csv&quot;, sep = &quot; t&quot;) labeled_data.head() . qid query doc_id relevance . 0 0 | A white dog runs in the grass | 1119015538_e8e796281e.jpg | 1 | . 1 1 | A boy jumps from one bed to another | 1131932671_c8d17751b3.jpg | 1 | . 2 2 | Three people and a sled | 115684808_cb01227802.jpg | 1 | . 3 3 | A group of people walking a city street in war... | 1174629344_a2e1a2bdbf.jpg | 1 | . 4 4 | Two children one of which is holding a stick a... | 1322323208_c7ecb742c6.jpg | 1 | . Evaluate the application and return per query results. . result = app.evaluate( labeled_data=labeled_data, eval_metrics=eval_metrics, query_model=query_models, id_field=&quot;image_file_name&quot;, per_query=True ) . result.head() . model query_id match_ratio recall_5 recall_100 reciprocal_rank_5 reciprocal_rank_100 . 0 RN50 | 0 | 0.012359 | 0.0 | 0.0 | 0.0 | 0.000000 | . 1 RN101 | 0 | 0.012359 | 0.0 | 1.0 | 0.0 | 0.013333 | . 2 RN50x4 | 0 | 0.012359 | 0.0 | 1.0 | 0.0 | 0.012987 | . 3 RN50x16 | 0 | 0.012359 | 0.0 | 0.0 | 0.0 | 0.000000 | . 4 ViT-B/32 | 0 | 0.012359 | 0.0 | 1.0 | 0.0 | 0.013889 | . Visualize RR@100: . import plotly.express as px fig = px.box(result, x=&quot;model&quot;, y=&quot;reciprocal_rank_100&quot;) fig.show() . Compute mean and median across models: . result[[&quot;model&quot;, &quot;reciprocal_rank_100&quot;]].groupby( &quot;model&quot; ).agg( Mean=(&#39;reciprocal_rank_100&#39;, &#39;mean&#39;), Median=(&#39;reciprocal_rank_100&#39;, &#39;median&#39;) ) . Mean Median . model . RN101 0.264650 | 0.083333 | . RN50 0.255586 | 0.066667 | . RN50x16 0.325659 | 0.100000 | . RN50x4 0.277444 | 0.076923 | . ViT-B/16 0.294005 | 0.100000 | . ViT-B/32 0.304529 | 0.111111 | .",
            "url": "https://thigm85.github.io/blog/text_image_search/clip_model/vespa/flicker8k/2021/10/28/compare-multiple-clip-models-for-text-image-search.html",
            "relUrl": "/text_image_search/clip_model/vespa/flicker8k/2021/10/28/compare-multiple-clip-models-for-text-image-search.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Evaluate text-image search app with Flickr 8k dataset",
            "content": "This post creates a labeled dataset out of the Flicker 8k image-caption dataset, builds a text processor that uses a CLIP model to map a text query into the same 512-dimensional space used to represent images and evaluate different query models using the Vespa python API. . Check the previous three posts for context: . Flicker 8k dataset first exploration | Understanding CLIP image pipeline | Vespa image search with PyTorch feeder | . Create labeled data . An (image, caption) pair will be considered relevant for our purposes if all three experts agreed on a relevance score equal to 4. . Load and check the expert judgments . from pandas import read_csv experts = read_csv( os.path.join(os.environ[&quot;DATA_FOLDER&quot;], &quot;ExpertAnnotations.txt&quot;), sep = &quot; t&quot;, header=None, names=[&quot;image_file_name&quot;, &quot;caption_id&quot;, &quot;expert_1&quot;, &quot;expert_2&quot;, &quot;expert_3&quot;] ) . experts.head() . image_file_name caption_id expert_1 expert_2 expert_3 . 0 1056338697_4f7d7ce270.jpg | 2549968784_39bfbe44f9.jpg#2 | 1 | 1 | 1 | . 1 1056338697_4f7d7ce270.jpg | 2718495608_d8533e3ac5.jpg#2 | 1 | 1 | 2 | . 2 1056338697_4f7d7ce270.jpg | 3181701312_70a379ab6e.jpg#2 | 1 | 1 | 2 | . 3 1056338697_4f7d7ce270.jpg | 3207358897_bfa61fa3c6.jpg#2 | 1 | 2 | 2 | . 4 1056338697_4f7d7ce270.jpg | 3286822339_5535af6b93.jpg#2 | 1 | 1 | 2 | . Check cases where all experts agrees . experts_agreement_bool = experts.apply( lambda x: x[&quot;expert_1&quot;] == x[&quot;expert_2&quot;] and x[&quot;expert_2&quot;] == x[&quot;expert_3&quot;], axis=1 ) experts_agreement = experts[experts_agreement_bool][ [&quot;image_file_name&quot;, &quot;caption_id&quot;, &quot;expert_1&quot;] ].rename(columns={&quot;expert_1&quot;:&quot;expert&quot;}) . experts_agreement.head() . image_file_name caption_id expert . 0 1056338697_4f7d7ce270.jpg | 2549968784_39bfbe44f9.jpg#2 | 1 | . 5 1056338697_4f7d7ce270.jpg | 3360930596_1e75164ce6.jpg#2 | 1 | . 6 1056338697_4f7d7ce270.jpg | 3545652636_0746537307.jpg#2 | 1 | . 8 106490881_5a2dd9b7bd.jpg | 1425069308_488e5fcf9d.jpg#2 | 1 | . 9 106490881_5a2dd9b7bd.jpg | 1714316707_8bbaa2a2ba.jpg#2 | 2 | . experts_agreement[&quot;expert&quot;].value_counts().sort_index() . 1 2350 2 580 3 214 4 247 Name: expert, dtype: int64 . Load captions data . captions = read_csv( os.path.join(os.environ[&quot;DATA_FOLDER&quot;], &quot;Flickr8k.token.txt&quot;), sep=&quot; t&quot;, header=None, names=[&quot;caption_id&quot;, &quot;caption&quot;] ) . captions.head() . caption_id caption . 0 1000268201_693b08cb0e.jpg#0 | A child in a pink dress is climbing up a set o... | . 1 1000268201_693b08cb0e.jpg#1 | A girl going into a wooden building . | . 2 1000268201_693b08cb0e.jpg#2 | A little girl climbing into a wooden playhouse . | . 3 1000268201_693b08cb0e.jpg#3 | A little girl climbing the stairs to her playh... | . 4 1000268201_693b08cb0e.jpg#4 | A little girl in a pink dress going into a woo... | . def get_caption(caption_id, captions): return captions[captions[&quot;caption_id&quot;] == caption_id][&quot;caption&quot;].values[0] . Relevant (image, text) pair . relevant_data = experts_agreement[experts_agreement[&quot;expert&quot;] == 4] relevant_data.head(3) . image_file_name caption_id expert . 43 1119015538_e8e796281e.jpg | 416106657_cab2a107a5.jpg#2 | 4 | . 53 1131932671_c8d17751b3.jpg | 1131932671_c8d17751b3.jpg#2 | 4 | . 66 115684808_cb01227802.jpg | 115684808_cb01227802.jpg#2 | 4 | . Create labeled data . from ntpath import basename from pandas import DataFrame labeled_data = DataFrame( data={ &quot;qid&quot;: list(range(relevant_data.shape[0])), &quot;query&quot;: [get_caption( caption_id=x, captions=captions ).replace(&quot; ,&quot;, &quot;&quot;).replace(&quot; .&quot;, &quot;&quot;) for x in list(relevant_data.caption_id)], &quot;doc_id&quot;: [basename(x) for x in list(relevant_data.image_file_name)], &quot;relevance&quot;: 1} ) labeled_data.head() . qid query doc_id relevance . 0 0 | A white dog runs in the grass | 1119015538_e8e796281e.jpg | 1 | . 1 1 | A boy jumps from one bed to another | 1131932671_c8d17751b3.jpg | 1 | . 2 2 | Three people and a sled | 115684808_cb01227802.jpg | 1 | . 3 3 | A group of people walking a city street in war... | 1174629344_a2e1a2bdbf.jpg | 1 | . 4 4 | Two children one of which is holding a stick a... | 1322323208_c7ecb742c6.jpg | 1 | . From text to embeddings . Create a text processor to map a text string into the same 512-dimensional space used to embed the images. . import clip import torch class TextProcessor(object): def __init__(self, model_name): self.model, _ = clip.load(model_name) def embed(self, text): text_tokens = clip.tokenize(text) with torch.no_grad(): text_features = model.encode_text(text_tokens).float() text_features /= text_features.norm(dim=-1, keepdim=True) return text_features.tolist()[0] . Evaluate . Define search evaluation metrics: . from vespa.evaluation import MatchRatio, Recall, ReciprocalRank eval_metrics = [ MatchRatio(), Recall(at=5), Recall(at=100), ReciprocalRank(at=5), ReciprocalRank(at=100) ] . Instantiate TextProcessor with a specific CLIP model. . text_processor = TextProcessor(model_name=&quot;ViT-B/32&quot;) . Create a QueryModel&#39;s to be evaluated. In this case we create two query models based on the ViT-B/32 CLIP model, one that sends the query as it is and another that prepends the prompt &quot;A photo of &quot; to the query before sending it, as suggest in the original CLIP paper. . from vespa.query import QueryModel def create_vespa_query(query, prompt = False): if prompt: query = &quot;A photo of &quot; + query.lower() return { &#39;yql&#39;: &#39;select * from sources * where ([{&quot;targetNumHits&quot;:100}]nearestNeighbor(vit_b_32_image,vit_b_32_text));&#39;, &#39;hits&#39;: 100, &#39;ranking.features.query(vit_b_32_text)&#39;: text_processor.embed(query), &#39;ranking.profile&#39;: &#39;vit-b-32-similarity&#39;, &#39;timeout&#39;: 10 } query_model_1 = QueryModel(name=&quot;vit_b_32&quot;, body_function=create_vespa_query) query_model_2 = QueryModel(name=&quot;vit_b_32_prompt&quot;, body_function=lambda x: create_vespa_query(x, prompt=True)) . Create a connection to the Vespa instance: . app = Vespa( url=os.environ[&quot;VESPA_END_POINT&quot;], cert = os.environ[&quot;PRIVATE_CERTIFICATE_PATH&quot;] ) . Evaluate the query models using the labeled data and metrics defined earlier. The labeled data uses the image_file_name and doc id. . from vespa.application import Vespa result = app.evaluate( labeled_data=labeled_data, eval_metrics=eval_metrics, query_model=[query_model_1, query_model_2], id_field=&quot;image_file_name&quot; ) . The results shows that there is a lot of improvements to be made on the pre-trained ViT-B/32 CLIP model. . result . model vit_b_32 vit_b_32_prompt . match_ratio mean 0.012359 | 0.012359 | . median 0.012359 | 0.012359 | . std 0.000000 | 0.000000 | . recall_5 mean 0.417004 | 0.412955 | . median 0.000000 | 0.000000 | . std 0.494065 | 0.493365 | . recall_100 mean 0.870445 | 0.870445 | . median 1.000000 | 1.000000 | . std 0.336495 | 0.336495 | . reciprocal_rank_5 mean 0.279285 | 0.268084 | . median 0.000000 | 0.000000 | . std 0.394814 | 0.386606 | . reciprocal_rank_100 mean 0.304849 | 0.293651 | . median 0.111111 | 0.100000 | . std 0.378595 | 0.370633 | .",
            "url": "https://thigm85.github.io/blog/text_image_search/clip_model/vespa/flicker8k/2021/10/26/evaluate-flicker8k-image-search.html",
            "relUrl": "/text_image_search/clip_model/vespa/flicker8k/2021/10/26/evaluate-flicker8k-image-search.html",
            "date": " • Oct 26, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Vespa text-image search with PyTorch feeder",
            "content": "This post describe how to define and deploy a Vespa image search app through Vespa python API. In addition, we create a Vespa feeder based on PyTorch Dataset/Dataloader. . Check the following posts for context about the data and the model used here: . Flicker 8k dataset first exploration | Understanding CLIP image pipeline | . Vespa image search app . Check pyvespa documentation for more info about Vespa python API. . Create application package . from vespa.package import ApplicationPackage, Field, HNSW, RankProfile, QueryTypeField app_package = ApplicationPackage(name=&quot;image_search&quot;) app_package.schema.add_fields( Field( name=&quot;image_file_name&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;] ), Field( name=&quot;vit_b_32_image&quot;, type=&quot;tensor&lt;float&gt;(x[512])&quot;, indexing=[&quot;attribute&quot;, &quot;index&quot;], ann=HNSW( distance_metric=&quot;euclidean&quot;, max_links_per_node=16, neighbors_to_explore_at_insert=500 ) ) ) app_package.schema.add_rank_profile( RankProfile( name=&quot;vit-b-32-similarity&quot;, inherits=&quot;default&quot;, first_phase=&quot;closeness(vit_b_32_image)&quot; ) ) app_package.query_profile_type.add_fields( QueryTypeField( name=&quot;ranking.features.query(vit_b_32_text)&quot;, type=&quot;tensor&lt;float&gt;(x[512])&quot; ) ) . Deploy application . from vespa.deployment import VespaDocker vespa_docker = VespaDocker(disk_folder=os.environ[&quot;DISK_FOLDER&quot;]) app = vespa_docker.deploy(application_package=app_package) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. Waiting for application status. Waiting for application status. Finished deployment. . Feeding . Create pytorch ImageFeedDataset . Create a custom Dataset that loads an image, transform it into a 512-dimension vector and return data into a pyvespa-compatible format. . import os import glob import ntpath import torch from torch.utils.data import Dataset from PIL import Image import clip class ImageFeedDataset(Dataset): def __init__(self, img_dir, image_embedding_name, model_name): self.model, self.preprocess = clip.load(model_name) self.img_dir = img_dir self.image_file_names = glob.glob(os.path.join(img_dir, &quot;*.jpg&quot;)) self.image_embedding_name = image_embedding_name def _from_image_to_vector(self, x): with torch.no_grad(): image_features = self.model.encode_image(self.preprocess(x).unsqueeze(0)).float() image_features /= image_features.norm(dim=-1, keepdim=True) return image_features def __len__(self): return len(self.image_file_names) def __getitem__(self, idx): image_file_name = self.image_file_names[idx] image = Image.open(image_file_name) image = self._from_image_to_vector(image) image_base_name = ntpath.basename(image_file_name) return { &quot;id&quot;: image_base_name.split(&quot;.jpg&quot;)[0], &quot;fields&quot;: { &quot;image_file_name&quot;: image_base_name, self.image_embedding_name: {&quot;values&quot;: image.tolist()[0]} } } . Instantiate Dataset and DataLoader . image_dataset = ImageFeedDataset( img_dir=os.environ[&quot;IMG_DIR&quot;], # Folder containing image files image_embedding_name=&quot;vit_b_32_image&quot;, # name of the Vespa field that will hold image embedding model_name=&quot;ViT-B/32&quot; # CLIP model name used to convert image into vector ) . dataloader will make it possible for us to loop through the dataset batch_size data points at a time. Since the objective is to feed the data to the vespa app, we can set shuffle to False. We also specify a custom collate_fn function so that pyvespa-compatible format is preserved when batching. . from torch.utils.data import DataLoader dataloader = DataLoader(image_dataset, batch_size=128, shuffle=False, collate_fn=lambda x: x) . Feed the data . Note that most of the time is spent creating the image embedding. So, pre-computing the embedding will provide a significant speed-up. . for idx, batch in enumerate(dataloader): print(&quot;Iteration: {}/{}&quot;.format(idx, len(dataloader))) app.feed_batch(batch=batch) . Iteration: 0/64 Iteration: 1/64 Iteration: 2/64 Iteration: 3/64 Iteration: 4/64 Iteration: 5/64 Iteration: 6/64 Iteration: 7/64 Iteration: 8/64 Iteration: 9/64 Iteration: 10/64 Iteration: 11/64 Iteration: 12/64 Iteration: 13/64 Iteration: 14/64 Iteration: 15/64 Iteration: 16/64 Iteration: 17/64 Iteration: 18/64 Iteration: 19/64 Iteration: 20/64 Iteration: 21/64 Iteration: 22/64 Iteration: 23/64 Iteration: 24/64 Iteration: 25/64 Iteration: 26/64 Iteration: 27/64 Iteration: 28/64 Iteration: 29/64 Iteration: 30/64 Iteration: 31/64 Iteration: 32/64 Iteration: 33/64 Iteration: 34/64 Iteration: 35/64 Iteration: 36/64 Iteration: 37/64 Iteration: 38/64 Iteration: 39/64 Iteration: 40/64 Iteration: 41/64 Iteration: 42/64 Iteration: 43/64 Iteration: 44/64 Iteration: 45/64 Iteration: 46/64 Iteration: 47/64 Iteration: 48/64 Iteration: 49/64 Iteration: 50/64 Iteration: 51/64 Iteration: 52/64 Iteration: 53/64 Iteration: 54/64 Iteration: 55/64 Iteration: 56/64 Iteration: 57/64 Iteration: 58/64 Iteration: 59/64 Iteration: 60/64 Iteration: 61/64 Iteration: 62/64 Iteration: 63/64 .",
            "url": "https://thigm85.github.io/blog/image%20processing/clip%20model/vespa/pytorch/pytorch%20dataset/2021/10/25/vespa-image-search-flicker8k.html",
            "relUrl": "/image%20processing/clip%20model/vespa/pytorch/pytorch%20dataset/2021/10/25/vespa-image-search-flicker8k.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Understanding CLIP image pipeline",
            "content": "This post builds on top of the my flicker 8k exploratory analysis post. . import os # sample images to use in this post relevant_image_names = [ &#39;2610447973_89227ff978.jpg&#39;, &#39;3071676551_a65741e372.jpg&#39;, &#39;3365783912_e12c3510d8.jpg&#39;, &#39;3589367895_5d3729e3ea.jpg&#39; ] os.environ[&quot;IMG_DIR&quot;] = &quot;data/2021-10-22-understanding-clip-image-pipeline&quot; . Understanding CLIP image preprecessing . CLIP image preprocessing pipeline . import clip model, preprocess = clip.load(&quot;ViT-B/32&quot;) . We can take a look into the image pre-processing pipeline. . preprocess . Compose( Resize(size=224, interpolation=PIL.Image.BICUBIC) CenterCrop(size=(224, 224)) &lt;function _convert_image_to_rgb at 0x10eaad1f0&gt; ToTensor() Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)) ) . Load files as PIL images . The CLIP preprocessing pipeline assumes we have a PIL image as input, so that is what we will use to load images here. . from PIL import Image images = [Image.open(os.path.join(os.environ[&quot;IMG_DIR&quot;], image_file_name)) for image_file_name in relevant_image_names] . import matplotlib.pyplot as plt def plot_pil_images(images): assert len(images) == 4, &quot;Number of images should be equal to 4&quot; fig = plt.figure(figsize=(10, 10)) for idx, image in enumerate(images): sub = fig.add_subplot(2,2,idx+1) imgplot = plt.imshow(image) plt.tight_layout() . plot_pil_images(images) . Resize . Resize the image so that the smaller of height and width have size 224. . preprocess.transforms[0] . Resize(size=224, interpolation=PIL.Image.BICUBIC) . processed_images = [preprocess.transforms[0](image) for image in images] . The effect is hard to note by just looking at the images as the proportion of the images continues to be the same, but it is easy to see when we print the size of the tensors below. . plot_pil_images(processed_images) . from torchvision.transforms import ToTensor for idx, (original_image, processed_image) in enumerate(zip(images, processed_images)): print(&quot;Image {}: nOriginal size: {} nProcessed size:{} n&quot;.format(idx+1, ToTensor()(original_image).shape, ToTensor()(processed_image).shape)) . Image 1: Original size: torch.Size([3, 333, 500]) Processed size:torch.Size([3, 224, 336]) Image 2: Original size: torch.Size([3, 333, 500]) Processed size:torch.Size([3, 224, 336]) Image 3: Original size: torch.Size([3, 375, 500]) Processed size:torch.Size([3, 224, 298]) Image 4: Original size: torch.Size([3, 500, 333]) Processed size:torch.Size([3, 336, 224]) . CenterCrop . Crop the center of the image such that the resulting images has size (224, 224) . preprocess.transforms[1] . CenterCrop(size=(224, 224)) . processed_images = [preprocess.transforms[1](image) for image in processed_images] . plot_pil_images(processed_images) . Convert to RGB . preprocess.transforms[2] . &lt;function clip.clip._convert_image_to_rgb(image)&gt; . This transform convert the image to RGB model. Since the images are already formatted with RGB encoding we will not see any difference. . from torch import equal equal(ToTensor()(preprocess.transforms[2](processed_images[0])), ToTensor()(processed_images[0])) . True . But we will apply this transform for completeness: . processed_images = [preprocess.transforms[2](image) for image in processed_images] . Convert PIL image to Tensor . preprocess.transforms[3] . ToTensor() . processed_images = [preprocess.transforms[3](image) for image in processed_images] . We now have tensors: . print([x.shape for x in processed_images]) . [torch.Size([3, 224, 224]), torch.Size([3, 224, 224]), torch.Size([3, 224, 224]), torch.Size([3, 224, 224])] . Normalize . Normalize will subtract the mean and divide by the standard deviation. The is one mean and one standard deviation for each of the three channels available. . preprocess.transforms[4] . Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)) . processed_images = [preprocess.transforms[4](image) for image in processed_images] . After the normalization the processed images are quite different from the original images. . from torchvision.transforms import ToPILImage plot_pil_images([ToPILImage()(x) for x in processed_images]) . Create embedding from processed images . Stack the four processed images on top of each other. . import numpy as np import torch device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; image_input = torch.tensor(np.stack(processed_images)).to(device) . image_input.shape . torch.Size([4, 3, 224, 224]) . Generate the image vectors. Each vector have 512 elements. . with torch.no_grad(): image_features = model.encode_image(image_input).float() . image_features.shape . torch.Size([4, 512]) . Use unsqueeze(0) if you want to apply the encoder to one image as it expected a 4D Tensor. . with torch.no_grad(): image_features = model.encode_image(processed_images[0].unsqueeze(0)) . image_features.shape . torch.Size([1, 512]) . Create ImageDataset . Create a custom Dataset that loads an image and optionally apply a transform function to it. . import os import glob from torch.utils.data import Dataset class ImageDataset(Dataset): def __init__(self, img_dir, transform=None): self.img_dir = img_dir self.image_file_names = glob.glob(os.path.join(img_dir, &quot;*.jpg&quot;)) self.transform = transform def __len__(self): return len(self.image_file_names) def __getitem__(self, idx): image_file_name = self.image_file_names[idx] image = Image.open(image_file_name) if self.transform: image = self.transform(image) return image . Here is how it looks without applying and transform function to it: . image_dataset = ImageDataset(img_dir=os.environ[&quot;IMG_DIR&quot;]) . Each iteration returns one of the original images: . next(iter(image_dataset)) . Apply the pre-processing image pipeline . image_dataset = ImageDataset(img_dir=os.environ[&quot;IMG_DIR&quot;], transform=preprocess) . It returns the processed Tensor when we apply the preprocess pipeline: . next(iter(image_dataset)).shape . torch.Size([3, 224, 224]) . Apply the image encoder . def from_image_to_vector(x, process_fn): with torch.no_grad(): image_features = model.encode_image(process_fn(x).unsqueeze(0)) return image_features . image_dataset = ImageDataset( img_dir=os.environ[&quot;IMG_DIR&quot;], transform=lambda x: from_image_to_vector(x, process_fn=preprocess) ) . Now the dataset returns a 512-dimensional vector associated with a specific image. . next(iter(image_dataset)).shape . torch.Size([1, 512]) . from torch.utils.data import DataLoader image_dataloader = DataLoader(image_dataset, batch_size=64, shuffle=False) .",
            "url": "https://thigm85.github.io/blog/image%20processing/clip%20model/dual%20encoder/pil/2021/10/22/understanding-clip-image-pipeline.html",
            "relUrl": "/image%20processing/clip%20model/dual%20encoder/pil/2021/10/22/understanding-clip-image-pipeline.html",
            "date": " • Oct 22, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Explore the Flicker 8k dataset",
            "content": "Download the data . Download instructions available on the jbrownlee&#39;s Datasets repo. . Expert judgements . Load and check the expert judgments . from pandas import read_csv experts = read_csv( &quot;flicker8k/ExpertAnnotations.txt&quot;, sep = &quot; t&quot;, header=None, names=[&quot;image_file_name&quot;, &quot;caption_id&quot;, &quot;expert_1&quot;, &quot;expert_2&quot;, &quot;expert_3&quot;] ) . experts.head() . image_file_name caption_id expert_1 expert_2 expert_3 . 0 1056338697_4f7d7ce270.jpg | 2549968784_39bfbe44f9.jpg#2 | 1 | 1 | 1 | . 1 1056338697_4f7d7ce270.jpg | 2718495608_d8533e3ac5.jpg#2 | 1 | 1 | 2 | . 2 1056338697_4f7d7ce270.jpg | 3181701312_70a379ab6e.jpg#2 | 1 | 1 | 2 | . 3 1056338697_4f7d7ce270.jpg | 3207358897_bfa61fa3c6.jpg#2 | 1 | 2 | 2 | . 4 1056338697_4f7d7ce270.jpg | 3286822339_5535af6b93.jpg#2 | 1 | 1 | 2 | . experts.shape . (5822, 5) . Check cases where all experts agrees . experts_agreement_bool = experts.apply( lambda x: x[&quot;expert_1&quot;] == x[&quot;expert_2&quot;] and x[&quot;expert_2&quot;] == x[&quot;expert_3&quot;], axis=1 ) . experts_agreement = experts[experts_agreement_bool][[&quot;image_file_name&quot;, &quot;caption_id&quot;, &quot;expert_1&quot;]].rename(columns={&quot;expert_1&quot;:&quot;expert&quot;}) . experts_agreement.head() . image_file_name caption_id expert . 0 1056338697_4f7d7ce270.jpg | 2549968784_39bfbe44f9.jpg#2 | 1 | . 5 1056338697_4f7d7ce270.jpg | 3360930596_1e75164ce6.jpg#2 | 1 | . 6 1056338697_4f7d7ce270.jpg | 3545652636_0746537307.jpg#2 | 1 | . 8 106490881_5a2dd9b7bd.jpg | 1425069308_488e5fcf9d.jpg#2 | 1 | . 9 106490881_5a2dd9b7bd.jpg | 1714316707_8bbaa2a2ba.jpg#2 | 2 | . experts_agreement[&quot;expert&quot;].value_counts().sort_index() . 1 2350 2 580 3 214 4 247 Name: expert, dtype: int64 . Load images and captions . import os import matplotlib.pyplot as plt import matplotlib.image as mpimg from textwrap import wrap . def load_image(file_name, relative_folder): return mpimg.imread(os.path.join(relative_folder, file_name)) . def get_caption(caption_id, captions): return captions[captions[&quot;caption_id&quot;] == caption_id][&quot;caption&quot;].values[0] . def plot_images_and_captions(image_names, caption_ids, relative_image_folder, wrap_value = 50): assert len(image_names) == len(caption_ids) == 4, &quot;Number of images and captions should be equal to 4&quot; fig = plt.figure(figsize=(10, 10)) for idx, (image_file_name, caption_id) in enumerate(zip(image_names, caption_ids)): sub = fig.add_subplot(2,2,idx+1) imgplot = plt.imshow(load_image(image_file_name, relative_folder=relative_image_folder)) sub.set_title(&quot; n&quot;.join(wrap(get_caption(caption_id, captions), wrap_value))) plt.tight_layout() . captions = read_csv(&quot;flicker8k/Flickr8k.token.txt&quot;, sep=&quot; t&quot;, header=None, names=[&quot;caption_id&quot;, &quot;caption&quot;]) . captions.head() . caption_id caption . 0 1000268201_693b08cb0e.jpg#0 | A child in a pink dress is climbing up a set o... | . 1 1000268201_693b08cb0e.jpg#1 | A girl going into a wooden building . | . 2 1000268201_693b08cb0e.jpg#2 | A little girl climbing into a wooden playhouse . | . 3 1000268201_693b08cb0e.jpg#3 | A little girl climbing the stairs to her playh... | . 4 1000268201_693b08cb0e.jpg#4 | A little girl in a pink dress going into a woo... | . Sample images . Relevant sample (score = 4) . relevant_pairs = experts_agreement[experts_agreement[&quot;expert&quot;] == 4].sample(4, random_state=675)[[&quot;image_file_name&quot;, &quot;caption_id&quot;]] relevant_pairs . image_file_name caption_id . 1869 2610447973_89227ff978.jpg | 2610447973_89227ff978.jpg#2 | . 2906 3071676551_a65741e372.jpg | 3071676551_a65741e372.jpg#2 | . 3917 3365783912_e12c3510d8.jpg | 3365783912_e12c3510d8.jpg#2 | . 4669 3589367895_5d3729e3ea.jpg | 3589367895_5d3729e3ea.jpg#2 | . relevant_image_names = list(relevant_pairs[&quot;image_file_name&quot;]) relevant_caption_ids = list(relevant_pairs[&quot;caption_id&quot;]) . plot_images_and_captions( image_names=relevant_image_names, caption_ids=relevant_caption_ids, relative_image_folder=&quot;flicker8k/Flicker8k_Dataset/&quot;, wrap_value=50 ) . Irrelevant sample (score = 1) . irrelevant_pairs = experts_agreement[experts_agreement[&quot;expert&quot;] == 1].sample(4, random_state=675)[[&quot;image_file_name&quot;, &quot;caption_id&quot;]] irrelevant_pairs . image_file_name caption_id . 4184 3461041826_0e24cdf597.jpg | 150387174_24825cf871.jpg#2 | . 3833 3349451628_4249a21c8f.jpg | 1764955991_5e53a28c87.jpg#2 | . 377 1446053356_a924b4893f.jpg | 2542662402_d781dd7f7c.jpg#2 | . 5274 468102269_135938e209.jpg | 166507476_9be5b9852a.jpg#2 | . irrelevant_image_names = list(irrelevant_pairs[&quot;image_file_name&quot;]) irrelevant_caption_ids = list(irrelevant_pairs[&quot;caption_id&quot;]) . plot_images_and_captions( image_names=irrelevant_image_names, caption_ids=irrelevant_caption_ids, relative_image_folder=&quot;flicker8k/Flicker8k_Dataset/&quot;, wrap_value=50 ) . Score = 2 (Caption describes minor aspects of the image) . pairs_score_2 = experts_agreement[experts_agreement[&quot;expert&quot;] == 2].sample(4, random_state=675)[[&quot;image_file_name&quot;, &quot;caption_id&quot;]] image_names_score_2 = list(pairs_score_2[&quot;image_file_name&quot;]) caption_ids_score_2 = list(pairs_score_2[&quot;caption_id&quot;]) . plot_images_and_captions( image_names=image_names_score_2, caption_ids=caption_ids_score_2, relative_image_folder=&quot;flicker8k/Flicker8k_Dataset/&quot;, wrap_value=50 ) . Score = 3 (Caption almost describes the images with minor mistakes) . pairs_score_3 = experts_agreement[experts_agreement[&quot;expert&quot;] == 3].sample(4, random_state=675)[[&quot;image_file_name&quot;, &quot;caption_id&quot;]] image_names_score_3 = list(pairs_score_3[&quot;image_file_name&quot;]) caption_ids_score_3 = list(pairs_score_3[&quot;caption_id&quot;]) . plot_images_and_captions( image_names=image_names_score_3, caption_ids=caption_ids_score_3, relative_image_folder=&quot;flicker8k/Flicker8k_Dataset/&quot;, wrap_value=50 ) .",
            "url": "https://thigm85.github.io/blog/flicker8k/dataset/image/nlp/2021/10/21/flicker8k-dataset-first-exploration.html",
            "relUrl": "/flicker8k/dataset/image/nlp/2021/10/21/flicker8k-dataset-first-exploration.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Generate images from video with imageio",
            "content": "From video to frames . Use imageio library to get frames out of a video. I used a sample.mp4 video as the example. Install imageio with pip install imageio-ffmpeg to use the video-format support of the library. . import os import imageio import matplotlib.pyplot as plt os.environ[&quot;VIDEO_PATH&quot;] = &quot;data/2021-10-20-from-video-to-image/sample.mp4&quot; os.environ[&quot;IMAGE_PATH&quot;] = &quot;data/2021-10-20-from-video-to-image/sample_frame.jpeg&quot; . I set 1 frame per second and the image size to be (1080, 1920). If the size parameter is not specified the reader will infer from the video file. . reader = imageio.get_reader( os.environ[&quot;VIDEO_PATH&quot;], fps=1, size=(1080, 1920) ) . The frame size for reading (1080, 1920) is different from the source frame size (1920, 1080). . Get the frames out of the video: . frames = [] for i, im in enumerate(reader): frames.append(im) print(&#39;Mean of frame %i is %1.1f&#39; % (i, im.mean())) . Mean of frame 0 is 128.8 Mean of frame 1 is 128.7 Mean of frame 2 is 128.4 Mean of frame 3 is 120.3 Mean of frame 4 is 122.5 Mean of frame 5 is 128.6 Mean of frame 6 is 132.4 Mean of frame 7 is 134.1 Mean of frame 8 is 137.4 Mean of frame 9 is 137.0 Mean of frame 10 is 135.7 Mean of frame 11 is 131.2 Mean of frame 12 is 131.9 . Frame data type . The frames are of type imageio.core.util.Array. We can convert them to numpy array if necessary with np.array. . frames[0].__class__ . imageio.core.util.Array . import numpy as np image = np.asarray(im) image.__class__ . numpy.ndarray . Crop a square in the center of the image . def crop_center_square(frame): y, x = frame.shape[0:2] min_dim = min(y, x) start_x = (x // 2) - (min_dim // 2) start_y = (y // 2) - (min_dim // 2) return frame[start_y : start_y + min_dim, start_x : start_x + min_dim] . Example: . i = 0 plt.figure(figsize=(10, 10)) ax = plt.subplot(1, 2, 1) plt.imshow(frames[i]) ax = plt.subplot(1, 2, 2) plt.imshow(crop_center_square(frames[0])) . &lt;matplotlib.image.AxesImage at 0x148fe09d0&gt; . Plot cropped frames . i = 0 plt.figure(figsize=(10, 10)) for idx, im in enumerate(frames[0:12]): ax = plt.subplot(3, 4, idx + 1) plt.imshow(crop_center_square(frames[idx])) . Save/load a frame . Save: . imageio.imwrite(os.environ[&quot;IMAGE_PATH&quot;], frames[0]) . Load: . image = imageio.imread(os.environ[&quot;IMAGE_PATH&quot;]) plt.figure(figsize=(10, 10)) plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x149aafe80&gt; .",
            "url": "https://thigm85.github.io/blog/imageio/video%20processing/image%20processing/2021/10/20/from-video-to-images.html",
            "relUrl": "/imageio/video%20processing/image%20processing/2021/10/20/from-video-to-images.html",
            "date": " • Oct 20, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Multi-label classification encoding with TensorFlow",
            "content": "Multi-label classication problems happen when an observation can belong to more than one class. They happen quite often in practice, one example being video classification. In order to solve multi-label classification problems with TensorFlow, we need to be able to express the label variable using multi-hot encoding. For the sake of this post, assume our classification has 5 classes and that each observation can belong to one or more classes. . Requirement . import tensorflow as tf . Tensorflow version used: . print(tf.__version__) . 2.6.0 . Single observation . Here is a single observation that belong to the second and third class. . indice = tf.constant([1, 2]) # We want to generate [0, 1, 1, 0, 0] . one_hot = tf.one_hot(indices=indice, depth=5) multi_hot = tf.reduce_max(one_hot, axis = 0) # reduce across axis = 0 . one_hot.shape . TensorShape([2, 5]) . multi_hot . &lt;tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 1., 1., 0., 0.], dtype=float32)&gt; . Batch of observation . indices = tf.ragged.constant([[1, 2], [1], [3, 2]]) # We want [ # [0, 1, 1, 0, 0], # [0, 1, 0, 0, 0], # [0, 0, 1, 1, 0] # ] . one_hot = tf.one_hot(indices=indices, depth=5) multi_hot = tf.reduce_max(one_hot, axis = 1) # reduce across axis = 1 . one_hot.shape . TensorShape([3, None, 5]) . multi_hot . &lt;tf.Tensor: shape=(3, 5), dtype=float32, numpy= array([[0., 1., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 1., 1., 0.]], dtype=float32)&gt; .",
            "url": "https://thigm85.github.io/blog/tensorflow/multi-label%20classification/2021/10/18/multi-label-encoding-tensorflow.html",
            "relUrl": "/tensorflow/multi-label%20classification/2021/10/18/multi-label-encoding-tensorflow.html",
            "date": " • Oct 18, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Create TensorFlow Dataset from TFRecord files",
            "content": "Play around with Youtube 8M video-level dataset. The goal of this section is to create a tf.data.Dataset from a set of .tfrecords file. . Requirements . This code works with tensorflow 2.6.0. . import tensorflow as tf . print(tf.__version__) . 2.6.0 . Load data . The sample data were downloaded with . curl data.yt8m.org/download.py | shard=1,100 partition=2/video/train mirror=us python . per instruction available on the YouTube 8M dataset download page. . Load raw dataset . Import libraries and specify data_folder. . import os import glob from tensorflow.data import TFRecordDataset . data_folder = &quot;/home/default/video&quot; . List .tfrecord files to be loaded. . filenames = glob.glob(os.path.join(data_folder, &quot;*.tfrecord&quot;)) print(filenames[0]); print(filenames[-1]) . /home/default/video/train0093.tfrecord /home/default/video/train3749.tfrecord . Load .tfrecord files into a raw (not parsed) dataset. . raw_dataset = tf.data.TFRecordDataset(filenames) . Parse raw dataset . Create a funtion to parse the raw data. According to YouTube 8M dataset download section, the video-level data are stored as tensorflow.Example protocol buffers with the following text format: . features: { feature: { key : &quot;id&quot; value: { bytes_list: { value: (Video id) } } } feature: { key : &quot;labels&quot; value: { int64_list: { value: [1, 522, 11, 172] # label list } } } feature: { # Average of all &#39;rgb&#39; features for the video key : &quot;mean_rgb&quot; value: { float_list: { value: [1024 float features] } } } feature: { # Average of all &#39;audio&#39; features for the video key : &quot;mean_audio&quot; value: { float_list: { value: [128 float features] } } } } . # Create a description of the features. feature_description = { &#39;id&#39;: tf.io.FixedLenFeature([1], tf.string, default_value=&#39;&#39;), &#39;labels&#39;: tf.io.FixedLenSequenceFeature([], tf.int64, default_value=0, allow_missing=True), &#39;mean_audio&#39;: tf.io.FixedLenFeature([128], tf.float32, default_value=[0.0] * 128), &#39;mean_rgb&#39;: tf.io.FixedLenFeature([1024], tf.float32, default_value=[0.0] * 1024), } def _parse_function(example_proto): # Parse the input `tf.train.Example` proto using the dictionary above. return tf.io.parse_single_example(example_proto, feature_description) . parsed_dataset = raw_dataset.map(_parse_function) parsed_dataset . &lt;MapDataset shapes: {id: (1,), labels: (None,), mean_audio: (128,), mean_rgb: (1024,)}, types: {id: tf.string, labels: tf.int64, mean_audio: tf.float32, mean_rgb: tf.float32}&gt; . Check parsed dataset . for parsed_record in parsed_dataset.take(1): print(repr(parsed_record)) . {&#39;id&#39;: &lt;tf.Tensor: shape=(1,), dtype=string, numpy=array([b&#39;eXbF&#39;], dtype=object)&gt;, &#39;labels&#39;: &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 0, 12])&gt;, &#39;mean_audio&#39;: &lt;tf.Tensor: shape=(128,), dtype=float32, numpy= array([-1.2556146 , 0.17297305, 0.53898615, 1.5446128 , 1.4344678 , 0.41190457, 1.2042887 , 0.9899097 , -0.28567997, 1.1892846 , 0.6182132 , -0.54916394, -0.02003632, 0.7124445 , -1.275734 , -1.0121363 , 0.8652152 , 0.45430297, -0.5905393 , -0.8244694 , 0.95853716, 0.379509 , -1.1317158 , 0.46737486, 1.3991169 , -0.4367456 , -0.287044 , -0.7412639 , 0.5608105 , 0.9686536 , 0.36370906, 0.15887815, 1.1279035 , -0.08369077, -0.20577091, -1.467152 , -0.9784904 , 0.44680086, 1.1796227 , 0.14648826, 1.3656982 , 0.12989263, -0.9865609 , -1.2897152 , 0.6123024 , 0.1184121 , 0.49931577, -1.1900278 , 0.0516886 , 0.16899465, -1.0225939 , -0.6807922 , -1.1495618 , 0.5336437 , -0.10267343, -0.14041142, -0.20417954, 0.37166587, 0.56979036, 0.7668918 , 0.17683779, -0.4835771 , 0.188432 , 0.948989 , 0.59286505, 0.7839421 , -0.29659215, 0.06305546, 0.13159767, 1.1180142 , 0.85737205, 1.5399523 , -0.28511164, -0.49676266, 0.21741751, -0.85834265, 0.88090146, -0.7543358 , 0.4161103 , -0.19713208, -0.13404599, 0.9562638 , 0.3493868 , 0.9435329 , -0.8736879 , -0.2188428 , -0.2544211 , 0.24140158, 0.31994662, 0.69403017, -1.1273963 , -0.801281 , 0.04793753, -0.69943386, 0.8120182 , -0.28852168, -0.16166747, 0.94978464, -1.2834635 , 0.32062864, -0.66567427, 1.2626008 , -1.583094 , -0.97621703, 1.3589919 , 0.43338794, -0.5152907 , -1.63595 , -0.4190133 , -0.16496386, -0.81412554, -0.22532192, -0.28386128, -0.4277658 , -0.7794566 , 0.16581193, -1.0593089 , -0.03117585, 0.0952237 , 1.1476818 , -0.28931737, -0.7578596 , -0.48096272, 0.36552775, 0.35063717, 0.5677443 , 1.4371959 , 0.81667864], dtype=float32)&gt;, &#39;mean_rgb&#39;: &lt;tf.Tensor: shape=(1024,), dtype=float32, numpy= array([ 0.5198898 , 0.30175963, -0.5135856 , ..., 0.44089007, 0.398037 , -0.48050806], dtype=float32)&gt;} .",
            "url": "https://thigm85.github.io/blog/youtube%208m/video%20data/tensorflow/2021/10/08/youtube-8m-video-level.html",
            "relUrl": "/youtube%208m/video%20data/tensorflow/2021/10/08/youtube-8m-video-level.html",
            "date": " • Oct 8, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Faster document operations with pyvespa",
            "content": "We will demonstrate that the new default implementation for document operations in pyvespa are now much faster due to async mode. . Define Question Answering app . from vespa.gallery import QuestionAnswering app_package = QuestionAnswering() . Deploy to Vespa Cloud . import os from vespa.package import VespaCloud # define your own WORK_DIR and VESPA_CLOUD_USER_KEY env variables disk_folder = os.path.join(os.getenv(&quot;WORK_DIR&quot;), &quot;sample_application&quot;) vespa_cloud = VespaCloud( tenant=&quot;vespa-team&quot;, application=&quot;pyvespa-integration&quot;, key_content=os.getenv(&quot;VESPA_CLOUD_USER_KEY&quot;).replace(r&quot; n&quot;, &quot; n&quot;), application_package=app_package, ) app = vespa_cloud.deploy( instance=&quot;msmarco&quot;, disk_folder=os.path.join(os.getenv(&quot;WORK_DIR&quot;), &quot;sample_application&quot;) ) . Deployment started in run 243 of dev-aws-us-east-1c for vespa-team.pyvespa-integration.msmarco. This may take about 15 minutes the first time. INFO [20:02:15] Deploying platform version 7.452.11 and application version unknown ... INFO [20:02:17] Deployment successful. INFO [20:02:17] Session 43999 for tenant &#39;vespa-team&#39; prepared and activated. INFO [20:02:17] ######## Details for all nodes ######## INFO [20:02:17] h5591e.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP INFO [20:02:17] platform vespa/cloud-tenant-rhel8:7.452.11 INFO [20:02:17] container-clustercontroller on port 19050 has config generation 43999, wanted is 43999 INFO [20:02:17] h5580f.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP INFO [20:02:17] platform vespa/cloud-tenant-rhel8:7.452.11 INFO [20:02:17] distributor on port 19111 has config generation 43997, wanted is 43999 INFO [20:02:17] searchnode on port 19107 has config generation 43999, wanted is 43999 INFO [20:02:17] storagenode on port 19102 has config generation 43997, wanted is 43999 INFO [20:02:17] h5577a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP INFO [20:02:17] platform vespa/cloud-tenant-rhel8:7.452.11 INFO [20:02:17] logserver-container on port 4080 has config generation 43999, wanted is 43999 INFO [20:02:17] h5592d.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP INFO [20:02:17] platform vespa/cloud-tenant-rhel8:7.452.11 INFO [20:02:17] container on port 4080 has config generation 43999, wanted is 43999 INFO [20:02:31] Found endpoints: INFO [20:02:31] - dev.aws-us-east-1c INFO [20:02:31] |-- https://qa-container.msmarco.pyvespa-integration.vespa-team.aws-us-east-1c.dev.z.vespa-app.cloud/ (cluster &#39;qa_container&#39;) INFO [20:02:32] Installation succeeded! Finished deployment. . Load sample data . import json, requests sentence_data = json.loads( requests.get(&quot;https://data.vespa.oath.cloud/blog/qa/sample_sentence_data_100.json&quot;).text ) . batch_feed = [ { &quot;id&quot;: idx, &quot;fields&quot;: sentence } for idx, sentence in enumerate(sentence_data) ] . Feed data . Asynchronous feeding - new default . import time start = time.time() response = app.feed_batch(schema=&quot;sentence&quot;, batch=batch_feed) print(&quot;{} seconds&quot;.format(time.time() - start)) . 1.146148920059204 seconds . Syncronous feeding - old default . start = time.time() response = app.feed_batch(schema=&quot;sentence&quot;, batch=batch_feed, asynchronous=False) print(&quot;{} seconds&quot;.format(time.time() - start)) . 68.66188383102417 seconds . Conclusion . Basic document operations are now much faster in pyvespa due to the new async implementation. This includes feed, get, update and delete operations. .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/speed/async/2021/08/22/demonstrate-asynchronous-operations.html",
            "relUrl": "/vespa/pyvespa/speed/async/2021/08/22/demonstrate-asynchronous-operations.html",
            "date": " • Aug 22, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Vespa stateless model evaluation",
            "content": "Text classification task with the Huggingface inference API . Hosted API . import requests API_URL = &quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot; headers = {&quot;Authorization&quot;: &quot;Bearer api_mxMKsfJoFDmvPdPziZLGuymSBQMbxVYoWg&quot;} def query(payload): response = requests.post(API_URL, headers=headers, json=payload) return response.json() . output = query({&quot;inputs&quot;: &quot;This is a test&quot;}) output . [[{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9670119881629944}, {&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.032987985759973526}]] . First time using the API, it returns an error indicating the it is loading the model. . {&#39;error&#39;: &#39;Model distilbert-base-uncased-finetuned-sst-2-english is currently loading&#39;, &#39;estimated_time&#39;: 20} . Local with their python API . from transformers import Pipeline, AutoTokenizer, AutoModelForSequenceClassification pipeline = Pipeline( tokenizer=AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;), model=AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;) ) . import math logits = pipeline(&quot;This is a test&quot;).tolist()[0] [math.exp(x)/(math.exp(logits[0])+math.exp(logits[1])) for x in logits] . [0.9670119572344816, 0.032988042765518436] . Vespa stateless model api - the bare minumum . Create the application package and deploy . **Note**: The step below assumes you exported a model to ONNX from vespa.package import ModelServer model_server = ModelServer( name=&quot;bert_model_server&quot;, model_file_path = &quot;data/2021-08-10-stateless-model-api/bert_tiny.onnx&quot; ) . Could just as easily deploy to Vespa Cloud: . from vespa.deployment import VespaDocker disk_folder = &quot;/Users/tmartins/model_server_docker&quot; vespa_docker = VespaDocker(disk_folder=disk_folder, port=8081) app = vespa_docker.deploy(application_package=model_server) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Finished deployment. . Interact with the application with Vespa REST api . Get the models available . !curl -s &#39;http://localhost:8081/model-evaluation/v1/&#39; . {&#34;bert_tiny&#34;:&#34;http://localhost:8080/model-evaluation/v1/bert_tiny&#34;} . Get information about a specific model . !curl -s &#39;http://localhost:8081/model-evaluation/v1/bert_tiny&#39; . {&#34;model&#34;:&#34;bert_tiny&#34;,&#34;functions&#34;:[{&#34;function&#34;:&#34;output_0&#34;,&#34;info&#34;:&#34;http://localhost:8080/model-evaluation/v1/bert_tiny/output_0&#34;,&#34;eval&#34;:&#34;http://localhost:8080/model-evaluation/v1/bert_tiny/output_0/eval&#34;,&#34;arguments&#34;:[{&#34;name&#34;:&#34;input_ids&#34;,&#34;type&#34;:&#34;tensor(d0[],d1[])&#34;},{&#34;name&#34;:&#34;attention_mask&#34;,&#34;type&#34;:&#34;tensor(d0[],d1[])&#34;},{&#34;name&#34;:&#34;token_type_ids&#34;,&#34;type&#34;:&#34;tensor(d0[],d1[])&#34;}]}]} . Write custom code to generate url encoded inputs . **Note**: Writing custom code to get the inputs right is messy and does not allow to improve speed unless the users writes their own custom java searcher. tokenizer = AutoTokenizer.from_pretrained( &quot;google/bert_uncased_L-2_H-128_A-2&quot; ) . from urllib.parse import urlencode def create_url_encoded_input(text, tokenizer): tokens = tokenizer(text) encoded_tokens = urlencode( { key: &quot;{&quot; + &quot;,&quot;.join( [ &quot;{{d0: 0, d1: {}}}: {}&quot;.format(idx, x) for idx, x in enumerate(value) ] ) + &quot;}&quot; for key, value in tokens.items() } ) return encoded_tokens . encoded_tokens = create_url_encoded_input(&quot;this is a text&quot;, tokenizer) encoded_tokens . &#39;input_ids=%7B%7Bd0%3A+0%2C+d1%3A+0%7D%3A+101%2C%7Bd0%3A+0%2C+d1%3A+1%7D%3A+2023%2C%7Bd0%3A+0%2C+d1%3A+2%7D%3A+2003%2C%7Bd0%3A+0%2C+d1%3A+3%7D%3A+1037%2C%7Bd0%3A+0%2C+d1%3A+4%7D%3A+3793%2C%7Bd0%3A+0%2C+d1%3A+5%7D%3A+102%7D&amp;token_type_ids=%7B%7Bd0%3A+0%2C+d1%3A+0%7D%3A+0%2C%7Bd0%3A+0%2C+d1%3A+1%7D%3A+0%2C%7Bd0%3A+0%2C+d1%3A+2%7D%3A+0%2C%7Bd0%3A+0%2C+d1%3A+3%7D%3A+0%2C%7Bd0%3A+0%2C+d1%3A+4%7D%3A+0%2C%7Bd0%3A+0%2C+d1%3A+5%7D%3A+0%7D&amp;attention_mask=%7B%7Bd0%3A+0%2C+d1%3A+0%7D%3A+1%2C%7Bd0%3A+0%2C+d1%3A+1%7D%3A+1%2C%7Bd0%3A+0%2C+d1%3A+2%7D%3A+1%2C%7Bd0%3A+0%2C+d1%3A+3%7D%3A+1%2C%7Bd0%3A+0%2C+d1%3A+4%7D%3A+1%2C%7Bd0%3A+0%2C+d1%3A+5%7D%3A+1%7D&#39; . Use the encoded tokens to get a prediction from Vespa: . from requests import get get(&quot;http://localhost:8081/model-evaluation/v1/bert_tiny/output_0/eval?{}&quot;.format(encoded_tokens)).json() . {&#39;cells&#39;: [{&#39;address&#39;: {&#39;d0&#39;: &#39;0&#39;, &#39;d1&#39;: &#39;0&#39;}, &#39;value&#39;: -0.02798202447593212}, {&#39;address&#39;: {&#39;d0&#39;: &#39;0&#39;, &#39;d1&#39;: &#39;1&#39;}, &#39;value&#39;: -0.1420438140630722}]} . Clean up the environment . import shutil shutil.rmtree(disk_folder, ignore_errors=True) vespa_docker.container.stop() vespa_docker.container.remove() . Vespa stateless model - full python api . Define the task and the model to use: . from vespa.ml import SequenceClassification model = SequenceClassification( model_id=&quot;bert_tiny&quot;, model=&quot;google/bert_uncased_L-2_H-128_A-2&quot; ) . Create the application package, no need to manually export the model. . from vespa.package import ModelServer model_server = ModelServer( name=&quot;bert_model_server&quot;, models=[model], ) . Deploy, could just as easily deploy to Vespa Cloud: . from vespa.deployment import VespaDocker disk_folder = &quot;/Users/tmartins/model_server_docker&quot; vespa_docker = VespaDocker(disk_folder=disk_folder, port=8081) app = vespa_docker.deploy(application_package=model_server) . Using framework PyTorch: 1.7.1 Found input input_ids with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;} Found input token_type_ids with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;} Found input attention_mask with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;} Found output output_0 with shape: {0: &#39;batch&#39;} Ensuring inputs are in correct order position_ids is not present in the generated input list. Generated inputs order: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;token_type_ids&#39;] Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Finished deployment. . Get models available: . app.get_model_endpoint() . {&#39;bert_tiny&#39;: &#39;http://localhost:8080/model-evaluation/v1/bert_tiny&#39;} . Get information about a specific model: . app.get_model_endpoint(&quot;bert_tiny&quot;) . {&#39;model&#39;: &#39;bert_tiny&#39;, &#39;functions&#39;: [{&#39;function&#39;: &#39;output_0&#39;, &#39;info&#39;: &#39;http://localhost:8080/model-evaluation/v1/bert_tiny/output_0&#39;, &#39;eval&#39;: &#39;http://localhost:8080/model-evaluation/v1/bert_tiny/output_0/eval&#39;, &#39;arguments&#39;: [{&#39;name&#39;: &#39;input_ids&#39;, &#39;type&#39;: &#39;tensor(d0[],d1[])&#39;}, {&#39;name&#39;: &#39;attention_mask&#39;, &#39;type&#39;: &#39;tensor(d0[],d1[])&#39;}, {&#39;name&#39;: &#39;token_type_ids&#39;, &#39;type&#39;: &#39;tensor(d0[],d1[])&#39;}]}]} . Get a prediction: . app.predict(x=&quot;this is a test&quot;, model_name=&quot;bert_tiny&quot;) . [0.009904447011649609, 0.04607260227203369] .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/stateless%20model%20evaluation/huggingface/2021/08/10/stateless-model-api.html",
            "relUrl": "/vespa/pyvespa/stateless%20model%20evaluation/huggingface/2021/08/10/stateless-model-api.html",
            "date": " • Aug 10, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Build a News recommendation app from python with Vespa",
            "content": "This part of the series introduces a new ranking signal: category click-through rate (CTR). The idea is that we can recommend popular content for users that don’t have a click history yet. Rather than just recommending based on articles, we recommend based on categories. However, these global CTR values can often change continuously, so we need an efficient way to update this value for all documents. We’ll do that by introducing parent-child relationships between documents in Vespa. We will also use sparse tensors directly in ranking. This post replicates this more detailed Vespa tutorial. . We assume that you have followed the part2 of the news recommendation tutorial. Therefore, you should have an app_package variable holding the news app definition and a Docker container named news running the application fed with data from the demo version of the MIND dataset. . Setting up a global category CTR document . If we add a category_ctr field in the news document, we would have to update all the sport&#39;s documents every time there is a change in the sport&#39;s CTR statistic. If we assume that the category CTR will change often, this turns out to be inefficient. . For these cases, Vespa introduced the parent-child relationship. Parents are global documents, which are automatically distributed to all content nodes. Other documents can reference these parents and “import” values for use in ranking. The benefit is that the global category CTR values only need to be written to one place: the global document. . from vespa.package import Schema, Document, Field app_package.add_schema( Schema( name=&quot;category_ctr&quot;, global_document=True, document=Document( fields=[ Field( name=&quot;ctrs&quot;, type=&quot;tensor&lt;float&gt;(category{})&quot;, indexing=[&quot;attribute&quot;], attribute=[&quot;fast-search&quot;] ), ] ) ) ) . We implement that by creating a new category_ctr schema and setting global_document=True to indicate that we want Vespa to keep a copy of these documents on all content nodes. Setting a document to be global is required for using it in a parent-child relationship. Note that we use a tensor with a single sparse dimension to hold the ctrs data. . Sparse tensors have strings as dimension addresses rather than a numeric index. More concretely, an example of such a tensor is (using the tensor literal form): . { {category: entertainment}: 0.2 }, {category: news}: 0.3 }, {category: sports}: 0.5 }, {category: travel}: 0.4 }, {category: finance}: 0.1 }, ... } . This tensor holds all the CTR scores for all the categories. When updating this tensor, we can update individual cells, and we don’t need to update the whole tensor. This operation is called tensor modify and can be helpful when you have large tensors. . Importing parent values in child documents . We need to set up two things to use the category_ctr tensor for ranking news documents. We need to reference the parent document (category_ctr in this case) and import the ctrs from the referenced parent document. . app_package.get_schema(&quot;news&quot;).add_fields( Field( name=&quot;category_ctr_ref&quot;, type=&quot;reference&lt;category_ctr&gt;&quot;, indexing=[&quot;attribute&quot;], ) ) . The field category_ctr_ref is a field of type reference of the category_ctr document type. When feeding this field, Vespa expects the fully qualified document id. For instance, if our global CTR document has the id id:category_ctr:category_ctr::global, that is the value that we need to feed to the category_ctr_ref field. A document can reference many parent documents. . from vespa.package import ImportedField app_package.get_schema(&quot;news&quot;).add_imported_field( ImportedField( name=&quot;global_category_ctrs&quot;, reference_field=&quot;category_ctr_ref&quot;, field_to_import=&quot;ctrs&quot;, ) ) . The imported field defines that we should import the ctrs field from the document referenced in the category_ctr_ref field. We name this as global_category_ctrs, and we can reference this as attribute(global_category_ctrs) during ranking. . Tensor expressions in ranking . Each news document has a category field of type string indicating which category the document belongs to. We want to use this information to select the correct CTR score stored in the global_category_ctrs. Unfortunately, tensor expressions only work on tensors, so we need to add a new field of type tensor called category_tensor to hold category information in a way that can be used in a tensor expression: . app_package.get_schema(&quot;news&quot;).add_fields( Field( name=&quot;category_tensor&quot;, type=&quot;tensor&lt;float&gt;(category{})&quot;, indexing=[&quot;attribute&quot;], ) ) . With the category_tensor field as defined above, we can use the tensor expression sum(attribute(category_tensor) * attribute(global_category_ctrs)) to select the specific CTR related to the category of the document being ranked. We implement this expression as a Function in the rank-profile below: . app_package.get_schema(&quot;news&quot;).add_rank_profile( RankProfile( name=&quot;recommendation_with_global_category_ctr&quot;, inherits=&quot;recommendation&quot;, functions=[ Function( name=&quot;category_ctr&quot;, expression=&quot;sum(attribute(category_tensor) * attribute(global_category_ctrs))&quot; ), Function( name=&quot;nearest_neighbor&quot;, expression=&quot;closeness(field, embedding)&quot; ) ], first_phase=&quot;nearest_neighbor * category_ctr&quot;, summary_features=[ &quot;attribute(category_tensor)&quot;, &quot;attribute(global_category_ctrs)&quot;, &quot;category_ctr&quot;, &quot;nearest_neighbor&quot; ] ) ) . In the new rank-profile, we have added a first phase ranking expression that multiplies the nearest-neighbor score with the category CTR score, implemented with the functions nearest_neighbor and category_ctr, respectively. As a first attempt, we just multiply the nearest-neighbor with the category CTR score, which might not be the best way to combine those two values. . Deploy . We can reuse the same container named news created in the first part of this tutorial. . from vespa.package import VespaDocker vespa_docker = VespaDocker.from_container_name_or_id(&quot;news&quot;) app = vespa_docker.deploy(application_package=app_package) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. Waiting for application status. Finished deployment. . Feed . Next, we will download the global category CTR data, already parsed in the format that is expected by a sparse tensor with the category dimension. . import requests, json global_category_ctr = json.loads( requests.get(&quot;https://data.vespa.oath.cloud/blog/news/global_category_ctr_parsed.json&quot;).text ) global_category_ctr . {&#39;ctrs&#39;: {&#39;cells&#39;: [{&#39;address&#39;: {&#39;category&#39;: &#39;entertainment&#39;}, &#39;value&#39;: 0.029266420380943244}, {&#39;address&#39;: {&#39;category&#39;: &#39;autos&#39;}, &#39;value&#39;: 0.028475809103747123}, {&#39;address&#39;: {&#39;category&#39;: &#39;tv&#39;}, &#39;value&#39;: 0.05374837981352176}, {&#39;address&#39;: {&#39;category&#39;: &#39;health&#39;}, &#39;value&#39;: 0.03531784305129329}, {&#39;address&#39;: {&#39;category&#39;: &#39;sports&#39;}, &#39;value&#39;: 0.05611187986670051}, {&#39;address&#39;: {&#39;category&#39;: &#39;music&#39;}, &#39;value&#39;: 0.05471192953054426}, {&#39;address&#39;: {&#39;category&#39;: &#39;news&#39;}, &#39;value&#39;: 0.04420778372641991}, {&#39;address&#39;: {&#39;category&#39;: &#39;foodanddrink&#39;}, &#39;value&#39;: 0.029256852366228187}, {&#39;address&#39;: {&#39;category&#39;: &#39;travel&#39;}, &#39;value&#39;: 0.025144552013730358}, {&#39;address&#39;: {&#39;category&#39;: &#39;finance&#39;}, &#39;value&#39;: 0.03231013195899643}, {&#39;address&#39;: {&#39;category&#39;: &#39;lifestyle&#39;}, &#39;value&#39;: 0.04423279317474416}, {&#39;address&#39;: {&#39;category&#39;: &#39;video&#39;}, &#39;value&#39;: 0.04006693315980292}, {&#39;address&#39;: {&#39;category&#39;: &#39;movies&#39;}, &#39;value&#39;: 0.03335647459420146}, {&#39;address&#39;: {&#39;category&#39;: &#39;weather&#39;}, &#39;value&#39;: 0.04532171803495617}, {&#39;address&#39;: {&#39;category&#39;: &#39;northamerica&#39;}, &#39;value&#39;: 0.0}, {&#39;address&#39;: {&#39;category&#39;: &#39;kids&#39;}, &#39;value&#39;: 0.043478260869565216}]}} . We can feed this data point to the document defined in the category_ctr. We will assign the global id to this document. Reference to this document can be done by using the Vespa id id:category_ctr:category_ctr::global. . response = app.feed_data_point(schema=&quot;category_ctr&quot;, data_id=&quot;global&quot;, fields=global_category_ctr) . We need to perform a partial update on the news documents to include information about the reference field category_ctr_ref and the new category_tensor that will have the value 1.0 for the specific category associated with each document. . news_category_ctr = json.loads( requests.get(&quot;https://data.vespa.oath.cloud/blog/news/news_category_ctr_update_parsed.json&quot;).text ) news_category_ctr[0] . {&#39;id&#39;: &#39;N3112&#39;, &#39;fields&#39;: {&#39;category_ctr_ref&#39;: &#39;id:category_ctr:category_ctr::global&#39;, &#39;category_tensor&#39;: {&#39;cells&#39;: [{&#39;address&#39;: {&#39;category&#39;: &#39;lifestyle&#39;}, &#39;value&#39;: 1.0}]}}} . for data_point in news_category_ctr: app.update_data(schema=&quot;news&quot;, data_id=data_point[&quot;id&quot;], fields=data_point[&quot;fields&quot;]) . Testing the new rank-profile . We will redefine the query_user_embedding function defined in the second part of this tutorial and use it to make a query involving the user U33527 and the recommendation_with_global_category_ctr rank-profile. . def parse_embedding(hit_json): embedding_json = hit_json[&quot;fields&quot;][&quot;embedding&quot;][&quot;cells&quot;] embedding_vector = [0.0] * len(embedding_json) for val in embedding_json: embedding_vector[int(val[&quot;address&quot;][&quot;d0&quot;])] = val[&quot;value&quot;] return embedding_vector def query_user_embedding(user_id): result = app.query(body={&quot;yql&quot;: &quot;select * from sources user where user_id contains &#39;{}&#39;;&quot;.format(user_id)}) embedding = parse_embedding(result.hits[0]) return embedding . yql = &quot;select * from sources news where &quot; &quot;([{&#39;targetHits&#39;: 10}]nearestNeighbor(embedding, user_embedding));&quot; result = app.query( body={ &quot;yql&quot;: yql, &quot;hits&quot;: 10, &quot;ranking.features.query(user_embedding)&quot;: str(query_user_embedding(user_id=&quot;U33527&quot;)), &quot;ranking.profile&quot;: &quot;recommendation_with_global_category_ctr&quot; } ) . The first hit below is a sports article. The global CTR document is also listed here, and the CTR score for the sports category is 0.0561. Thus, the result of the category_ctr function is 0.0561 as intended. The nearest_neighbor score is 0.149, and the resulting relevance score is 0.00836. So, this worked as expected. . result.hits[0] . {&#39;id&#39;: &#39;id:news:news::N5316&#39;, &#39;relevance&#39;: 0.008369192847921151, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;news&#39;, &#39;documentid&#39;: &#39;id:news:news::N5316&#39;, &#39;news_id&#39;: &#39;N5316&#39;, &#39;category&#39;: &#39;sports&#39;, &#39;subcategory&#39;: &#39;football_nfl&#39;, &#39;title&#39;: &#34;Matthew Stafford&#39;s status vs. Bears uncertain, Sam Martin will play&#34;, &#39;abstract&#39;: &#34;Stafford&#39;s start streak could be in jeopardy, according to Ian Rapoport.&#34;, &#39;url&#39;: &#34;https://www.msn.com/en-us/sports/football_nfl/matthew-stafford&#39;s-status-vs.-bears-uncertain,-sam-martin-will-play/ar-BBWwcVN?ocid=chopendata&#34;, &#39;date&#39;: 20191112, &#39;clicks&#39;: 0, &#39;impressions&#39;: 1, &#39;summaryfeatures&#39;: {&#39;attribute(category_tensor)&#39;: {&#39;type&#39;: &#39;tensor&lt;float&gt;(category{})&#39;, &#39;cells&#39;: [{&#39;address&#39;: {&#39;category&#39;: &#39;sports&#39;}, &#39;value&#39;: 1.0}]}, &#39;attribute(global_category_ctrs)&#39;: {&#39;type&#39;: &#39;tensor&lt;float&gt;(category{})&#39;, &#39;cells&#39;: [{&#39;address&#39;: {&#39;category&#39;: &#39;entertainment&#39;}, &#39;value&#39;: 0.029266420751810074}, {&#39;address&#39;: {&#39;category&#39;: &#39;autos&#39;}, &#39;value&#39;: 0.0284758098423481}, {&#39;address&#39;: {&#39;category&#39;: &#39;tv&#39;}, &#39;value&#39;: 0.05374838039278984}, {&#39;address&#39;: {&#39;category&#39;: &#39;health&#39;}, &#39;value&#39;: 0.03531784191727638}, {&#39;address&#39;: {&#39;category&#39;: &#39;sports&#39;}, &#39;value&#39;: 0.05611187964677811}, {&#39;address&#39;: {&#39;category&#39;: &#39;music&#39;}, &#39;value&#39;: 0.05471193045377731}, {&#39;address&#39;: {&#39;category&#39;: &#39;news&#39;}, &#39;value&#39;: 0.04420778527855873}, {&#39;address&#39;: {&#39;category&#39;: &#39;foodanddrink&#39;}, &#39;value&#39;: 0.029256852343678474}, {&#39;address&#39;: {&#39;category&#39;: &#39;travel&#39;}, &#39;value&#39;: 0.025144552811980247}, {&#39;address&#39;: {&#39;category&#39;: &#39;finance&#39;}, &#39;value&#39;: 0.032310131937265396}, {&#39;address&#39;: {&#39;category&#39;: &#39;lifestyle&#39;}, &#39;value&#39;: 0.044232793152332306}, {&#39;address&#39;: {&#39;category&#39;: &#39;video&#39;}, &#39;value&#39;: 0.040066931396722794}, {&#39;address&#39;: {&#39;category&#39;: &#39;movies&#39;}, &#39;value&#39;: 0.033356472849845886}, {&#39;address&#39;: {&#39;category&#39;: &#39;weather&#39;}, &#39;value&#39;: 0.045321717858314514}, {&#39;address&#39;: {&#39;category&#39;: &#39;northamerica&#39;}, &#39;value&#39;: 0.0}, {&#39;address&#39;: {&#39;category&#39;: &#39;kids&#39;}, &#39;value&#39;: 0.043478261679410934}]}, &#39;rankingExpression(category_ctr)&#39;: 0.05611187964677811, &#39;rankingExpression(nearest_neighbor)&#39;: 0.14915188666574342, &#39;vespa.summaryFeatures.cached&#39;: 0.0}}} . Conclusion . This tutorial introduced parent-child relationships and demonstrated it through a global CTR feature we used in ranking. We also introduced ranking with (sparse) tensor expressions. .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/news%20recommendation/mind/2021/05/13/news-parent-child.html",
            "relUrl": "/vespa/pyvespa/news%20recommendation/mind/2021/05/13/news-parent-child.html",
            "date": " • May 13, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Build sentence/paragraph level QA application from python with Vespa",
            "content": "We will walk through the steps necessary to create a question answering (QA) application that can retrieve sentence or paragraph level answers based on a combination of semantic and/or term-based search. We start by discussing the dataset used and the question and sentence embeddings generated for semantic search. We then include the steps necessary to create and deploy a Vespa application to serve the answers. We make all the required data available to feed the application and show how to query for sentence and paragraph level answers based on a combination of semantic and term-based search. . This tutorial is based on earlier work by the Vespa team to reproduce the results of the paper ReQA: An Evaluation for End-to-End Answer Retrieval Models by Ahmad Et al. using the Stanford Question Answering Dataset (SQuAD) v1.1 dataset. . About the data . We are going to use the Stanford Question Answering Dataset (SQuAD) v1.1 dataset. The data contains paragraphs (denoted here as context), and each paragraph has questions that have answers in the associated paragraph. We have parsed the dataset and organized the data that we will use in this tutorial to make it easier to follow along. . Paragraph . import requests, json context_data = json.loads( requests.get(&quot;https://data.vespa.oath.cloud/blog/qa/qa_squad_context_data.json&quot;).text ) . Each context data point contains a context_id that uniquely identifies a paragraph, a text field holding the paragraph string, and a questions field holding a list of question ids that can be answered from the paragraph text. We also include a dataset field to identify the data source if we want to index more than one dataset in our application. . context_data[0] . {&#39;text&#39;: &#39;Architecturally, the school has a Catholic character. Atop the Main Building &#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34;Venite Ad Me Omnes&#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.&#39;, &#39;dataset&#39;: &#39;squad&#39;, &#39;questions&#39;: [0, 1, 2, 3, 4], &#39;context_id&#39;: 0} . Questions . According to the data point above, context_id = 0 can be used to answer the questions with id = [0, 1, 2, 3, 4]. We can load the file containing the questions and display those first five questions. . from pandas import read_csv # Note that squad_queries.txt has approx. 1 Gb due to the 512-sized question embeddings questions = read_csv( filepath_or_buffer=&quot;https://data.vespa.oath.cloud/blog/qa/squad_queries.txt&quot;, sep=&quot; t&quot;, names=[&quot;question_id&quot;, &quot;question&quot;, &quot;number_answers&quot;, &quot;embedding&quot;] ) . questions[[&quot;question_id&quot;, &quot;question&quot;]].head() . question_id question . 0 0 | To whom did the Virgin Mary allegedly appear i... | . 1 1 | What is in front of the Notre Dame Main Building? | . 2 2 | The Basilica of the Sacred heart at Notre Dame... | . 3 3 | What is the Grotto at Notre Dame? | . 4 4 | What sits on top of the Main Building at Notre... | . Paragraph sentences . To build a more accurate application, we can break the paragraphs down into sentences. For example, the first sentence below comes from the paragraph with context_id = 0 and can answer the question with question_id = 4. . # Note that qa_squad_sentence_data.json has approx. 1 Gb due to the 512-sized sentence embeddings sentence_data = json.loads( requests.get(&quot;https://data.vespa.oath.cloud/blog/qa/qa_squad_sentence_data.json&quot;).text ) . {k:sentence_data[0][k] for k in [&quot;text&quot;, &quot;dataset&quot;, &quot;questions&quot;, &quot;context_id&quot;]} . {&#39;text&#39;: &#34;Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary.&#34;, &#39;dataset&#39;: &#39;squad&#39;, &#39;questions&#39;: [4], &#39;context_id&#39;: 0} . Embeddings . We want to combine semantic (dense) and term-based (sparse) signals to answer the questions sent to our application. We have generated embeddings for both the questions and the sentences to implement the semantic search, each having size equal to 512. . questions[[&quot;question_id&quot;, &quot;embedding&quot;]].head(1) . question_id embedding . 0 0 | [-0.025649750605225563, -0.01708591915667057, ... | . sentence_data[0][&quot;sentence_embedding&quot;][&quot;values&quot;][0:5] # display the first five elements . [-0.005731593817472458, 0.007575507741421461, -0.06413306295871735, -0.007967847399413586, -0.06464996933937073] . Here is the script containing the code that we used to generate the sentence and questions embeddings. We used Google&#39;s Universal Sentence Encoder at the time but feel free to replace it with embeddings generated by your preferred model. . Create and deploy the application . We can now build a sentence-level Question answering application based on the data described above. . Schema to hold context information . The context schema will have a document containing the four relevant fields described in the data section. We create an index for the text field and use enable-bm25 to pre-compute data required to speed up the use of BM25 for ranking. The summary indexing indicates that all the fields will be included in the requested context documents. The attribute indexing store the fields in memory as an attribute for sorting, querying, and grouping. . from vespa.package import Document, Field context_document = Document( fields=[ Field(name=&quot;questions&quot;, type=&quot;array&lt;int&gt;&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;]), Field(name=&quot;dataset&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;]), Field(name=&quot;context_id&quot;, type=&quot;int&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;]), Field(name=&quot;text&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;, &quot;index&quot;], index=&quot;enable-bm25&quot;), ] ) . The default fieldset means query tokens will be matched against the text field by default. We defined two rank-profiles (bm25 and nativeRank) to illustrate that we can define and experiment with as many rank-profiles as we want. You can create different ones using the ranking expressions and features available. . from vespa.package import Schema, FieldSet, RankProfile context_schema = Schema( name=&quot;context&quot;, document=context_document, fieldsets=[FieldSet(name=&quot;default&quot;, fields=[&quot;text&quot;])], rank_profiles=[ RankProfile(name=&quot;bm25&quot;, inherits=&quot;default&quot;, first_phase=&quot;bm25(text)&quot;), RankProfile(name=&quot;nativeRank&quot;, inherits=&quot;default&quot;, first_phase=&quot;nativeRank(text)&quot;)] ) . Schema to hold sentence information . The document of the sentence schema will inherit the fields defined in the context document to avoid unnecessary duplication of the same field types. Besides, we add the sentence_embedding field defined to hold a one-dimensional tensor of floats of size 512. We will store the field as an attribute in memory and build an ANN index using the HNSW (hierarchical navigable small world) algorithm. Read this blog post to know more about Vespa’s journey to implement ANN search and the documentation for more information about the HNSW parameters. . from vespa.package import HNSW sentence_document = Document( inherits=&quot;context&quot;, fields=[ Field( name=&quot;sentence_embedding&quot;, type=&quot;tensor&lt;float&gt;(x[512])&quot;, indexing=[&quot;attribute&quot;, &quot;index&quot;], ann=HNSW( distance_metric=&quot;euclidean&quot;, max_links_per_node=16, neighbors_to_explore_at_insert=500 ) ) ] ) . For the sentence schema, we define three rank profiles. The semantic-similarity uses the Vespa closeness ranking feature, which is defined as 1/(1 + distance) so that sentences with embeddings closer to the question embedding will be ranked higher than sentences that are far apart. The bm25 is an example of a term-based rank profile, and bm25-semantic-similarity combines both term-based and semantic-based signals as an example of a hybrid approach. . sentence_schema = Schema( name=&quot;sentence&quot;, document=sentence_document, fieldsets=[FieldSet(name=&quot;default&quot;, fields=[&quot;text&quot;])], rank_profiles=[ RankProfile( name=&quot;semantic-similarity&quot;, inherits=&quot;default&quot;, first_phase=&quot;closeness(sentence_embedding)&quot; ), RankProfile( name=&quot;bm25&quot;, inherits=&quot;default&quot;, first_phase=&quot;bm25(text)&quot; ), RankProfile( name=&quot;bm25-semantic-similarity&quot;, inherits=&quot;default&quot;, first_phase=&quot;bm25(text) + closeness(sentence_embedding)&quot; ) ] ) . Build the application package . We can now define our qa application by creating an application package with both the context_schema and the sentence_schema that we defined above. In addition, we need to inform Vespa that we plan to send a query ranking feature named query_embedding with the same type that we used to define the sentence_embedding field. . from vespa.package import ApplicationPackage, QueryProfile, QueryProfileType, QueryTypeField app_package = ApplicationPackage( name=&quot;qa&quot;, schema=[context_schema, sentence_schema], query_profile=QueryProfile(), query_profile_type=QueryProfileType( fields=[ QueryTypeField( name=&quot;ranking.features.query(query_embedding)&quot;, type=&quot;tensor&lt;float&gt;(x[512])&quot; ) ] ) ) . Deploy the application . We can deploy the app_package in a Docker container (or to Vespa Cloud): . from vespa.package import VespaDocker vespa_docker = VespaDocker( port=8081, container_memory=&quot;8G&quot;, disk_folder=&quot;/Users/username/qa_app&quot; # requires absolute path ) app = vespa_docker.deploy(application_package=app_package) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. Waiting for application status. Finished deployment. . Vespa(http://localhost, 8081) . Feed the data . Once deployed, we can use the Vespa instance app to interact with the application. We can start by feeding context and sentence data. . for idx, sentence in enumerate(sentence_data): app.feed_data_point(schema=&quot;sentence&quot;, data_id=idx, fields=sentence) . for context in context_data: app.feed_data_point(schema=&quot;context&quot;, data_id=context[&quot;context_id&quot;], fields=context) . Sentence level retrieval . The query below sends the first question embedding (questions.loc[0, &quot;embedding&quot;]) through the ranking.features.query(query_embedding) parameter and use the nearestNeighbor search operator to retrieve the closest 100 sentences in embedding space using Euclidean distance as configured in the HNSW settings. The sentences returned will be ranked by the semantic-similarity rank profile defined in the sentence schema. . result = app.query(body={ &#39;yql&#39;: &#39;select * from sources sentence where ([{&quot;targetNumHits&quot;:100}]nearestNeighbor(sentence_embedding,query_embedding));&#39;, &#39;hits&#39;: 100, &#39;ranking.features.query(query_embedding)&#39;: questions.loc[0, &quot;embedding&quot;], &#39;ranking.profile&#39;: &#39;semantic-similarity&#39; }) . result.hits[0] . {&#39;id&#39;: &#39;id:sentence:sentence::2&#39;, &#39;relevance&#39;: 0.5540203635649571, &#39;source&#39;: &#39;qa_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;sentence&#39;, &#39;documentid&#39;: &#39;id:sentence:sentence::2&#39;, &#39;questions&#39;: [0], &#39;dataset&#39;: &#39;squad&#39;, &#39;context_id&#39;: 0, &#39;text&#39;: &#39;It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.&#39;}} . Sentence level hybrid retrieval . In addition to sending the query embedding, we can send the question string (questions.loc[0, &quot;question&quot;]) via the query parameter and use the or operator to retrieve documents that satisfy either the semantic operator nearestNeighbor or the term-based operator userQuery. Choosing type equal any means that the term-based operator will retrieve all the documents that match at least one query token. The retrieved documents will be ranked by the hybrid rank-profile bm25-semantic-similarity. . result = app.query(body={ &#39;yql&#39;: &#39;select * from sources sentence where ([{&quot;targetNumHits&quot;:100}]nearestNeighbor(sentence_embedding,query_embedding)) or userQuery();&#39;, &#39;query&#39;: questions.loc[0, &quot;question&quot;], &#39;type&#39;: &#39;any&#39;, &#39;hits&#39;: 100, &#39;ranking.features.query(query_embedding)&#39;: questions.loc[0, &quot;embedding&quot;], &#39;ranking.profile&#39;: &#39;bm25-semantic-similarity&#39; }) . result.hits[0] . {&#39;id&#39;: &#39;id:sentence:sentence::2&#39;, &#39;relevance&#39;: 44.46252359752296, &#39;source&#39;: &#39;qa_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;sentence&#39;, &#39;documentid&#39;: &#39;id:sentence:sentence::2&#39;, &#39;questions&#39;: [0], &#39;dataset&#39;: &#39;squad&#39;, &#39;context_id&#39;: 0, &#39;text&#39;: &#39;It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.&#39;}} . Paragraph level retrieval . For paragraph-level retrieval, we use Vespa&#39;s grouping feature to retrieve paragraphs instead of sentences. In the sample query below, we group by context_id and use the paragraph’s max sentence score to represent the paragraph level score. We limit the number of paragraphs returned by 3, and each paragraph contains at most two sentences. We return all the summary features for each sentence. All those configurations can be changed to fit different use cases. . result = app.query(body={ &#39;yql&#39;: (&#39;select * from sources sentence where ([{&quot;targetNumHits&quot;:10000}]nearestNeighbor(sentence_embedding,query_embedding)) |&#39; &#39;all(group(context_id) max(3) order(-max(relevance())) each( max(2) each(output(summary())) as(sentences)) as(paragraphs));&#39;), &#39;hits&#39;: 0, &#39;ranking.features.query(query_embedding)&#39;: questions.loc[0, &quot;embedding&quot;], &#39;ranking.profile&#39;: &#39;sentence-semantic-similarity&#39; }) . paragraphs = result.json[&quot;root&quot;][&quot;children&quot;][0][&quot;children&quot;][0] . paragraphs[&quot;children&quot;][0] # top-ranked paragraph . {&#39;id&#39;: &#39;group:long:0&#39;, &#39;relevance&#39;: 1.0, &#39;value&#39;: &#39;0&#39;, &#39;children&#39;: [{&#39;id&#39;: &#39;hitlist:sentences&#39;, &#39;relevance&#39;: 1.0, &#39;label&#39;: &#39;sentences&#39;, &#39;continuation&#39;: {&#39;next&#39;: &#39;BKAAAAABGBEBC&#39;}, &#39;children&#39;: [{&#39;id&#39;: &#39;id:sentence:sentence::2&#39;, &#39;relevance&#39;: 0.5540203635649571, &#39;source&#39;: &#39;qa_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;sentence&#39;, &#39;documentid&#39;: &#39;id:sentence:sentence::2&#39;, &#39;questions&#39;: [0], &#39;dataset&#39;: &#39;squad&#39;, &#39;context_id&#39;: 0, &#39;text&#39;: &#39;It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.&#39;}}, {&#39;id&#39;: &#39;id:sentence:sentence::0&#39;, &#39;relevance&#39;: 0.4668025534074384, &#39;source&#39;: &#39;qa_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;sentence&#39;, &#39;documentid&#39;: &#39;id:sentence:sentence::0&#39;, &#39;questions&#39;: [4], &#39;dataset&#39;: &#39;squad&#39;, &#39;context_id&#39;: 0, &#39;text&#39;: &#34;Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary.&#34;}}]}]} . paragraphs[&quot;children&quot;][1] # second-ranked paragraph . {&#39;id&#39;: &#39;group:long:28&#39;, &#39;relevance&#39;: 0.6666666666666666, &#39;value&#39;: &#39;28&#39;, &#39;children&#39;: [{&#39;id&#39;: &#39;hitlist:sentences&#39;, &#39;relevance&#39;: 1.0, &#39;label&#39;: &#39;sentences&#39;, &#39;continuation&#39;: {&#39;next&#39;: &#39;BKAAABCABGBEBC&#39;}, &#39;children&#39;: [{&#39;id&#39;: &#39;id:sentence:sentence::188&#39;, &#39;relevance&#39;: 0.5209270028414069, &#39;source&#39;: &#39;qa_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;sentence&#39;, &#39;documentid&#39;: &#39;id:sentence:sentence::188&#39;, &#39;questions&#39;: [142], &#39;dataset&#39;: &#39;squad&#39;, &#39;context_id&#39;: 28, &#39;text&#39;: &#39;The Grotto of Our Lady of Lourdes, which was built in 1896, is a replica of the original in Lourdes, France.&#39;}}, {&#39;id&#39;: &#39;id:sentence:sentence::184&#39;, &#39;relevance&#39;: 0.4590959251360276, &#39;source&#39;: &#39;qa_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;sentence&#39;, &#39;documentid&#39;: &#39;id:sentence:sentence::184&#39;, &#39;questions&#39;: [140], &#39;dataset&#39;: &#39;squad&#39;, &#39;context_id&#39;: 28, &#39;text&#39;: &#39;It is built in French Revival style and it is decorated by stained glass windows imported directly from France.&#39;}}]}]} . Conclusion and future work . This work used Google&#39;s Universal Sentence Encoder to generate the embeddings. It would be nice to compare evaluation metrics with embeddings generated by the most recent Facebook&#39;s Dense Passage Retrieval methodology. This Vespa blog post uses DPR to reproduce the state-of-the-art baseline for retrieval-based question-answering systems within a single, scalable production-ready application. .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/qa/search/2021/04/07/semantic-retrieval-for-question-answering-applications.html",
            "relUrl": "/vespa/pyvespa/qa/search/2021/04/07/semantic-retrieval-for-question-answering-applications.html",
            "date": " • Apr 7, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Build a News recommendation app from python with Vespa",
            "content": "In this part, we&#39;ll start transforming our application from news search to news recommendation using the embeddings created in this tutorial. An embedding vector will represent each user and news article. We will make the embeddings used available for download to make it easier to follow this post along. When a user comes, we retrieve his embedding and use it to retrieve the closest news articles via an approximate nearest neighbor (ANN) search. We also show that Vespa can jointly apply general filtering and ANN search, unlike competing alternatives available in the market. . We assume that you have followed the news search tutorial. Therefore, you should have an app_package variable holding the news search app definition and a Docker container named news running a search application fed with news articles from the demo version of the MIND dataset. . Add a user schema . We need to add another document type to represent a user. We set up the schema to search for a user_id and retrieve the user’s embedding vector. . from vespa.package import Schema, Document, Field app_package.add_schema( Schema( name=&quot;user&quot;, document=Document( fields=[ Field( name=&quot;user_id&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;], attribute=[&quot;fast-search&quot;] ), Field( name=&quot;embedding&quot;, type=&quot;tensor&lt;float&gt;(d0[51])&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;] ) ] ) ) ) . We build an index for the attribute field user_id by specifying the fast-search attribute. Remember that attribute fields are held in memory and are not indexed by default. . The embedding field is a tensor field. Tensors in Vespa are flexible multi-dimensional data structures and, as first-class citizens, can be used in queries, document fields, and constants in ranking. Tensors can be either dense or sparse or both and can contain any number of dimensions. Please see the tensor user guide for more information. Here we have defined a dense tensor with a single dimension (d0 - dimension 0), representing a vector. 51 is the size of the embeddings used in this post. . We now have one schema for the news and one schema for the user. . [schema.name for schema in app_package.schemas] . [&#39;news&#39;, &#39;user&#39;] . Index news embeddings . Similarly to the user schema, we will use a dense tensor to represent the news embeddings. But unlike the user embedding field, we will index the news embedding by including index in the indexing argument and specify that we want to build the index using the HNSW (hierarchical navigable small world) algorithm. The distance metric used is euclidean. Read this blog post to know more about Vespa’s journey to implement ANN search. . from vespa.package import Field, HNSW app_package.get_schema(name=&quot;news&quot;).add_fields( Field( name=&quot;embedding&quot;, type=&quot;tensor&lt;float&gt;(d0[51])&quot;, indexing=[&quot;attribute&quot;, &quot;index&quot;], ann=HNSW(distance_metric=&quot;euclidean&quot;) ) ) . Recommendation using embeddings . Here, we’ve added a ranking expression using the closeness ranking feature, which calculates the euclidean distance and uses that to rank the news articles. This rank-profile depends on using the nearestNeighbor search operator, which we’ll get back to below when searching. But for now, this expects a tensor in the query to use as the initial search point. . from vespa.package import RankProfile app_package.get_schema(name=&quot;news&quot;).add_rank_profile( RankProfile( name=&quot;recommendation&quot;, inherits=&quot;default&quot;, first_phase=&quot;closeness(field, embedding)&quot; ) ) . Query Profile Type . The recommendation rank profile above requires that we send a tensor along with the query. For Vespa to bind the correct types, it needs to know the expected type of this query parameter. . from vespa.package import QueryTypeField app_package.query_profile_type.add_fields( QueryTypeField( name=&quot;ranking.features.query(user_embedding)&quot;, type=&quot;tensor&lt;float&gt;(d0[51])&quot; ) ) . This query profile type instructs Vespa to expect a float tensor with dimension d0[51] when the query parameter ranking.features.query(user_embedding) is passed. We’ll see how this works together with the nearestNeighbor search operator below. . Redeploy the application . We made all the required changes to turn our news search app into a news recommendation app. We can now redeploy the app_package to our running container named news. . from vespa.package import VespaDocker vespa_docker = VespaDocker.from_container_name_or_id(&quot;news&quot;) app = vespa_docker.deploy(application_package=app_package) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Finished deployment. . app.deployment_message . [&#34;Uploading application &#39;/app/application&#39; using http://localhost:19071/application/v2/tenant/default/session&#34;, &#34;Session 7 for tenant &#39;default&#39; created.&#34;, &#39;Preparing session 7 using http://localhost:19071/application/v2/tenant/default/session/7/prepared&#39;, &#34;WARNING: Host named &#39;news&#39; may not receive any config since it is not a canonical hostname. Disregard this warning when testing in a Docker container.&#34;, &#34;Session 7 for tenant &#39;default&#39; prepared.&#34;, &#39;Activating session 7 using http://localhost:19071/application/v2/tenant/default/session/7/active&#39;, &#34;Session 7 for tenant &#39;default&#39; activated.&#34;, &#39;Checksum: 62d964000c4ff4a5280b342cd8d95c80&#39;, &#39;Timestamp: 1616671116728&#39;, &#39;Generation: 7&#39;, &#39;&#39;] . Feeding and partial updates: news and user embeddings . To keep this tutorial easy to follow, we make the parsed embeddings available for download. To build them yourself, please follow this tutorial. . import requests, json user_embeddings = json.loads( requests.get(&quot;https://thigm85.github.io/data/mind/mind_demo_user_embeddings_parsed.json&quot;).text ) news_embeddings = json.loads( requests.get(&quot;https://thigm85.github.io/data/mind/mind_demo_news_embeddings_parsed.json&quot;).text ) . We just created the user schema, so we need to feed user data for the first time. . for user_embedding in user_embeddings: response = app.feed_data_point( schema=&quot;user&quot;, data_id=user_embedding[&quot;user_id&quot;], fields=user_embedding ) . For the news documents, we just need to update the embedding field added to the news schema. . for news_embedding in news_embeddings: response = app.update_data( schema=&quot;news&quot;, data_id=news_embedding[&quot;news_id&quot;], fields={&quot;embedding&quot;: news_embedding[&quot;embedding&quot;]} ) . Fetch the user embedding . Next, we create a query_user_embedding function to retrieve the user embedding by the user_id. Of course, you could do this more efficiently using a Vespa Searcher as described here, but keeping everything in python at this point makes learning easier. . def parse_embedding(hit_json): embedding_json = hit_json[&quot;fields&quot;][&quot;embedding&quot;][&quot;cells&quot;] embedding_vector = [0.0] * len(embedding_json) for val in embedding_json: embedding_vector[int(val[&quot;address&quot;][&quot;d0&quot;])] = val[&quot;value&quot;] return embedding_vector def query_user_embedding(user_id): result = app.query(body={&quot;yql&quot;: &quot;select * from sources user where user_id contains &#39;{}&#39;;&quot;.format(user_id)}) embedding = parse_embedding(result.hits[0]) return embedding . The function will query Vespa, retrieve the embedding and parse it into a list of floats. Here are the first five elements of the user U63195&#39;s embedding. . query_user_embedding(user_id=&quot;U63195&quot;)[:5] . [0.0, -0.1694680005311966, -0.0703359991312027, -0.03539799898862839, 0.14579899609088898] . Get recommendations . ANN search . The following yql instructs Vespa to select the title and the category from the ten news documents closest to the user embedding. . yql = &quot;select title, category from sources news where ([{&#39;targetHits&#39;: 10}]nearestNeighbor(embedding, user_embedding));&quot; . We also specify that we want to rank those documents by the recommendation rank-profile that we defined earlier and send the user embedding via the query profile type ranking.features.query(user_embedding) that we also defined in our app_package. . result = app.query( body={ &quot;yql&quot;: yql, &quot;hits&quot;: 10, &quot;ranking.features.query(user_embedding)&quot;: str(query_user_embedding(user_id=&quot;U63195&quot;)), &quot;ranking.profile&quot;: &quot;recommendation&quot; } ) . Here are the first two hits out of the ten returned. . result.hits[0:2] . [{&#39;id&#39;: &#39;index:news_content/0/aca03f4ba2274dd95b58db9a&#39;, &#39;relevance&#39;: 0.1460561756063909, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;category&#39;: &#39;music&#39;, &#39;title&#39;: &#39;Broadway Star Laurel Griggs Suffered Asthma Attack Before She Died at Age 13&#39;}}, {&#39;id&#39;: &#39;index:news_content/0/bd02238644c604f3a2d53364&#39;, &#39;relevance&#39;: 0.14591827245062294, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;category&#39;: &#39;tv&#39;, &#39;title&#39;: &#34;Rip Taylor&#39;s Cause of Death Revealed, Memorial Service Scheduled for Later This Month&#34;}}] . Combine ANN search with query filters . Vespa ANN search is fully integrated into the Vespa query tree. This integration means that we can include query filters and the ANN search will be applied only to documents that satisfy the filters. No need to do pre- or post-processing involving filters. . The following yql search over news documents that have sports as their category. . yql = &quot;select title, category from sources news where &quot; &quot;([{&#39;targetHits&#39;: 10}]nearestNeighbor(embedding, user_embedding)) AND &quot; &quot;category contains &#39;sports&#39;;&quot; . result = app.query( body={ &quot;yql&quot;: yql, &quot;hits&quot;: 10, &quot;ranking.features.query(user_embedding)&quot;: str(query_user_embedding(user_id=&quot;U63195&quot;)), &quot;ranking.profile&quot;: &quot;recommendation&quot; } ) . Here are the first two hits out of the ten returned. Notice the category field. . result.hits[0:2] . [{&#39;id&#39;: &#39;index:news_content/0/375ea340c21b3138fae1a05c&#39;, &#39;relevance&#39;: 0.14417346200569972, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;category&#39;: &#39;sports&#39;, &#39;title&#39;: &#39;Charles Rogers, former Michigan State football, Detroit Lions star, dead at 38&#39;}}, {&#39;id&#39;: &#39;index:news_content/0/2b892989020ddf7796dae435&#39;, &#39;relevance&#39;: 0.14404365847394848, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;category&#39;: &#39;sports&#39;, &#39;title&#39;: &#34;&#39;Monday Night Football&#39; commentator under fire after belittling criticism of 49ers kicker for missed field goal&#34;}}] .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/news%20recommendation/mind/2021/03/08/news-tutorial-embeddings.html",
            "relUrl": "/vespa/pyvespa/news%20recommendation/mind/2021/03/08/news-tutorial-embeddings.html",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "Build a News recommendation app from python with Vespa",
            "content": "We will build a news recommendation app in Vespa without leaving a python environment. In this first part of the series, we want to develop an application with basic search functionality. Future posts will add recommendation capabilities based on embeddings and other ML models. This series is a simplified version of Vespa&#39;s News search and recommendation tutorial. We will also use the demo version of the Microsoft News Dataset (MIND) so that anyone can follow along on their laptops. . Dataset . The original Vespa news search tutorial provides a script to download, parse and convert the MIND dataset to Vespa format. To make things easier for you, we made the final parsed data required for this tutorial available for download: . import requests, json data = json.loads( requests.get(&quot;https://thigm85.github.io/data/mind/mind_demo_fields_parsed.json&quot;).text ) data[0] . {&#39;abstract&#39;: &#34;Shop the notebooks, jackets, and more that the royals can&#39;t live without.&#34;, &#39;title&#39;: &#39;The Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By&#39;, &#39;subcategory&#39;: &#39;lifestyleroyals&#39;, &#39;news_id&#39;: &#39;N3112&#39;, &#39;category&#39;: &#39;lifestyle&#39;, &#39;url&#39;: &#39;https://www.msn.com/en-us/lifestyle/lifestyleroyals/the-brands-queen-elizabeth,-prince-charles,-and-prince-philip-swear-by/ss-AAGH0ET?ocid=chopendata&#39;, &#39;date&#39;: 20191103, &#39;clicks&#39;: 0, &#39;impressions&#39;: 0} . The final parsed data used here is a list where each element is a dictionary containing relevant fields about a news article such as title and category. We also have information about the number of impressions and clicks the article has received. The demo version of the mind dataset has 28.603 news articles included. . len(data) . 28603 . Install pyvespa . !pip install pyvespa . Create the search app . Create the application package. app_package will hold all the relevant data related to your application&#39;s specification. . from vespa.package import ApplicationPackage app_package = ApplicationPackage(name=&quot;news&quot;) . Add fields to the schema. Here is a short description of the non-obvious arguments used below: . indexing argument: configures the indexing pipeline for a field, which defines how Vespa will treat input during indexing. . &quot;index&quot;: Create a search index for this field. . | &quot;summary&quot;: Lets this field be part of the document summary in the result set. . | &quot;attribute&quot;: Store this field in memory as an attribute — for sorting, querying and grouping. . | . | index argument: configure how Vespa should create the search index. . &quot;enable-bm25&quot;: set up an index compatible with bm25 ranking for text search. | . | attribute argument: configure how Vespa should treat an attribute field. . &quot;fast-search&quot;: Build an index for an attribute field. By default, no index is generated for attributes, and search over these defaults to a linear scan. | . | . from vespa.package import Field app_package.schema.add_fields( Field(name=&quot;news_id&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;], attribute=[&quot;fast-search&quot;]), Field(name=&quot;category&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;]), Field(name=&quot;subcategory&quot;, type=&quot;string&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;]), Field(name=&quot;title&quot;, type=&quot;string&quot;, indexing=[&quot;index&quot;, &quot;summary&quot;], index=&quot;enable-bm25&quot;), Field(name=&quot;abstract&quot;, type=&quot;string&quot;, indexing=[&quot;index&quot;, &quot;summary&quot;], index=&quot;enable-bm25&quot;), Field(name=&quot;url&quot;, type=&quot;string&quot;, indexing=[&quot;index&quot;, &quot;summary&quot;]), Field(name=&quot;date&quot;, type=&quot;int&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;]), Field(name=&quot;clicks&quot;, type=&quot;int&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;]), Field(name=&quot;impressions&quot;, type=&quot;int&quot;, indexing=[&quot;summary&quot;, &quot;attribute&quot;]), ) . Add a fieldset to the schema. Fieldset allows us to search over multiple fields easily. In this case, searching over the default fieldset is equivalent to searching over title and abstract. . from vespa.package import FieldSet app_package.schema.add_field_set( FieldSet(name=&quot;default&quot;, fields=[&quot;title&quot;, &quot;abstract&quot;]) ) . We have enough to deploy the first version of our application. Later in this tutorial, we will include an article’s popularity into the relevance score used to rank the news that matches our queries. . Deploy the app on Docker . If you have Docker installed on your machine, you can deploy the app_package in a local Docker container: . from vespa.package import VespaDocker vespa_docker = VespaDocker( port=8080, container_memory=&quot;8G&quot;, disk_folder=&quot;/Users/tmartins/news&quot; # change for your desired absolute folder ) app = vespa_docker.deploy( application_package=app_package, ) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. Waiting for application status. Finished deployment. . vespa_docker will parse the app_package and write all the necessary Vespa config files to the disk_folder. It will then create the docker containers and use the Vespa config files to deploy the Vespa application. We can then use the app instance to interact with the deployed application, such as for feeding and querying. If you want to know more about what happens behind the scenes, we suggest you go through this getting started with Docker tutorial. . Feed data to the app . We can use the feed_data_point method. We need to specify: . data_id: unique id to identify the data point . | fields: dictionary with keys matching the field names defined in our application package schema. . | schema: name of the schema we want to feed data to. When we created an application package, we created a schema by default with the same name as the application name, news in our case. . | . for article in data: res = app.feed_data_point( data_id=article[&quot;news_id&quot;], fields=article, schema=&quot;news&quot; ) . Query the app . We can use the Vespa Query API through app.query to unlock the full query flexibility Vespa can offer. . Search over indexed fields using keywords . Select all the fields from documents where default (title or abstract) contains the keyword &#39;music&#39;. . res = app.query(body={&quot;yql&quot; : &quot;select * from sources * where default contains &#39;music&#39;;&quot;}) res.hits[0] . {&#39;id&#39;: &#39;index:news_content/0/5f1b30d14d4a15050dae9f7f&#39;, &#39;relevance&#39;: 0.25641557752127125, &#39;source&#39;: &#39;news_content&#39;} . Select title and abstract where title contains &#39;music&#39; and default contains &#39;festival&#39;. . res = app.query(body = {&quot;yql&quot; : &quot;select title, abstract from sources * where title contains &#39;music&#39; AND default contains &#39;festival&#39;;&quot;}) res.hits[0] . {&#39;id&#39;: &#39;index:news_content/0/988f76793a855e48b16dc5d3&#39;, &#39;relevance&#39;: 0.19587240022210403, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#34;At Least 3 Injured In Stampede At Travis Scott&#39;s Astroworld Music Festival&#34;, &#39;abstract&#39;: &#34;A stampede Saturday outside rapper Travis Scott&#39;s Astroworld musical festival in Houston, left three people injured. Minutes before the gates were scheduled to open at noon, fans began climbing over metal barricades and surged toward the entrance, according to local news reports.&#34;}} . Search by document type . Select the title of all the documents with document type equal to news. Our application has only one document type, so the query below retrieves all our documents. . res = app.query(body = {&quot;yql&quot; : &quot;select title from sources * where sddocname contains &#39;news&#39;;&quot;}) res.hits[0] . {&#39;id&#39;: &#39;index:news_content/0/698f73a87a936f1c773f2161&#39;, &#39;relevance&#39;: 0.0, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#39;The Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By&#39;}} . Search over attribute fields such as date . Since date is not specified with attribute=[&quot;fast-search&quot;] there is no index built for it. Therefore, search over it is equivalent to doing a linear scan over the values of the field. . res = app.query(body={&quot;yql&quot; : &quot;select title, date from sources * where date contains &#39;20191110&#39;;&quot;}) res.hits[0] . {&#39;id&#39;: &#39;index:news_content/0/debbdfe653c6d11f71cc2353&#39;, &#39;relevance&#39;: 0.0017429193899782135, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#39;These Cranberry Sauce Recipes Are Perfect for Thanksgiving Dinner&#39;, &#39;date&#39;: 20191110}} . Since the default fieldset is formed by indexed fields, Vespa will first filter by all the documents that contain the keyword &#39;weather&#39; within title or abstract, before scanning the date field for &#39;20191110&#39;. . res = app.query(body={&quot;yql&quot; : &quot;select title, abstract, date from sources * where default contains &#39;weather&#39; AND date contains &#39;20191110&#39;;&quot;}) res.hits[0] . {&#39;id&#39;: &#39;index:news_content/0/bb88325ae94d888c46538d0b&#39;, &#39;relevance&#39;: 0.27025156546141466, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#39;Weather forecast in St. Louis&#39;, &#39;abstract&#39;: &#34;What&#39;s the weather today? What&#39;s the weather for the week? Here&#39;s your forecast.&#34;, &#39;date&#39;: 20191110}} . We can also perform range searches: . res = app.query({&quot;yql&quot; : &quot;select date from sources * where date &lt;= 20191110 AND date &gt;= 20191108;&quot;}) res.hits[0] . {&#39;id&#39;: &#39;index:news_content/0/c41a873213fdcffbb74987c0&#39;, &#39;relevance&#39;: 0.0017429193899782135, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;date&#39;: 20191109}} . Sorting . By default, Vespa sorts the hits by descending relevance score. The relevance score is given by the nativeRank unless something else is specified, as we will do later in this post. . res = app.query(body={&quot;yql&quot; : &quot;select title, date from sources * where default contains &#39;music&#39;;&quot;}) res.hits[:2] . [{&#39;id&#39;: &#39;index:news_content/0/5f1b30d14d4a15050dae9f7f&#39;, &#39;relevance&#39;: 0.25641557752127125, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#39;Music is hot in Nashville this week&#39;, &#39;date&#39;: 20191101}}, {&#39;id&#39;: &#39;index:news_content/0/6a031d5eff95264c54daf56d&#39;, &#39;relevance&#39;: 0.23351089409559303, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#39;Apple Music Replay highlights your favorite tunes of the year&#39;, &#39;date&#39;: 20191105}}] . However, we can explicitly order by a given field with the order keyword. . res = app.query(body={&quot;yql&quot; : &quot;select title, date from sources * where default contains &#39;music&#39; order by date;&quot;}) res.hits[:2] . [{&#39;id&#39;: &#39;index:news_content/0/d0d7e1c080f0faf5989046d8&#39;, &#39;relevance&#39;: 0.0, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#34;Elton John&#39;s second farewell tour stop in Cleveland shows why he&#39;s still standing after all these years&#34;, &#39;date&#39;: 20191031}}, {&#39;id&#39;: &#39;index:news_content/0/abf7f6f46ff2a96862075155&#39;, &#39;relevance&#39;: 0.0, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#39;The best hair metal bands&#39;, &#39;date&#39;: 20191101}}] . order sorts in ascending order by default, we can override that with the desc keyword: . res = app.query(body={&quot;yql&quot; : &quot;select title, date from sources * where default contains &#39;music&#39; order by date desc;&quot;}) res.hits[:2] . [{&#39;id&#39;: &#39;index:news_content/0/934a8d976ff8694772009362&#39;, &#39;relevance&#39;: 0.0, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#39;Korg Minilogue XD update adds key triggers for synth sequences&#39;, &#39;date&#39;: 20191113}}, {&#39;id&#39;: &#39;index:news_content/0/4feca287fdfa1d027f61e7bf&#39;, &#39;relevance&#39;: 0.0, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#39;Tom Draper, Black Music Industry Pioneer, Dies at 79&#39;, &#39;date&#39;: 20191113}}] . Grouping . We can use Vespa&#39;s grouping feature to compute the three news categories with the highest number of document counts: . news with 9115 articles . | sports with 6765 articles . | finance with 1886 articles . | . res = app.query(body={&quot;yql&quot; : &quot;select * from sources * where sddocname contains &#39;news&#39; limit 0 | all(group(category) max(3) order(-count())each(output(count())));&quot;}) res.hits[0] . {&#39;id&#39;: &#39;group:root:0&#39;, &#39;relevance&#39;: 1.0, &#39;continuation&#39;: {&#39;this&#39;: &#39;&#39;}, &#39;children&#39;: [{&#39;id&#39;: &#39;grouplist:category&#39;, &#39;relevance&#39;: 1.0, &#39;label&#39;: &#39;category&#39;, &#39;continuation&#39;: {&#39;next&#39;: &#39;BGAAABEBGBC&#39;}, &#39;children&#39;: [{&#39;id&#39;: &#39;group:string:news&#39;, &#39;relevance&#39;: 1.0, &#39;value&#39;: &#39;news&#39;, &#39;fields&#39;: {&#39;count()&#39;: 9115}}, {&#39;id&#39;: &#39;group:string:sports&#39;, &#39;relevance&#39;: 0.6666666666666666, &#39;value&#39;: &#39;sports&#39;, &#39;fields&#39;: {&#39;count()&#39;: 6765}}, {&#39;id&#39;: &#39;group:string:finance&#39;, &#39;relevance&#39;: 0.3333333333333333, &#39;value&#39;: &#39;finance&#39;, &#39;fields&#39;: {&#39;count()&#39;: 1886}}]}]} . Use news popularity signal for ranking . Vespa uses nativeRank to compute relevance scores by default. We will create a new rank-profile that includes a popularity signal in our relevance score computation. . from vespa.package import RankProfile, Function app_package.schema.add_rank_profile( RankProfile( name=&quot;popularity&quot;, inherits=&quot;default&quot;, functions=[ Function( name=&quot;popularity&quot;, expression=&quot;if (attribute(impressions) &gt; 0, attribute(clicks) / attribute(impressions), 0)&quot; ) ], first_phase=&quot;nativeRank(title, abstract) + 10 * popularity&quot; ) ) . Our new rank-profile will be called popularity. Here is a breakdown of what is included above: . inherits=&quot;default&quot; | . This configures Vespa to create a new rank profile named popularity, which inherits all the default rank-profile properties; only properties that are explicitly defined, or overridden, will differ from those of the default rank-profile. . function popularity | . This sets up a function that can be called from other expressions. This function calculates the number of clicks divided by impressions for indicating popularity. However, this isn’t really the best way of calculating this, as an article with a low number of impressions can score high on such a value, even though uncertainty is high. But it is a start :) . first-phase | . Relevance calculations in Vespa are two-phased. The calculations done in the first phase are performed on every single document matching your query. In contrast, the second phase calculations are only done on the top n documents as determined by the calculations done in the first phase. We are just going to use the first-phase for now. . expression: nativeRank + 10 * popularity | . This expression is used to rank documents. Here, the default ranking expression — the nativeRank of the default fieldset — is included to make the query relevant, while the second term calls the popularity function. The weighted sum of these two terms is the final relevance for each document. Note that the weight here, 10, is set by observation. A better approach would be to learn such values using machine learning, which we&#39;ll get back to in future posts. . Redeploy the application . Since we have changed the application package, we need to redeploy our application: . app = vespa_docker.deploy( application_package=app_package, ) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. Waiting for application status. Finished deployment. . app.deployment_message . [&#34;Uploading application &#39;/app/application&#39; using http://localhost:19071/application/v2/tenant/default/session&#34;, &#34;Session 3 for tenant &#39;default&#39; created.&#34;, &#39;Preparing session 3 using http://localhost:19071/application/v2/tenant/default/session/3/prepared&#39;, &#34;WARNING: Host named &#39;news&#39; may not receive any config since it is not a canonical hostname. Disregard this warning when testing in a Docker container.&#34;, &#34;Session 3 for tenant &#39;default&#39; prepared.&#34;, &#39;Activating session 3 using http://localhost:19071/application/v2/tenant/default/session/3/active&#39;, &#34;Session 3 for tenant &#39;default&#39; activated.&#34;, &#39;Checksum: fa83365f9aacba5133026e09c3e42cea&#39;, &#39;Timestamp: 1615287349323&#39;, &#39;Generation: 3&#39;, &#39;&#39;] . Query using the new popularity signal . When the redeployment is complete, we can use it to rank the matched documents by using the ranking argument. . res = app.query(body={ &quot;yql&quot; : &quot;select * from sources * where default contains &#39;music&#39;;&quot;, &quot;ranking&quot; : &quot;popularity&quot; }) res.hits[0] . {&#39;id&#39;: &#39;id:news:news::N5870&#39;, &#39;relevance&#39;: 5.156596018746151, &#39;source&#39;: &#39;news_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;news&#39;, &#39;documentid&#39;: &#39;id:news:news::N5870&#39;, &#39;news_id&#39;: &#39;N5870&#39;, &#39;category&#39;: &#39;music&#39;, &#39;subcategory&#39;: &#39;musicnews&#39;, &#39;title&#39;: &#39;Country music group Alabama reschedules their Indy show until next October 2020&#39;, &#39;abstract&#39;: &#39;INDIANAPOLIS, Ind. Fans of the highly acclaimed country music group Alabama, scheduled to play Bankers Life Fieldhouse Saturday night, will have to wait until next year to see the group. The group famous for such notable songs like &#34;If You &#39;re Gonna Play in Texas&#34;, &#34;Love In The First Degree&#34;, and &#34;She and I&#34;, made the announcement that their 50th Anniversary Tour is being rescheduled till ...&#39;, &#39;url&#39;: &#39;https://www.msn.com/en-us/music/musicnews/country-music-group-alabama-reschedules-their-indy-show-until-next-october-2020/ar-BBWB0d7?ocid=chopendata&#39;, &#39;date&#39;: 20191108, &#39;clicks&#39;: 1, &#39;impressions&#39;: 2}} .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/news%20recommendation/mind/2021/03/02/news-tutorial-basic-search.html",
            "relUrl": "/vespa/pyvespa/news%20recommendation/mind/2021/03/02/news-tutorial-basic-search.html",
            "date": " • Mar 2, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Run search engine experiments in Vespa from python",
            "content": "pyvespa provides a python API to Vespa. The library’s primary goal is to allow for faster prototyping and facilitate Machine Learning experiments for Vespa applications. . There are three ways you can get value out of pyvespa: . You can connect to a running Vespa application. . | You can build and deploy a Vespa application using pyvespa API. . | You can deploy an application from Vespa config files stored on disk. . | We will review each of those methods. . Connect to a running Vespa application . In case you already have a Vespa application running somewhere, you can directly instantiate the Vespa class with the appropriate endpoint. The example below connects to the cord19.vespa.ai application: . from vespa.application import Vespa app = Vespa(url = &quot;https://api.cord19.vespa.ai&quot;) . We are then good to go and ready to interact with the application through pyvespa, e.g., to query: . app.query(body = { &#39;yql&#39;: &#39;select title from sources * where userQuery();&#39;, &#39;hits&#39;: 1, &#39;summary&#39;: &#39;short&#39;, &#39;timeout&#39;: &#39;1.0s&#39;, &#39;query&#39;: &#39;coronavirus temperature sensitivity&#39;, &#39;type&#39;: &#39;all&#39;, &#39;ranking&#39;: &#39;default&#39; }).hits . [{&#39;id&#39;: &#39;index:content/1/ad8f0a6204288c0d497399a2&#39;, &#39;relevance&#39;: 0.36920467353113595, &#39;source&#39;: &#39;content&#39;, &#39;fields&#39;: {&#39;title&#39;: &#39;&lt;hi&gt;Temperature&lt;/hi&gt; &lt;hi&gt;Sensitivity&lt;/hi&gt;: A Potential Method for the Generation of Vaccines against the Avian &lt;hi&gt;Coronavirus&lt;/hi&gt; Infectious Bronchitis Virus&#39;}}] . Build and deploy with pyvespa API . You can also build your Vespa application from scratch using the pyvespa API. Here is a simple example: . from vespa.package import ApplicationPackage, Field, RankProfile app_package = ApplicationPackage(name = &quot;sampleapp&quot;) app_package.schema.add_fields( Field( name=&quot;title&quot;, type=&quot;string&quot;, indexing=[&quot;index&quot;, &quot;summary&quot;], index=&quot;enable-bm25&quot;) ) app_package.schema.add_rank_profile( RankProfile( name=&quot;bm25&quot;, inherits=&quot;default&quot;, first_phase=&quot;bm25(title)&quot; ) ) . We can then deploy app_package to a Docker container (or directly to VespaCloud): . from vespa.package import VespaDocker vespa_docker = VespaDocker( disk_folder=&quot;/Users/username/sample_app&quot;, # chose your absolute folder container_memory=&quot;8G&quot;, port=8080 ) app = vespa_docker.deploy(application_package=app_package) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. Waiting for application status. Finished deployment. . app holds an instance of the Vespa class just like our first example, and we can use it to feed and query the application just deployed. We can also go to the Vespa configuration files stored in the disk_folder, modify them and deploy them directly from the disk using the method discussed in the next section. This can be useful when we want to fine-tune our application based on Vespa features not available through the pyvespa API. . There is also the possibility to explicitly export app_package to Vespa configuration files (without deploying them) through the export_application_package method: . vespa_docker.export_application_package(application_package=app_package) . Deploy from Vespa config files . pyvespa API provides a subset of the functionality available in Vespa. The reason is that pyvespa is meant to be used as an experimentation tool for Information Retrieval (IR) and not for building production-ready applications. So, the python API expands based on the needs we have to replicate common use cases that often require IR experimentation. . If your application requires functionality or fine-tuning not available in pyvespa, you simply build it directly through Vespa configuration files as shown in many examples on Vespa docs. But even in this case, you can still get value out of pyvespa by deploying it from python based on the Vespa configuration files stored on disk. To show that, we can clone and deploy the news search app covered in this Vespa tutorial: . !git clone https://github.com/vespa-engine/sample-apps.git . The Vespa configuration files of the news search app are stored in the sample-apps/news/app-3-searching/ folder: . !tree sample-apps/news/app-3-searching/ . sample-apps/news/app-3-searching/ ├── hosts.xml ├── schemas │   └── news.sd └── services.xml 1 directory, 3 files . We can then deploy to a Docker container from disk: . from vespa.package import VespaDocker vespa_docker_news = VespaDocker( disk_folder=&quot;/Users/username/sample-apps/news/app-3-searching/&quot;, container_memory=&quot;8G&quot;, port=8081 ) app = vespa_docker_news.deploy_from_disk(application_name=&quot;news&quot;) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. Waiting for application status. Finished deployment. . Again, app holds an instance of the Vespa class just like our first example, and we can use it to feed and query the application just deployed. . Final thoughts . We covered three different ways to connect to a Vespa application from python using the pyvespa library. Those methods provide great workflow flexibility. They allow you to quickly get started with pyvespa experimentation while enabling you to modify Vespa config files to include features not available in the pyvespa API without losing the ability to experiment with the added features. .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/2021/02/24/news-tutorial-getting-started-on-docker.html",
            "relUrl": "/vespa/pyvespa/2021/02/24/news-tutorial-getting-started-on-docker.html",
            "date": " • Feb 24, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Build a basic text search application from python with Vespa",
            "content": "This post will introduce you to the simplified pyvespa API that allows us to build a basic text search application from scratch with just a few code lines from python. Follow-up posts will add layers of complexity by incrementally building on top of the basic app described here. . pyvespa exposes a subset of Vespa API in python. The library’s primary goal is to allow for faster prototyping and facilitate Machine Learning experiments for Vespa applications. I have written about how we can use it to connect and interact with running Vespa applications and evaluate Vespa ranking functions from python. This time, we focus on building and deploying applications from scratch. . Install . The pyvespa simplified API introduced here was released on version 0.2.0 . pip3 install pyvespa&gt;=0.2.0 . Define the application . As an example, we will build an application to search through CORD19 sample data. . Create an application package . The first step is to create a Vespa ApplicationPackage: . from vespa.package import ApplicationPackage app_package = ApplicationPackage(name=&quot;cord19&quot;) . Add fields to the Schema . We can then add fields to the application&#39;s Schema created by default in app_package. . from vespa.package import Field app_package.schema.add_fields( Field( name = &quot;cord_uid&quot;, type = &quot;string&quot;, indexing = [&quot;attribute&quot;, &quot;summary&quot;] ), Field( name = &quot;title&quot;, type = &quot;string&quot;, indexing = [&quot;index&quot;, &quot;summary&quot;], index = &quot;enable-bm25&quot; ), Field( name = &quot;abstract&quot;, type = &quot;string&quot;, indexing = [&quot;index&quot;, &quot;summary&quot;], index = &quot;enable-bm25&quot; ) ) . cord_uid will store the cord19 document ids, while title and abstract are self explanatory. . | All the fields, in this case, are of type string. . | Including &quot;index&quot; in the indexing list means that Vespa will create a searchable index for title and abstract. You can read more about which options is available for indexing in the Vespa documentation. . | Setting index = &quot;enable-bm25&quot; makes Vespa pre-compute quantities to make it fast to compute the bm25 score. We will use BM25 to rank the documents retrieved. . | . Search multiple fields when querying . A Fieldset groups fields together for searching. For example, the default fieldset defined below groups title and abstract together. . from vespa.package import FieldSet app_package.schema.add_field_set( FieldSet(name = &quot;default&quot;, fields = [&quot;title&quot;, &quot;abstract&quot;]) ) . Define how to rank the documents matched . We can specify how to rank the matched documents by defining a RankProfile. In this case, we defined the bm25 rank profile that combines that BM25 scores computed over the title and abstract fields. . from vespa.package import RankProfile app_package.schema.add_rank_profile( RankProfile( name = &quot;bm25&quot;, first_phase = &quot;bm25(title) + bm25(abstract)&quot; ) ) . Deploy your application . We have now defined a basic text search app containing relevant fields, a fieldset to group fields together, and a rank profile to rank matched documents. It is time to deploy our application. We can locally deploy our app_package using Docker without leaving the notebook, by creating an instance of VespaDocker, as shown below: . from vespa.package import VespaDocker vespa_docker = VespaDocker( port=8080, disk_folder=&quot;/Users/username/cord19_app&quot; ) app = vespa_docker.deploy( application_package = app_package, ) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. . app now holds a Vespa instance, which we are going to use to interact with our application. Congratulations, you now have a Vespa application up and running. . It is important to know that pyvespa simply provides a convenient API to define Vespa application packages from python. vespa_docker.deploy export Vespa configuration files to the disk_folder defined above. Going through those files is an excellent way to start learning about Vespa syntax. . Feed some data . Our first action after deploying a Vespa application is usually to feed some data to it. To make it easier to follow, we have prepared a DataFrame containing 100 rows and the cord_uid, title, and abstract columns required by our schema definition. . from pandas import read_csv parsed_feed = read_csv( &quot;https://thigm85.github.io/data/cord19/parsed_feed_100.csv&quot; ) . parsed_feed . cord_uid title abstract . 0 ug7v899j | Clinical features of culture-proven Mycoplasma... | OBJECTIVE: This retrospective chart review des... | . 1 02tnwd4m | Nitric oxide: a pro-inflammatory mediator in l... | Inflammatory diseases of the respiratory tract... | . 2 ejv2xln0 | Surfactant protein-D and pulmonary host defense | Surfactant protein-D (SP-D) participates in th... | . 3 2b73a28n | Role of endothelin-1 in lung disease | Endothelin-1 (ET-1) is a 21 amino acid peptide... | . 4 9785vg6d | Gene expression in epithelial cells in respons... | Respiratory syncytial virus (RSV) and pneumoni... | . ... ... | ... | ... | . 95 63bos83o | Global Surveillance of Emerging Influenza Viru... | BACKGROUND: Effective influenza surveillance r... | . 96 hqc7u9w3 | Transmission Parameters of the 2001 Foot and M... | Despite intensive ongoing research, key aspect... | . 97 87zt7lew | Efficient replication of pneumonia virus of mi... | Pneumonia virus of mice (PVM; family Paramyxov... | . 98 wgxt36jv | Designing and conducting tabletop exercises to... | BACKGROUND: Since 2001, state and local health... | . 99 qbldmef1 | Transcript-level annotation of Affymetrix prob... | BACKGROUND: The wide use of Affymetrix microar... | . 100 rows × 3 columns . We can then iterate through the DataFrame above and feed each row by using the app.feed_data_point method: . The schema name is by default set to be equal to the application name, which is cord19 in this case. . | When feeding data to Vespa, we must have a unique id for each data point. We will use cord_uid here. . | . for idx, row in parsed_feed.iterrows(): fields = { &quot;cord_uid&quot;: str(row[&quot;cord_uid&quot;]), &quot;title&quot;: str(row[&quot;title&quot;]), &quot;abstract&quot;: str(row[&quot;abstract&quot;]) } response = app.feed_data_point( schema = &quot;cord19&quot;, data_id = str(row[&quot;cord_uid&quot;]), fields = fields, ) . You can also inspect the response to each request if desired. . response.json() . {&#39;pathId&#39;: &#39;/document/v1/cord19/cord19/docid/qbldmef1&#39;, &#39;id&#39;: &#39;id:cord19:cord19::qbldmef1&#39;} . Query your application . With data fed, we can start to query our text search app. We can use the Vespa Query language directly by sending the required parameters to the body argument of the app.query method. . query = { &#39;yql&#39;: &#39;select * from sources * where userQuery();&#39;, &#39;query&#39;: &#39;What is the role of endothelin-1&#39;, &#39;ranking&#39;: &#39;bm25&#39;, &#39;type&#39;: &#39;any&#39;, &#39;presentation.timing&#39;: True, &#39;hits&#39;: 3 } . res = app.query(body=query) res.hits[0] . {&#39;id&#39;: &#39;id:cord19:cord19::2b73a28n&#39;, &#39;relevance&#39;: 20.79338929607865, &#39;source&#39;: &#39;cord19_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;cord19&#39;, &#39;documentid&#39;: &#39;id:cord19:cord19::2b73a28n&#39;, &#39;cord_uid&#39;: &#39;2b73a28n&#39;, &#39;title&#39;: &#39;Role of endothelin-1 in lung disease&#39;, &#39;abstract&#39;: &#39;Endothelin-1 (ET-1) is a 21 amino acid peptide with diverse biological activity that has been implicated in numerous diseases. ET-1 is a potent mitogen regulator of smooth muscle tone, and inflammatory mediator that may play a key role in diseases of the airways, pulmonary circulation, and inflammatory lung diseases, both acute and chronic. This review will focus on the biology of ET-1 and its role in lung disease.&#39;}} . We can also define the same query by using the QueryModel abstraction that allows us to specify how we want to match and rank our documents. In this case, we defined that we want to: . match our documents using the OR operator, which matches all the documents that share at least one term with the query. | rank the matched documents using the bm25 rank profile defined in our application package. | . from vespa.query import QueryModel, RankProfile as Ranking, OR res = app.query( query=&quot;What is the role of endothelin-1&quot;, query_model=QueryModel( match_phase = OR(), rank_profile = Ranking(name=&quot;bm25&quot;) ) ) res.hits[0] . {&#39;id&#39;: &#39;id:cord19:cord19::2b73a28n&#39;, &#39;relevance&#39;: 20.79338929607865, &#39;source&#39;: &#39;cord19_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;cord19&#39;, &#39;documentid&#39;: &#39;id:cord19:cord19::2b73a28n&#39;, &#39;cord_uid&#39;: &#39;2b73a28n&#39;, &#39;title&#39;: &#39;Role of endothelin-1 in lung disease&#39;, &#39;abstract&#39;: &#39;Endothelin-1 (ET-1) is a 21 amino acid peptide with diverse biological activity that has been implicated in numerous diseases. ET-1 is a potent mitogen regulator of smooth muscle tone, and inflammatory mediator that may play a key role in diseases of the airways, pulmonary circulation, and inflammatory lung diseases, both acute and chronic. This review will focus on the biology of ET-1 and its role in lung disease.&#39;}} . Using the Vespa Query Language as in our first example gives you the full power and flexibility that Vespa can offer. In contrast, the QueryModel abstraction focuses on specific use cases and can be more useful for ML experiments, but this is a future post topic. .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/cord19/2021/02/10/basic-text-search-simplified-api.html",
            "relUrl": "/vespa/pyvespa/cord19/2021/02/10/basic-text-search-simplified-api.html",
            "date": " • Feb 10, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Semantic search with embedding representation from python",
            "content": "After the release of version 0.2.0 for pyvespa, we can now create a semantic search application based on embedding representation from python with just a few lines of code: . from vespa.package import ApplicationPackage, Field, QueryTypeField, RankProfile app_package = ApplicationPackage(name = &quot;msmarco&quot;) app_package.schema.add_fields( Field( name = &quot;id&quot;, type = &quot;string&quot;, indexing = [&quot;attribute&quot;, &quot;summary&quot;] ), Field( name = &quot;title&quot;, type = &quot;string&quot;, indexing = [&quot;index&quot;, &quot;summary&quot;], index = &quot;enable-bm25&quot; ), Field( name = &quot;title_bert&quot;, type = &quot;tensor&lt;float&gt;(x[768])&quot;, indexing = [&quot;attribute&quot;] ) ) app_package.query_profile_type.add_fields( QueryTypeField( name=&quot;ranking.features.query(title_bert)&quot;, type=&quot;tensor&lt;float&gt;(x[768])&quot; ) ) app_package.schema.add_rank_profile( RankProfile( name = &quot;bert_title&quot;, first_phase = &quot;sum(query(title_bert)*attribute(title_bert))&quot; ) ) . We will cover the step-by-step of the code block in the next sections. The goal of this post is to illustrate the pyvespa API that will enable you to create your own application rather than focusing on how to create an effective application or chosing the best transformers model to use for embedding generation. . Install pyvespa . We will use version 0.2.0 here, which can be installed via pip . pip install pyvespa==0.2.0 . Define the application . The first step is to create an application package with the ApplicationPackage class. We will name our application msmarco because we want to build a full document ranking application based on the MS MARCO dataset. . from vespa.package import ApplicationPackage app_package = ApplicationPackage(name = &quot;msmarco&quot;) . We will then add three fields to our application. Each document will have an unique id, a title and a 768 dimensional vector named title_bert that will store the title embedding. . from vespa.package import Field app_package.schema.add_fields( Field( name = &quot;id&quot;, type = &quot;string&quot;, indexing = [&quot;attribute&quot;, &quot;summary&quot;] ), Field( name = &quot;title&quot;, type = &quot;string&quot;, indexing = [&quot;index&quot;, &quot;summary&quot;], index = &quot;enable-bm25&quot; ), Field( name = &quot;title_bert&quot;, type = &quot;tensor&lt;float&gt;(x[768])&quot;, indexing = [&quot;attribute&quot;] ) ) . We need to define a query ranking feature responsible to hold the query embedding at query time. . from vespa.package import QueryTypeField app_package.query_profile_type.add_fields( QueryTypeField( name=&quot;ranking.features.query(title_bert)&quot;, type=&quot;tensor&lt;float&gt;(x[768])&quot; ) ) . Now that we have defined a document and a query embedding in the application, we can use them in a RankProfile to rank the documents matched by the query. In this case we will define a dot-product between the query embedding query(title_bert) and the document embedding attribute(title_bert). . from vespa.package import RankProfile app_package.schema.add_rank_profile( RankProfile( name = &quot;bert_title&quot;, first_phase = &quot;sum(query(title_bert)*attribute(title_bert))&quot; ) ) . Deploy the application . import os from vespa.package import VespaDocker vespa_docker = VespaDocker(port=8089) os.environ[&quot;WORK_DIR&quot;] = &quot;/Users/tmartins&quot; disk_folder = os.path.join(os.getenv(&quot;WORK_DIR&quot;), &quot;sample_application&quot;) app = vespa_docker.deploy( application_package = app_package, disk_folder=disk_folder ) . Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for configuration server. Waiting for application status. Waiting for application status. . Load the BERT model . Loading one of the many models available. . from sentence_transformers import SentenceTransformer bert_model = SentenceTransformer(&quot;distilbert-base-nli-stsb-mean-tokens&quot;) . Define a function that take a text as input and return a vector of floats as output. . import numpy as np def normalized_bert_encoder(text): vector = bert_model.encode([text])[0].tolist() norm = np.linalg.norm(vector) if norm &gt; 0.0: vector = vector / norm return vector.tolist() . Feed data . from pandas import read_csv docs = read_csv(&quot;https://thigm85.github.io/data/msmarco/docs_100.tsv&quot;, sep = &quot; t&quot;) docs = docs[docs[&#39;title&#39;].str.strip().astype(bool)] # remove empty titles docs.shape . (88, 3) . docs.head(2) . id title body . 0 D2185715 | What Is an Appropriate Gift for a Bris | Hub Pages Religion and Philosophy Judaism... | . 1 D2819479 | lunge | 1lungenoun ˈlənj Popularity Bottom 40 of... | . for idx, row in docs.iterrows(): response = app.feed_data_point( schema = &quot;msmarco&quot;, data_id = row[&quot;id&quot;], fields = { &quot;id&quot;: row[&quot;id&quot;], &quot;title&quot;: row[&quot;title&quot;], &quot;title_bert&quot;: {&quot;values&quot;: normalized_bert_encoder(row[&quot;title&quot;])} } ) . Define a query model . from vespa.query import QueryModel, QueryRankingFeature, Union, WeakAnd, ANN, RankProfile query_model = QueryModel( query_properties=[QueryRankingFeature(name=&quot;title_bert&quot;, mapping=normalized_bert_encoder)], match_phase=Union( WeakAnd(field=&quot;title&quot;, hits=10), ANN( doc_vector=&quot;title_bert&quot;, query_vector=&quot;title_bert&quot;, hits=10, label=&quot;ann_title&quot; ) ), rank_profile=RankProfile(name=&quot;bert_title&quot;) ) . At this point we can query our application: . query_results = app.query(query=&quot;What is science?&quot;, query_model=query_model, debug_request=False) . query_results.hits[0] . {&#39;id&#39;: &#39;id:msmarco:msmarco::D2089371&#39;, &#39;relevance&#39;: 0.6983165740966797, &#39;source&#39;: &#39;msmarco_content&#39;, &#39;fields&#39;: {&#39;sddocname&#39;: &#39;msmarco&#39;, &#39;documentid&#39;: &#39;id:msmarco:msmarco::D2089371&#39;, &#39;id&#39;: &#39;D2089371&#39;, &#39;title&#39;: &#39;Inquiry Science&#39;}} .",
            "url": "https://thigm85.github.io/blog/ms%20marco/vespa/pyvespa/semantic%20search/text%20search/search/2020/12/04/semantic-search-embedding-representation.html",
            "relUrl": "/ms%20marco/vespa/pyvespa/semantic%20search/text%20search/search/2020/12/04/semantic-search-embedding-representation.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Timing Vespa queries",
            "content": "When comparing different query models, we might want to compute the time difference between them. Doing that from the outside is tricky because it is hard to separate network and other non-search related costs out of the final numbers. . Luckily we can accomplish this by setting presentation.timing equal to true in the request body. Without relying on pyvespa API, we can add the keyword parameter presentation.timing in the app.query call. . from vespa.application import Vespa from vespa.query import Query, RankProfile, OR app = Vespa(url = &quot;https://api.cord19.vespa.ai&quot;) result = app.query( query=&quot;what is covid?&quot;, query_model=Query( match_phase=OR(), rank_profile=RankProfile(name=&quot;bm25&quot;) ), **{&quot;presentation.timing&quot;: &quot;true&quot;} ) result.json[&#39;timing&#39;] . {&#39;querytime&#39;: 0.01, &#39;summaryfetchtime&#39;: 0.002, &#39;searchtime&#39;: 0.013000000000000001} . Such a useful feature could be part of the pyvespa API: . result = app.query( query=&quot;what is covid?&quot;, query_model=Query( match_phase=OR(), rank_profile=RankProfile(name=&quot;bm25&quot;) ), timing=True ) .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/2020/12/02/vespa-timing.html",
            "relUrl": "/vespa/pyvespa/2020/12/02/vespa-timing.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "The Probabilistic Relevance Framework",
            "content": "Assumptions behind the Probabilistic Relevance Framework Relevance is a property of a individual document | Relevance is binary, a document is either relevant or not. | . | . Basic model . Assumptions . conditional independence between documents term given relevance info debatable but shown to be robust in practice | . | Only query terms are relevant to rank the documents Reasonable assumption in the absence of other information | . | Model . $$P(rel|d, q) approx sum_q U_i(tf_i) propto_q sum_{q, tf_i&gt;0} w_i$$ . $$U_i(x) = log frac{P(TF_i=x|rel)}{P(TF_i=x| sim rel)}, w_i = U_i(tf_i) - U_i(0) = log frac{P(TF_i=x|rel)P(TF_i=0| sim rel )}{P(TF_i=x| sim rel )P(TF_i=0|rel)}$$ . Comments . The advantage of using $w_i$ instead of U_i is that we only need to compute the score for documents that contain at least one query term. | The model is not restricted to terms and term-frequencies, any attribute of the document of query-document pair can be included. Any discrete property with a natural zero can be dealt with using the W_i form of the weight. | If we want to include a property without such a natural zero, we need to revert to the U_i form. | . | The approximations above work if we are only interested in the ranking problem. They do not provide accurate probabilities due to the approximations and transformations performed to arrive at the simplified equation. | . The Binary Independence Model . Assumptions . TF_i is a binary variable. That is the term is either present or absent. Define t_i as the event that the term is present in the document | . | Model . Under the binary assumption w_i becomes . $$w_i^{BIM} = log frac{P(t_i|rel)(1-P(t_i| sim rel))}{(1-P(t_i|rel))P(t_i| sim rel)}$$ . | We can estimate the probabilities below using the following quantities: . N: Size of the whole collection | n_i: Number of documents in the collection containing t_i | R: Relevant set size (i.e., number of documents judged relevant) | r_i: Number of judged relevant docs containing t_i . $$P(t_i|rel)=r_i/R, quad P(t_i| sim rel)=(n_i - r_i)/(N-R)$$ . | . | Using the estimates above and a small 0.5 correction for robustness, we have . $$w_i^{BIM} = log frac{(r_i + 0.5)(N-R-n_i+r_i+0.5)}{(n_i-r_i+0.5)(R-r_i+0.5)}$$ . The resulting formula is the well-known Robertson/Sprck Jones weight, also denoted as w_i^{RSJ} | . | . Absence of relevant information . If we assume that the entire collection is non-relevant, we can use r_i = R = 0 in the formula above leading to a close approximation to the classical idf . $$w_i^{IDF} = log frac{N - n_i+0.5}{n_i+0.5}$$ . | . Comments . We assumed above that non-judged document is non-relevant | The absence of relevant information assumption is equivalent of saying that P(t_i|rel) = 0.5 | .",
            "url": "https://thigm85.github.io/blog/search/term-based%20matching/bm25/2020/12/01/probabilistic-relevance-framework.html",
            "relUrl": "/search/term-based%20matching/bm25/2020/12/01/probabilistic-relevance-framework.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Fine-tuning a BERT model for search applications",
            "content": "There are cases where the inputs to your Transformer model are pairs of sentences, but you want to process each sentence of the pair at different times due to your application’s nature. . The search use case . Search applications are one example. They involve a large collection of documents that can be pre-processed and stored before a search action is required. On the other hand, a query triggers a search action, and we can only process it in real-time. Search apps’ goal is to return the most relevant documents to the query as quickly as possible. By applying the tokenizer to the documents as soon as we feed them to the application, we only need to tokenize the query when a search action is required, saving us precious time. . In addition to applying the tokenizer at different times, you also want to retain adequate control about encoding your pair of sentences. For search, you might want to have a joint input vector of length 128 where the query, which is usually smaller than the document, contributes with 32 tokens while the document can take up to 96 tokens. . Training and serving compatibility . When training a Transformer model for search, you want to ensure that the training data will follow the same pattern used by the search engine serving the final model. I have written a blog post on how to get started with BERT model fine-tuning using the transformer library. This piece will adapt the training routine with a custom encoding based on two separate tokenizers to reproduce how a Vespa application would serve the model once deployed. . Create independent BERT encodings . The only change required is simple but essential. In my previous post, we discussed the vanilla case where we simply applied the tokenizer directly to the pairs of queries and documents. . from transformers import BertTokenizerFast model_name = &quot;google/bert_uncased_L-4_H-512_A-8&quot; tokenizer = BertTokenizerFast.from_pretrained(model_name) train_encodings = tokenizer(train_queries, train_docs, truncation=True, padding=&#39;max_length&#39;, max_length=128) val_encodings = tokenizer(val_queries, val_docs, truncation=True, padding=&#39;max_length&#39;, max_length=128) . In the search case, we create the create_bert_encodings function that will apply two different tokenizers, one for the query and the other for the document. In addition to allowing for different query and document max_length, we also need to set add_special_tokens=False and not use padding as those need to be included by our custom code when joining the tokens generated by the tokenizer. . def create_bert_encodings(queries, docs, tokenizer, query_input_size, doc_input_size): queries_encodings = tokenizer( queries, truncation=True, max_length=query_input_size-2, add_special_tokens=False ) docs_encodings = tokenizer( docs, truncation=True, max_length=doc_input_size-1, add_special_tokens=False ) TOKEN_NONE=0 TOKEN_CLS=101 TOKEN_SEP=102 input_ids = [] token_type_ids = [] attention_mask = [] for query_input_ids, doc_input_ids in zip(queries_encodings[&quot;input_ids&quot;], docs_encodings[&quot;input_ids&quot;]): # create input id input_id = [TOKEN_CLS] + query_input_ids + [TOKEN_SEP] + doc_input_ids + [TOKEN_SEP] number_tokens = len(input_id) padding_length = max(128 - number_tokens, 0) input_id = input_id + [TOKEN_NONE] * padding_length input_ids.append(input_id) # create token id token_type_id = [0] * len([TOKEN_CLS] + query_input_ids + [TOKEN_SEP]) + [1] * len(doc_input_ids + [TOKEN_SEP]) + [TOKEN_NONE] * padding_length token_type_ids.append(token_type_id) # create attention_mask attention_mask.append([1] * number_tokens + [TOKEN_NONE] * padding_length) encodings = { &quot;input_ids&quot;: input_ids, &quot;token_type_ids&quot;: token_type_ids, &quot;attention_mask&quot;: attention_mask } return encodings . We then create the train_encodings and val_encodings required by the training routine. Everything else on the training routine works just the same. . from transformers import BertTokenizerFast model_name = &quot;google/bert_uncased_L-4_H-512_A-8&quot; tokenizer = BertTokenizerFast.from_pretrained(model_name) train_encodings = create_bert_encodings( queries=train_queries, docs=train_docs, tokenizer=tokenizer, query_input_size=32, doc_input_size=96 ) val_encodings = create_bert_encodings( queries=val_queries, docs=val_docs, tokenizer=tokenizer, query_input_size=32, doc_input_size=96 ) . Conclusion and future work . Training a model to deploy in a search application require us to ensure that the training encodings are compatible with encodings used at serving time. We generate document encodings offline when feeding the documents to the search engine while creating query encoding at run-time upon arrival of the query. It is often relevant to use different maximum lengths for queries and documents, and other possible configurations. . We showed how to customize BERT model encodings to ensure this training and serving compatibility. However, a better approach is to build tools that bridge the gap between training and serving by allowing users to request training data that respects by default the encodings used when serving the model. pyvespa will include such integration to make it easier for Vespa users to train BERT models without having to adjust the encoding generation manually as we did above. .",
            "url": "https://thigm85.github.io/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html",
            "relUrl": "/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Proposal for a simplified pyvespa API",
            "content": "While experimenting with embeddings in pyvespa, I noticed that I could make significant improvements on the pyvespa API. . Simpler API . Simplify the API for the most common and basic usage (one default schema, one default query profile with root query profile type) while making it easy to configure more complex usage (multiple schemas, for example). | . Default Schema . Before simplification: . from vespa.package import Document, Field, Schema, FieldSet, RankProfile, ApplicationPackage document = Document( fields=[ Field(name = &quot;id&quot;, type = &quot;string&quot;, indexing = [&quot;attribute&quot;, &quot;summary&quot;]), Field(name = &quot;title&quot;, type = &quot;string&quot;, indexing = [&quot;index&quot;, &quot;summary&quot;], index = &quot;enable-bm25&quot;), Field(name = &quot;body&quot;, type = &quot;string&quot;, indexing = [&quot;index&quot;, &quot;summary&quot;], index = &quot;enable-bm25&quot;) ] ) msmarco_schema = Schema( name = &quot;msmarco&quot;, document = document, fieldsets = [FieldSet(name = &quot;default&quot;, fields = [&quot;title&quot;, &quot;body&quot;])], rank_profiles = [RankProfile(name = &quot;default&quot;, first_phase = &quot;nativeRank(title, body)&quot;)] ) app_package = ApplicationPackage(name = &quot;msmarco&quot;, schema=msmarco_schema) . After simplification: . from vespa.package import Field, FieldSet, RankProfile, ApplicationPackage app_package = ApplicationPackage(name = &quot;msmarco&quot;) app_package.schema.add_field( Field(name = &quot;id&quot;, type = &quot;string&quot;, indexing = [&quot;attribute&quot;, &quot;summary&quot;]), Field(name = &quot;title&quot;, type = &quot;string&quot;, indexing = [&quot;index&quot;, &quot;summary&quot;], index = &quot;enable-bm25&quot;), Field(name = &quot;body&quot;, type = &quot;string&quot;, indexing = [&quot;index&quot;, &quot;summary&quot;], index = &quot;enable-bm25&quot;) ) app_package.schema.add_field_set(FieldSet(name = &quot;default&quot;, fields = [&quot;title&quot;, &quot;body&quot;])) app_package.schema.add_rank_profile(RankProfile(name = &quot;default&quot;, first_phase = &quot;nativeRank(title, body)&quot;)) . When required we can use a similar pattern to deal with multiple schemas use cases where the .schema attribute is short for .schemas(&quot;default&quot;) . app_package.schemas(&quot;my_other_schema&quot;).add_field( Field(name = &quot;id&quot;, type = &quot;string&quot;, indexing = [&quot;attribute&quot;, &quot;summary&quot;]), Field(name = &quot;user_name&quot;, type = &quot;string&quot;, indexing = [&quot;attribute&quot;, &quot;summary&quot;]), ) . Default query profile with root query profile type . To support embeddings we need to enable the creation of query profile and query profile types. Example of what we want to accomplish: . query profile type | . &lt;query-profile-type id=&quot;root&quot;&gt; &lt;field name=&quot;ranking.features.query(tensor_bert)&quot; type=&quot;tensor&amp;lt;float&amp;gt;(x[768])&quot; /&gt; &lt;/query-profile-type&gt; . query profile | . &lt;query-profile id=&quot;default&quot; type=&quot;root&quot;&gt; &lt;field name=&quot;maxHits&quot;&gt;1000&lt;/field&gt; &lt;/query-profile&gt; . Verbose API: . query_profile_type = QueryProfileType( id=&quot;root&quot;, fields = [ QueryTypeField( name=&quot;ranking.features.query(tensor_bert)&quot;, type=&quot;tensor&lt;float&gt;(x[768])&quot; ) ] ) query_profile = QueryProfile( id=&quot;default&quot;, type=&quot;root&quot;, fields=[QueryField(name=&quot;maxHits&quot;, value=1000)] ) app_package.add_query_profile_type(query_profile_type) app_package.add_query_profile(query_profile) . Simplified API: Assuming default query profile and root query profile type. . app_package.query_profile_type.add_field( QueryTypeField( name=&quot;ranking.features.query(tensor_bert)&quot;, type=&quot;tensor&lt;float&gt;(x[768])&quot; ) ) app_package.query_profile.add_field( QueryField(name=&quot;maxHits&quot;, value=1000) ) . When required we can use a similar pattern to deal with multiple profiles where .query_profile attribute is short for .query_profiles(&quot;default&quot;). Similarly for .query_profile_type. . Embedding as doc/query representation . Create the application with the simplified API . Lets see how we can use embeddings for document and query representation using the simplified API. . app_package = ApplicationPackage(name = &quot;msmarco&quot;) app_package.schema.add_field( Field(name = &quot;id&quot;, type = &quot;string&quot;, indexing = [&quot;attribute&quot;, &quot;summary&quot;]), Field(name = &quot;title&quot;, type = &quot;string&quot;, indexing = [&quot;index&quot;, &quot;summary&quot;], index = &quot;enable-bm25&quot;), Field(name = &quot;title_bert&quot;, type = &quot;tensor&lt;float&gt;(x[768])&quot;, indexing = [&quot;attribute&quot;]) ) app_package.query_profile_type.add_field( QueryTypeField( name=&quot;ranking.features.query(tensor_bert)&quot;, type=&quot;tensor&lt;float&gt;(x[768])&quot; ) ) app_package.schema.add_rank_profile( RankProfile( name = &quot;bert_title&quot;, first_phase = &quot;sum(query(tensor_bert)*attribute(title_bert))&quot; ) ) . Feeding . It is annoying the need to specify {&quot;values&quot;: [...]} for indexed tensors, but this is the user&#39;s responsability for now. . response = app.feed_data_point( data_id = &quot;test_id&quot;, fields = { &quot;id&quot;: &quot;test_id&quot;, &quot;title&quot;: &quot;this is a test title&quot;, &quot;title_bert&quot;: { &quot;values&quot;: create_embedding(text=&quot;this is a test title&quot;) # from string to list of floats } } ) . Using embeddings on queries . Here I propose a unified approach to define and use embeddings in the applications. | . The ANN operator currently accepts an embedding function, and take care of converting query to embedding. . results = app.query( query=&quot;Where is my text?&quot;, query_model = Query( match_phase=ANN( doc_vector=&quot;title_bert&quot;, query_vector=&quot;tensor_bert&quot;, embedding_model=create_embedding, hits=10, ), rank_profile=Ranking(name=&quot;bert_title&quot;) ), ) . But we need to manually do the conversion when not using an embedding friendly operator, such as term-based OR. . other_args = { &quot;ranking.features.query(tensor_bert)&quot;: create_embedding(text=&quot;this is a test query&quot;) } results = app.query( query=&quot;Where is my text?&quot;, query_model = Query( match_phase=OR(), rank_profile=Ranking(name=&quot;default&quot;) ), hits = 2, **other_args ) . I think we should unify those two approaches by moving the embedding creation to the Query model. Besides unifying usage, this makes sense because the embedding used is actually part of the query model. Changing the embedding function actually defines a different query model. . # term-based matching query_model = Query( query_properties=[ QueryRankingFeature(name=&quot;tensor_bert&quot;, mapping=create_embedding) ], match_phase=OR(), rank_profile=Ranking(name=&quot;default&quot;) ) # embedding-based matching query_model = Query( query_properties=[ QueryRankingFeature(name=&quot;tensor_bert&quot;, mapping=create_embedding) ], match_phase=ANN( doc_vector=&quot;title_bert&quot;, query_vector=&quot;tensor_bert&quot;, hits=10, ), rank_profile=Ranking(name=&quot;bert_title&quot;) ) # same usage for both results = app.query( query=&quot;Where is my text?&quot;, query_model=query_model ) .",
            "url": "https://thigm85.github.io/blog/pyvespa/embeddings/2020/11/17/pyvespa-simplified-api.html",
            "relUrl": "/pyvespa/embeddings/2020/11/17/pyvespa-simplified-api.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Fine-tuning a BERT model with transformers",
            "content": "This post describes a simple way to get started with fine-tuning transformer models. It will cover the basics and introduce you to the amazing Trainer class from the transformers library. I will leave important topics such as hyperparameter tuning, cross-validation and more detailed model validation to followup posts. . We use a dataset built from COVID-19 Open Research Dataset Challenge. This work is one small piece of a larger project that is to build the cord19 search app. . You can run the code from Google Colab but do not forget to enable GPU support. . Install required libraries . !pip install pandas transformers . Load the dataset . In order to fine-tune the BERT models for the cord19 application we need to generate a set of query-document features as well as labels that indicate which documents are relevant for the specific queries. For this exercise we will use the query string to represent the query and the title string to represent the documents. . from pandas import read_csv training_data = read_csv(&quot;https://thigm85.github.io/data/cord19/cord19-query-title-label.csv&quot;) training_data.head() . query title label . 0 coronavirus origin | Monophyletic Relationship between Severe Acute... | 1 | . 1 coronavirus origin | Comprehensive overview of COVID-19 based on cu... | 1 | . 2 coronavirus origin | The SARS, MERS and novel coronavirus (COVID-19... | 1 | . 3 coronavirus origin | Evidence for zoonotic origins of Middle East r... | 1 | . 4 coronavirus origin | Deadly virus effortlessly hops species | 1 | . There are 50 unique queries. . len(training_data[&quot;query&quot;].unique()) . 50 . For each query we have a list of documents, divided between relevant (label=1) and irrelevant (label=0). . training_data[[&quot;title&quot;, &quot;label&quot;]].groupby(&quot;label&quot;).count() . title . label . 0 30272 | . 1 21120 | . Data split . We are going to use a simple data split into train and validation sets for illustration purposes. Even though we have more than 50 thousand data points when we consider unique query and document pairs, I believe this specific case would benefit from cross-validation since it has only 50 queries containing relevance judgement. . from sklearn.model_selection import train_test_split train_queries, val_queries, train_docs, val_docs, train_labels, val_labels = train_test_split( training_data[&quot;query&quot;].tolist(), training_data[&quot;title&quot;].tolist(), training_data[&quot;label&quot;].tolist(), test_size=.2 ) . Create BERT encodings . Create train and validation encodings. In order to do that we need to chose which BERT model to use. We will use padding and truncation because the training routine expects all tensors within a batch to have the same dimensions. . from transformers import BertTokenizerFast model_name = &quot;google/bert_uncased_L-4_H-512_A-8&quot; tokenizer = BertTokenizerFast.from_pretrained(model_name) train_encodings = tokenizer(train_queries, train_docs, truncation=True, padding=&#39;max_length&#39;, max_length=128) val_encodings = tokenizer(val_queries, val_docs, truncation=True, padding=&#39;max_length&#39;, max_length=128) . Create a custom dataset . Now that we have the encodings and the labels we can create a Dataset object as described in the transformers webpage about custom datasets. . import torch class Cord19Dataset(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) train_dataset = Cord19Dataset(train_encodings, train_labels) val_dataset = Cord19Dataset(val_encodings, val_labels) . Fine-tune the BERT model . We are going to use BertForSequenceClassification, since we are trying to classify query and document pairs into two distinct classes (non-relevant, relevant). . from transformers import BertForSequenceClassification model = BertForSequenceClassification.from_pretrained(model_name) . We can set requires_grad to False for all the base model parameters in order to fine-tune only the task-specific parameters. . for param in model.base_model.parameters(): param.requires_grad = False . We can then fine-tune the model with Trainer. Below is a basic routine with out-of-the-box set of parameters. Care should be taken when chosing the parameters below, but this is out of the scope of this piece. . from transformers import Trainer, TrainingArguments training_args = TrainingArguments( output_dir=&#39;./results&#39;, # output directory evaluation_strategy=&quot;epoch&quot;, # Evaluation is done at the end of each epoch. num_train_epochs=3, # total number of training epochs per_device_train_batch_size=16, # batch size per device during training per_device_eval_batch_size=64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay save_total_limit=1, # limit the total amount of checkpoints. Deletes the older checkpoints. ) trainer = Trainer( model=model, # the instantiated 🤗 Transformers model to be trained args=training_args, # training arguments, defined above train_dataset=train_dataset, # training dataset eval_dataset=val_dataset # evaluation dataset ) trainer.train() . Export the model to onnx . Once training is complete we can export the model using the ONNX format to be deployed elsewhere. I assume below that you have access to a GPU, which you can get from Google Colab for example. . from torch.onnx import export device = torch.device(&quot;cuda&quot;) model_onnx_path = &quot;model.onnx&quot; dummy_input = ( train_dataset[0][&quot;input_ids&quot;].unsqueeze(0).to(device), train_dataset[0][&quot;token_type_ids&quot;].unsqueeze(0).to(device), train_dataset[0][&quot;attention_mask&quot;].unsqueeze(0).to(device) ) input_names = [&quot;input_ids&quot;, &quot;token_type_ids&quot;, &quot;attention_mask&quot;] output_names = [&quot;logits&quot;] export( model, dummy_input, model_onnx_path, input_names = input_names, output_names = output_names, verbose=False, opset_version=11 ) .",
            "url": "https://thigm85.github.io/blog/bert/transformers/2020/11/12/fine-tune-bert-basic-transformers-trainer.html",
            "relUrl": "/bert/transformers/2020/11/12/fine-tune-bert-basic-transformers-trainer.html",
            "date": " • Nov 12, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Debug first iteration of BERT models",
            "content": "Context . I have fine-tuned a BERT model using a simple training routine. I then deployed a simplified cord19 application on my laptop to validate the model. We will investigate some strange results we found in a previous sprint. . Evaluate ranking functions . Connect to my local Vespa app. . from vespa.application import Vespa app = Vespa(url = &quot;http://localhost&quot;, port = 8080) . Define different query models. bert_index_1 uses the correct output from the BERT model used, which is the probability of the document being relevant. bert was a mistake I made when I used the wrong output to rank the documents. It is still here because it will be part of my investigations later. . from vespa.query import Query, RankProfile as Ranking, OR query_models = { &quot;or_bm25&quot;: Query( match_phase = OR(), rank_profile = Ranking(name=&quot;bm25&quot;) ), &quot;or_bm25_bert&quot;: Query( match_phase = OR(), rank_profile = Ranking(name=&quot;bert&quot;) ), &quot;or_bm25_bert_index_1&quot;: Query( match_phase = OR(), rank_profile = Ranking(name=&quot;bert_index_1&quot;) ) } . The evaluation metrics that we want to compute. . from vespa.evaluation import MatchRatio, Recall, ReciprocalRank, NormalizedDiscountedCumulativeGain eval_metrics = [MatchRatio(), Recall(at=10), ReciprocalRank(at=10), NormalizedDiscountedCumulativeGain(at=10)] . Load labeled data. You can download it here. . import json labelled_data = json.load(open(&quot;cord19/labelled_data.json&quot;, &quot;r&quot;)) . We will need to tokenizer to convert the query string to embedding vector. . from transformers import BertTokenizerFast tokenizer = BertTokenizerFast.from_pretrained(&#39;bert-base-uncased&#39;) . Compute evaluation metrics for each query model and each query. . from pandas import DataFrame evaluations = {} for query_model in query_models: evaluation = [] for query_data in labelled_data: print(query_data[&quot;query_id&quot;]) evaluation_query = app.evaluate_query( eval_metrics=eval_metrics, query_model=query_models[query_model], query_id=query_data[&quot;query_id&quot;], query=query_data[&quot;query&quot;], id_field = &quot;cord_uid&quot;, relevant_docs=query_data[&quot;relevant_docs&quot;], hits = 10, timeout=&quot;100s&quot;, **{&quot;ranking.features.query(query_token_ids)&quot;: str(tokenizer( str(query_data[&quot;query&quot;]), truncation=True, padding=&quot;max_length&quot;, max_length=64, add_special_tokens=False )[&quot;input_ids&quot;])} ) evaluation.append(evaluation_query) evaluations[query_model] = DataFrame.from_records(evaluation) . Organize the data into a nicer format to work with. . import pandas as pd metric_values = [] for query_model in query_models: for metric in eval_metrics: metric_values.append( pd.DataFrame( data={ &quot;query_model&quot;: query_model, &quot;metric&quot;: metric.name, &quot;value&quot;: evaluations[query_model][metric.name + &quot;_value&quot;].to_list() } ) ) metric_values = pd.concat(metric_values, ignore_index=True) . Recall issue . The recall issue is that different query models were giving different recall metrics, even though they all had the same matching and ranking-phase and were just reordering the top 10 positions. . second-phase { rerank-count: 10 expression: sum(eval) } . metric_values[metric_values.metric == &quot;recall_10&quot;].groupby([&#39;query_model&#39;, &#39;metric&#39;]).median() . value . query_model metric . or_bm25 recall_10 0.007412 | . or_bm25_bert recall_10 0.008076 | . or_bm25_bert_index_1 recall_10 0.008118 | . It seems that Vespa reorder the top 11 documents, even though rerank-count: 10, as I show below. . Identify which queries are responsible for the difference: . from pandas import merge recall_measures = merge( left=evaluations[&quot;or_bm25&quot;], right=evaluations[&quot;or_bm25_bert_index_1&quot;], on=&quot;query_id&quot; )[[&quot;query_id&quot;, &quot;recall_10_value_x&quot;, &quot;recall_10_value_y&quot;]] recall_measures[recall_measures.recall_10_value_x != recall_measures.recall_10_value_y] . query_id recall_10_value_x recall_10_value_y . 14 15 | 0.006726 | 0.004484 | . 16 17 | 0.006974 | 0.008368 | . 20 21 | 0.006088 | 0.007610 | . 32 33 | 0.006515 | 0.009772 | . 38 39 | 0.007165 | 0.008188 | . 40 41 | 0.014045 | 0.016854 | . 49 50 | 0.020134 | 0.013423 | . Query two different rank profiles. . query_data = labelled_data[14] result_bm25 = app.query(query=query_data[&quot;query&quot;], query_model=query_models[&quot;or_bm25&quot;], hits = 10, ) bm25_ids = [hit[&quot;fields&quot;][&quot;cord_uid&quot;] for hit in result_bm25.hits] result_bm25_bert = app.query(query=query_data[&quot;query&quot;], query_model=query_models[&quot;or_bm25_bert_index_1&quot;], hits = 10, timeout=&quot;100s&quot;, **{&quot;ranking.features.query(query_token_ids)&quot;: str(tokenizer( str(query_data[&quot;query&quot;]), truncation=True, padding=&quot;max_length&quot;, max_length=64, add_special_tokens=False )[&quot;input_ids&quot;])} ) bm25_bert_ids = [hit[&quot;fields&quot;][&quot;cord_uid&quot;] for hit in result_bm25_bert.hits] . Check which id is in the BERT top 10 but not in the BM25 top 10: . id_in_bert_not_in_bm25 = [x for x in bm25_bert_ids if x not in bm25_ids] id_in_bert_not_in_bm25 . [&#39;ecu579el&#39;] . List the top 11 results from BM25. Notice that the missing doc is in the 11th position. . result_bm25_11 = [hit[&quot;fields&quot;][&quot;cord_uid&quot;] for hit in app.query(query=query_data[&quot;query&quot;], query_model=query_models[&quot;or_bm25&quot;], hits = 11).hits] . result_bm25_11 . [&#39;zpek8i5e&#39;, &#39;75u57fw1&#39;, &#39;up5jpq45&#39;, &#39;qmrntk43&#39;, &#39;cxfzs68n&#39;, &#39;y2nhss9u&#39;, &#39;94puwlbm&#39;, &#39;zpmdrh4q&#39;, &#39;fpexj3s5&#39;, &#39;axljtddn&#39;, &#39;ecu579el&#39;] . Positive and null NDGC issue . When querying @bergum dev instance I found cases where the NDCG @ 10 was positive for BM25 model and zero for the BERT re-rank model, as we can see in the picture below, which makes no sense. . . I could not reproduce the NDCG issue and the results were as expected on my local instance, as I show below. . from pandas import merge ndcg_measures = merge( left=evaluations[&quot;or_bm25&quot;], right=evaluations[&quot;or_bm25_bert_index_1&quot;], on=&quot;query_id&quot; )[[&quot;query_id&quot;, &quot;ndcg_10_value_x&quot;, &quot;ndcg_10_value_y&quot;]] . ndcg_measures . query_id ndcg_10_value_x ndcg_10_value_y . 0 1 | 0.683159 | 0.812003 | . 1 2 | 0.000000 | 0.000000 | . 2 3 | 0.450853 | 0.619669 | . 3 4 | 0.000000 | 0.000000 | . 4 5 | 0.397809 | 0.455605 | . 5 6 | 0.678762 | 0.901013 | . 6 7 | 0.888733 | 0.629200 | . 7 8 | 0.527845 | 0.947807 | . 8 9 | 0.859413 | 0.569139 | . 9 10 | 0.541696 | 0.880740 | . 10 11 | 0.000000 | 0.000000 | . 11 12 | 0.510384 | 0.844481 | . 12 13 | 0.500000 | 0.386853 | . 13 14 | 0.847790 | 0.855857 | . 14 15 | 0.882121 | 0.859719 | . 15 16 | 0.588160 | 0.792087 | . 16 17 | 0.674788 | 0.907813 | . 17 18 | 0.758879 | 0.652883 | . 18 19 | 0.695950 | 0.421776 | . 19 20 | 0.769846 | 0.991829 | . 20 21 | 0.581889 | 0.651197 | . 21 22 | 0.301030 | 0.430677 | . 22 23 | 0.735734 | 0.686502 | . 23 24 | 0.963487 | 0.857319 | . 24 25 | 0.570642 | 0.650921 | . 25 26 | 0.918849 | 0.868415 | . 26 27 | 0.533893 | 0.493208 | . 27 28 | 0.548702 | 0.575719 | . 28 29 | 0.966813 | 0.907663 | . 29 30 | 0.811837 | 0.804581 | . 30 31 | 0.000000 | 0.000000 | . 31 32 | 0.430677 | 0.630930 | . 32 33 | 0.707489 | 0.426919 | . 33 34 | 0.430677 | 0.630930 | . 34 35 | 0.489969 | 0.906025 | . 35 36 | 0.846117 | 0.879854 | . 36 37 | 0.950421 | 0.950421 | . 37 38 | 0.783761 | 0.800132 | . 38 39 | 0.976409 | 0.710612 | . 39 40 | 0.815931 | 0.559814 | . 40 41 | 0.806327 | 0.970409 | . 41 42 | 0.763499 | 0.706410 | . 42 43 | 0.926285 | 0.702700 | . 43 44 | 0.833223 | 0.579375 | . 44 45 | 0.804465 | 0.710549 | . 45 46 | 0.965444 | 0.622067 | . 46 47 | 0.796498 | 0.717333 | . 47 48 | 0.505179 | 0.917655 | . 48 49 | 0.531761 | 0.864027 | . 49 50 | 0.445734 | 0.422790 | . import plotly.graph_objects as go ids=ndcg_measures.query_id.tolist() fig = go.Figure(data=[ go.Bar(name=&#39;BM25&#39;, x=ids, y=ndcg_measures.ndcg_10_value_x.tolist()), go.Bar(name=&#39;BM25 + BERT&#39;, x=ids, y=ndcg_measures.ndcg_10_value_y.tolist()) ]) # Change the bar mode fig.update_layout(barmode=&#39;group&#39;, xaxis=dict(type=&#39;category&#39;)) fig.show() . Bergum&#39;s instance . @bergum dev instance was down, so I could not check his results again, but below is the code I would use it. . from vespa.application import Vespa app = Vespa( url=&quot;https://bergum.cord-19.vespa-team.aws-us-east-1c.dev.public.vespa.oath.cloud&quot;, cert=&quot;/Users/tmartins/projects/vespa/pyvespa/docs/sphinx/source/use_cases/cord19/data-plane-joint.txt&quot; ) . from vespa.query import Query, RankProfile, OR query_models = { &quot;or_bm25&quot;: Query( match_phase = OR(), rank_profile = Ranking(name=&quot;bm25&quot;) ), &quot;or_bm25_bert&quot;: Query( match_phase = OR(), rank_profile = Ranking(name=&quot;bert&quot;) ) } . from pandas import DataFrame evaluations = {} for query_model in query_models: evaluation = [] for query_data in labelled_data: print(query_data[&quot;query_id&quot;]) body = { &quot;yql&quot;: &quot;select * from sources * where userQuery();&quot;, &quot;query&quot;: query_data[&quot;query&quot;], &quot;type&quot;: &quot;any&quot;, &quot;model.defaultIndex&quot;: &quot;default&quot;, &quot;hits&quot;: 10, &quot;collapsefield&quot;: &quot;title&quot;, &quot;timeout&quot;: &quot;100s&quot;, &quot;ranking&quot;: query_models[query_model].rank_profile.name } evaluation_query = app.evaluate_query( eval_metrics=eval_metrics, query_model=query_models[query_model], query_id=query_data[&quot;query_id&quot;], query=query_data[&quot;query&quot;], id_field = &quot;cord_uid&quot;, relevant_docs=query_data[&quot;relevant_docs&quot;], body=body, ) evaluation.append(evaluation_query) evaluations[query_model] = DataFrame.from_records(evaluation) . Effect of using different model outputs from BERT . For a moment I thought that the results were similar, nor matter this model output I used. This would indicate a bug. After further analysis I realized that this made no sense, as expected. Using the right model output yielded much better results. . import plotly.express as px fig = px.box(metric_values[(metric_values.metric == &quot;ndcg_10&quot;) &amp; (metric_values.query_model != &quot;or_bm25&quot;)], x=&quot;query_model&quot;, y=&quot;value&quot;, points=&quot;all&quot;) fig.show() .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/sprint/2020/11/08/cord19-debug-bert-issues.html",
            "relUrl": "/vespa/pyvespa/sprint/2020/11/08/cord19-debug-bert-issues.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Fine-tuning a BERT model for search applications with Optuna tuning",
            "content": "In case running on Google Colab . Mount your Google Drive to be able to load and save data to it. . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . Install libraries used in this post. . !pip install transformers torch onnxruntime optuna . Prepare the dataset . Load the data . In order to fine-tune the BERT models for the cord19 application we need to generate a set of query-document features as well as labels that indicate which documents are relevant for the specific queries. For this exercise we will use the query string to represent the query and the title string to represent the documents. . The file labelled_data.json contains information about the query string and the file training_all_judgement_data.csv contain information about labels and title string. Those files were created and covered elsewhere but you can download them here and here. . import json from pandas import read_csv labelled_data = json.load(open(&quot;/content/drive/My Drive/cord19/labelled_data_all.json&quot;, &quot;r&quot;)) training_data = read_csv(&quot;/content/drive/My Drive/cord19/training_all_jugdments_data.csv&quot;) . training_data has almost everything we need, except the query string. . training_data.head() . document_id query_id label title-full . 0 005b2j4b | 1 | 2 | Monophyletic Relationship between Severe Acute... | . 1 00fmeepz | 1 | 1 | Comprehensive overview of COVID-19 based on cu... | . 2 010vptx3 | 1 | 2 | The SARS, MERS and novel coronavirus (COVID-19... | . 3 0194oljo | 1 | 1 | Evidence for zoonotic origins of Middle East r... | . 4 021q9884 | 1 | 1 | Deadly virus effortlessly hops species | . The query string can be obtained from the labelled_data. . print(labelled_data[0][&quot;query_id&quot;], labelled_data[0][&quot;query&quot;]) . 1 coronavirus origin . Compatible BERT encodings . Since we are training a model that will be deployed in a search application, we need to ensure that the training encodings are compatible with encodings used at serving time. At serving time, document encodings will be applied offline when feeding the documents to the search engine while the query encoding will be applied at run-time upon arrival of the query. In addition, it might be relevant to use different maximum length for queries and documents. . def create_bert_encodings(queries, docs, tokenizer, query_input_size, doc_input_size): queries_encodings = tokenizer( queries, truncation=True, max_length=query_input_size-2, add_special_tokens=False ) docs_encodings = tokenizer( docs, truncation=True, max_length=doc_input_size-1, add_special_tokens=False ) TOKEN_NONE=0 TOKEN_CLS=101 TOKEN_SEP=102 input_ids = [] token_type_ids = [] attention_mask = [] for query_input_ids, doc_input_ids in zip(queries_encodings[&quot;input_ids&quot;], docs_encodings[&quot;input_ids&quot;]): # create input id input_id = [TOKEN_CLS] + query_input_ids + [TOKEN_SEP] + doc_input_ids + [TOKEN_SEP] number_tokens = len(input_id) padding_length = max(128 - number_tokens, 0) input_id = input_id + [TOKEN_NONE] * padding_length input_ids.append(input_id) # create token id token_type_id = [0] * len([TOKEN_CLS] + query_input_ids + [TOKEN_SEP]) + [1] * len(doc_input_ids + [TOKEN_SEP]) + [TOKEN_NONE] * padding_length token_type_ids.append(token_type_id) # create attention_mask attention_mask.append([1] * number_tokens + [TOKEN_NONE] * padding_length) encodings = { &quot;input_ids&quot;: input_ids, &quot;token_type_ids&quot;: token_type_ids, &quot;attention_mask&quot;: attention_mask } return encodings . Create Datasets . Create a list for queries (represented by the query string), docs (represented by the doc titles) and labels from the labelled_data and training_data that we loaded earlier. . train_queries = [] train_docs = [] train_labels = [] for data_point in labelled_data: query_id = data_point[&quot;query_id&quot;] titles = training_data[training_data[&quot;query_id&quot;] == query_id][&quot;title-full&quot;].tolist() train_docs.extend(titles) train_labels.extend([1 if x &gt; 0 else 0 for x in training_data[training_data[&quot;query_id&quot;] == query_id][&quot;label&quot;].tolist()]) query = data_point[&quot;query&quot;] train_queries.extend([query] * len(titles)) . We are going to use a simple data split into train and validation sets for illustration purposes. The cord19 use case probably needs cross-validation to be used since it has only 50 queries containing relevance judgement. . from sklearn.model_selection import train_test_split train_queries, val_queries, train_docs, val_docs, train_labels, val_labels = train_test_split( train_queries, train_docs, train_labels, test_size=.2 ) . Create train and validation encodings. In order to do that we need to chose which BERT model to use, and the maximum size used for the resulting query and document vector. . model_name = &quot;google/bert_uncased_L-4_H-512_A-8&quot; query_input_size=24 doc_input_size=64 . from transformers import BertTokenizerFast tokenizer = BertTokenizerFast.from_pretrained(model_name) train_encodings = create_bert_encodings( queries=train_queries, docs=train_docs, tokenizer=tokenizer, query_input_size=query_input_size, doc_input_size=doc_input_size ) val_encodings = create_bert_encodings( queries=val_queries, docs=val_docs, tokenizer=tokenizer, query_input_size=query_input_size, doc_input_size=doc_input_size ) . Now that we have the encodings and the labels we can create a Dataset object as described in the transformers webpage about custom datasets. . import torch class Cord19Dataset(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) train_dataset = Cord19Dataset(train_encodings, train_labels) val_dataset = Cord19Dataset(val_encodings, val_labels) . Fine-tune the BERT model . We can then fine-tune the model (only task specific weights). . Define accuracy metric. . from transformers import EvalPrediction import numpy as np def compute_metrics(p: EvalPrediction): preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions preds = np.argmax(preds, axis=1) return {&quot;accuracy&quot;: (preds == p.label_ids).astype(np.float32).mean().item()} . Hyperparameter tunning with Optuna. . from transformers import BertForSequenceClassification, Trainer, TrainingArguments training_args = TrainingArguments( output_dir=&#39;/content/results&#39;, # output directory #evaluation_strategy=&quot;epoch&quot;, # Evaluation is done at the end of each epoch. evaluation_strategy=&quot;steps&quot;, # Evaluation is done (and logged) every eval_steps. eval_steps=1000, # Number of update steps between two evaluations per_device_eval_batch_size=64, # batch size for evaluation save_total_limit=1, # limit the total amount of checkpoints. Deletes the older checkpoints. ) def model_init(): model = BertForSequenceClassification.from_pretrained(model_name) for param in model.base_model.parameters(): param.requires_grad = False return model trainer = Trainer( args=training_args, # training arguments, defined above train_dataset=train_dataset, # training dataset eval_dataset=val_dataset, # evaluation dataset compute_metrics=compute_metrics, # metrics to be computed model_init=model_init # Instantiate model before training starts ) def my_hp_space(trial): return { &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-4, 1e-2, log=True), &quot;num_train_epochs&quot;: trial.suggest_int(&quot;num_train_epochs&quot;, 1, 20), &quot;seed&quot;: trial.suggest_int(&quot;seed&quot;, 1, 40), &quot;per_device_train_batch_size&quot;: trial.suggest_categorical(&quot;per_device_train_batch_size&quot;, [4, 8, 16, 32, 64]), } def my_objective(metrics): return metrics[&quot;eval_loss&quot;] best_run = trainer.hyperparameter_search(direction=&quot;minimize&quot;, hp_space=my_hp_space, compute_objective=my_objective, n_trials=100) with open(&quot;/content/drive/My Drive/cord19/best_run.json&quot;, &quot;w+&quot;) as f: f.write(json.dumps(best_run.hyperparameters)) . Inspect best parameters . best_run.hyperparameters . Retrain using the best parameters and the entire dataset (need to create complete_dataset) . training_args = TrainingArguments( output_dir=&#39;/content/results&#39;, # output directory evaluation_strategy=&quot;epoch&quot;, # Evaluation is done at the end of each epoch. per_device_eval_batch_size=64, # batch size for evaluation save_total_limit=2, # limit the total amount of checkpoints. Deletes the older checkpoints. **best_run.hyperparameters ) trainer = Trainer( args=training_args, # training arguments, defined above train_dataset=complte_dataset, # training dataset compute_metrics=compute_metrics, # metrics to be computed model_init=model_init # Instantiate model before training starts ) trainer.train() .",
            "url": "https://thigm85.github.io/blog/search/cord19/bert/transformers/optuna/2020/11/07/bert-training-optuna-tuning.html",
            "relUrl": "/search/cord19/bert/transformers/optuna/2020/11/07/bert-training-optuna-tuning.html",
            "date": " • Nov 7, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Feed, get, update and delete Vespa data with pyvespa",
            "content": "Connect to Vespa instance . Connect to a running Vespa instance: . app = Vespa(url = &quot;http://localhost&quot;, port = 8080) . Assume the Vespa instance has a Schema called msmarco with the following fields: . document = Document( fields=[ Field(name=&quot;id&quot;, type=&quot;string&quot;, indexing=[&quot;attribute&quot;, &quot;summary&quot;]), Field( name=&quot;title&quot;, type=&quot;string&quot;, indexing=[&quot;index&quot;, &quot;summary&quot;], index=&quot;enable-bm25&quot;, ), Field( name=&quot;body&quot;, type=&quot;string&quot;, indexing=[&quot;index&quot;, &quot;summary&quot;], index=&quot;enable-bm25&quot;, ), ] ) . Data operations . Feed data . response = app.feed_data_point( schema=&quot;msmarco&quot;, data_id=&quot;1&quot;, fields={ &quot;id&quot;: &quot;1&quot;, &quot;title&quot;: &quot;this is my first title&quot;, &quot;body&quot;: &quot;this is my first body&quot;, }, ) assert response.json()[&quot;id&quot;] == &quot;id:msmarco:msmarco::1&quot; . Get data . response = app.get_data(schema=&quot;msmarco&quot;, data_id=&quot;1&quot;) expected_data = { &quot;fields&quot;: { &quot;id&quot;: &quot;1&quot;, &quot;title&quot;: &quot;this is my first title&quot;, &quot;body&quot;: &quot;this is my first body&quot;, }, &quot;id&quot;: &quot;id:msmarco:msmarco::1&quot;, &quot;pathId&quot;: &quot;/document/v1/msmarco/msmarco/docid/1&quot; } assert response.status_code == 200 assert response.json() == expected_data . Update data . response = app.update_data( schema=&quot;msmarco&quot;, data_id=&quot;1&quot;, fields={&quot;title&quot;: &quot;this is my updated title&quot;} ) assert response.json()[&quot;id&quot;] == &quot;id:msmarco:msmarco::1&quot; . Delete data . response = app.delete_data(schema=&quot;msmarco&quot;, data_id=&quot;1&quot;) assert response.json()[&quot;id&quot;] == &quot;id:msmarco:msmarco::1&quot; .",
            "url": "https://thigm85.github.io/blog/search/pyvespa/vespa/2020/11/06/pyvespa-include-data-operations.html",
            "relUrl": "/search/pyvespa/vespa/2020/11/06/pyvespa-include-data-operations.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "How to evaluate Vespa ranking functions from python",
            "content": "Download processed data . We can start by downloading the data that we have processed before. . import requests, json from pandas import read_csv topics = json.loads( requests.get(&quot;https://thigm85.github.io/data/cord19/topics.json&quot;).text ) relevance_data = read_csv(&quot;https://thigm85.github.io/data/cord19/relevance_data.csv&quot;) . topics contain data about the 50 topics available, including query, question and narrative. . topics[&quot;1&quot;] . {&#39;query&#39;: &#39;coronavirus origin&#39;, &#39;question&#39;: &#39;what is the origin of COVID-19&#39;, &#39;narrative&#39;: &#34;seeking range of information about the SARS-CoV-2 virus&#39;s origin, including its evolution, animal source, and first transmission into humans&#34;} . relevance_data contains the relevance judgments for each of the 50 topics. . relevance_data.head(5) . topic_id round_id cord_uid relevancy . 0 1 | 4.5 | 005b2j4b | 2 | . 1 1 | 4.0 | 00fmeepz | 1 | . 2 1 | 0.5 | 010vptx3 | 2 | . 3 1 | 2.5 | 0194oljo | 1 | . 4 1 | 4.0 | 021q9884 | 1 | . Install pyvespa . We are going to use pyvespa to evaluate ranking functions from python. . !pip install pyvespa . pyvespa provides a python API to Vespa. It allow us to create, modify, deploy and interact with running Vespa instances. The main goal of the library is to allow for faster prototyping and to facilitate Machine Learning experiments for Vespa applications. . Format the labeled data into expected pyvespa format . pyvespa expects labeled data to follow the format illustrated below. It is a list of dict where each dict represents a query containing query_id, query and a list of relevant_docs. Each relevant document contains a required id key and an optional score key. . labeled_data = [ { &#39;query_id&#39;: 1, &#39;query&#39;: &#39;coronavirus origin&#39;, &#39;relevant_docs&#39;: [{&#39;id&#39;: &#39;005b2j4b&#39;, &#39;score&#39;: 2}, {&#39;id&#39;: &#39;00fmeepz&#39;, &#39;score&#39;: 1}] }, { &#39;query_id&#39;: 2, &#39;query&#39;: &#39;coronavirus response to weather changes&#39;, &#39;relevant_docs&#39;: [{&#39;id&#39;: &#39;01goni72&#39;, &#39;score&#39;: 2}, {&#39;id&#39;: &#39;03h85lvy&#39;, &#39;score&#39;: 2}] } ] . We can create labeled_data from the topics and relevance_data that we downloaded before. We are only going to include documents with relevance score &gt; 0 into the final list. . labeled_data = [ { &quot;query_id&quot;: int(topic_id), &quot;query&quot;: topics[topic_id][&quot;query&quot;], &quot;relevant_docs&quot;: [ { &quot;id&quot;: row[&quot;cord_uid&quot;], &quot;score&quot;: row[&quot;relevancy&quot;] } for idx, row in relevance_data[relevance_data.topic_id == int(topic_id)].iterrows() if row[&quot;relevancy&quot;] &gt; 0 ] } for topic_id in topics.keys()] . Define query models to be evaluated . We are going to define two query models to be evaluated here. Both will match all the documents that share at least one term with the query. This is defined by setting match_phase = OR(). . The difference between the query models happens in the ranking phase. The or_default model will rank documents based on nativeRank while the or_bm25 model will rank documents based on BM25. Discussion about those two types of ranking is out of the scope of this tutorial. It is enough to know that they rank documents according to two different formulas. . Those ranking profiles were defined by the team behind the cord19 app and can be found here. . from vespa.query import Query, RankProfile, OR query_models = { &quot;or_default&quot;: Query( match_phase = OR(), rank_profile = RankProfile(name=&quot;default&quot;) ), &quot;or_bm25&quot;: Query( match_phase = OR(), rank_profile = RankProfile(name=&quot;bm25t5&quot;) ) } . Define metrics to be used in the evaluation . We would like to compute the following metrics: . The percentage of documents matched by the query . | Recall @ 10 . | Reciprocal rank @ 10 . | NDCG @ 10 . | . from vespa.evaluation import ( MatchRatio, Recall, ReciprocalRank, NormalizedDiscountedCumulativeGain, ) eval_metrics = [ MatchRatio(), Recall(at=10), ReciprocalRank(at=10), NormalizedDiscountedCumulativeGain(at=10) ] . Evaluate . Connect to a running Vespa instance: . from vespa.application import Vespa app = Vespa(url = &quot;https://api.cord19.vespa.ai&quot;) . Compute the metrics defined above for each query model and store the results in a dictionary. . evaluations = {} for query_model in query_models: evaluations[query_model] = app.evaluate( labeled_data = labeled_data, eval_metrics = eval_metrics, query_model = query_models[query_model], id_field = &quot;cord_uid&quot;, hits = 10 ) . Analyze results . Let’s first combine the data into one DataFrame in a format to facilitate a comparison between query models. . import pandas as pd metric_values = [] for query_model in query_models: for metric in eval_metrics: metric_values.append( pd.DataFrame( data={ &quot;query_model&quot;: query_model, &quot;metric&quot;: metric.name, &quot;value&quot;: evaluations[query_model][metric.name + &quot;_value&quot;].to_list() } ) ) metric_values = pd.concat(metric_values, ignore_index=True) metric_values.head() . query_model metric value . 0 or_default | match_ratio | 0.231523 | . 1 or_default | match_ratio | 0.755509 | . 2 or_default | match_ratio | 0.265400 | . 3 or_default | match_ratio | 0.843403 | . 4 or_default | match_ratio | 0.901592 | . We can see below that the query model based on BM25 is superior across all metrics considered here. . metric_values.groupby([&#39;query_model&#39;, &#39;metric&#39;]).mean() . value . query_model metric . or_bm25 match_ratio 0.412386 | . ndcg_10 0.651929 | . recall_10 0.007654 | . reciprocal_rank_10 0.610270 | . or_default match_ratio 0.412386 | . ndcg_10 0.602556 | . recall_10 0.005435 | . reciprocal_rank_10 0.564437 | . We can also visualize the distribution of the metrics across the queries to get a better picture of the results. . import plotly.express as px fig = px.box( metric_values[metric_values.metric == &quot;ndcg_10&quot;], x=&quot;query_model&quot;, y=&quot;value&quot;, title=&quot;Ndgc @ 10&quot;, points=&quot;all&quot; ) fig.show() .",
            "url": "https://thigm85.github.io/blog/search/cord19/vespa/pyvespa/2020/11/05/cord19-connect-evaluate.html",
            "relUrl": "/search/cord19/vespa/pyvespa/2020/11/05/cord19-connect-evaluate.html",
            "date": " • Nov 5, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Fine-tuning a BERT model for search applications",
            "content": "Load the dataset . In order to fine-tune the BERT models for the cord19 application we need to generate a set of query-document features as well as labels that indicate which documents are relevant for the specific queries. For this exercise we will use the query string to represent the query and the title string to represent the documents. . The file labelled_data.json contains information about the query string and the file training_all_judgement_data.csv contain information about labels and title string. Those files were created and covered elsewhere but you can download them here and here. . import json from pandas import read_csv labelled_data = json.load(open(&quot;labelled_data_all.json&quot;, &quot;r&quot;)) training_data = read_csv(&quot;training_all_jugdments_data.csv&quot;) . training_data has almost everything we need, except the query string. . training_data.head() . document_id query_id label title-full . 0 005b2j4b | 1 | 2 | Monophyletic Relationship between Severe Acute... | . 1 00fmeepz | 1 | 1 | Comprehensive overview of COVID-19 based on cu... | . 2 010vptx3 | 1 | 2 | The SARS, MERS and novel coronavirus (COVID-19... | . 3 0194oljo | 1 | 1 | Evidence for zoonotic origins of Middle East r... | . 4 021q9884 | 1 | 1 | Deadly virus effortlessly hops species | . The query string can be obtained from the labelled_data. . print(labelled_data[0][&quot;query_id&quot;], labelled_data[0][&quot;query&quot;]) . 1 coronavirus origin . Compatible BERT encodings . Since we are training a model that will be deployed in a search application, we need to ensure that the training encodings are compatible with encodings used at serving time. At serving time, document encodings will be applied offline when feeding the documents to the search engine while the query encoding will be applied at run-time upon arrival of the query. In addition, it might be relevant to use different maximum length for queries and documents. . def create_bert_encodings(queries, docs, tokenizer, query_input_size, doc_input_size): queries_encodings = tokenizer( queries, truncation=True, max_length=query_input_size-2, add_special_tokens=False ) docs_encodings = tokenizer( docs, truncation=True, max_length=doc_input_size-1, add_special_tokens=False ) TOKEN_NONE=0 TOKEN_CLS=101 TOKEN_SEP=102 input_ids = [] token_type_ids = [] attention_mask = [] for query_input_ids, doc_input_ids in zip(queries_encodings[&quot;input_ids&quot;], docs_encodings[&quot;input_ids&quot;]): # create input id input_id = [TOKEN_CLS] + query_input_ids + [TOKEN_SEP] + doc_input_ids + [TOKEN_SEP] number_tokens = len(input_id) padding_length = max(128 - number_tokens, 0) input_id = input_id + [TOKEN_NONE] * padding_length input_ids.append(input_id) # create token id token_type_id = [0] * len([TOKEN_CLS] + query_input_ids + [TOKEN_SEP]) + [1] * len(doc_input_ids + [TOKEN_SEP]) + [TOKEN_NONE] * padding_length token_type_ids.append(token_type_id) # create attention_mask attention_mask.append([1] * number_tokens + [TOKEN_NONE] * padding_length) encodings = { &quot;input_ids&quot;: input_ids, &quot;token_type_ids&quot;: token_type_ids, &quot;attention_mask&quot;: attention_mask } return encodings . Create Datasets . Create a list for queries (represented by the query string), docs (represented by the doc titles) and labels from the labelled_data and training_data that we loaded earlier. . train_queries = [] train_docs = [] train_labels = [] for data_point in labelled_data: query_id = data_point[&quot;query_id&quot;] titles = training_data[training_data[&quot;query_id&quot;] == query_id][&quot;title-full&quot;].tolist() train_docs.extend(titles) train_labels.extend([1 if x &gt; 0 else 0 for x in training_data[training_data[&quot;query_id&quot;] == query_id][&quot;label&quot;].tolist()]) query = data_point[&quot;query&quot;] train_queries.extend([query] * len(titles)) . We are going to use a simple data split into train and validation sets for illustration purposes. The cord19 use case probably needs cross-validation to be used since it has only 50 queries containing relevance judgement. . from sklearn.model_selection import train_test_split train_queries, val_queries, train_docs, val_docs, train_labels, val_labels = train_test_split( train_queries, train_docs, train_labels, test_size=.2 ) . Create train and validation encodings. In order to do that we need to chose which BERT model to use, and the maximum size used for the resulting query and document vector. . model_name = &quot;google/bert_uncased_L-4_H-512_A-8&quot; query_input_size=24 doc_input_size=64 . from transformers import BertTokenizerFast tokenizer = BertTokenizerFast.from_pretrained(model_name) train_encodings = create_bert_encodings( queries=train_queries, docs=train_docs, tokenizer=tokenizer, query_input_size=query_input_size, doc_input_size=doc_input_size ) val_encodings = create_bert_encodings( queries=val_queries, docs=val_docs, tokenizer=tokenizer, query_input_size=query_input_size, doc_input_size=doc_input_size ) . Now that we have the encodings and the labels we can create a Dataset object as described in the transformers webpage about custom datasets. . import torch class Cord19Dataset(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) train_dataset = Cord19Dataset(train_encodings, train_labels) val_dataset = Cord19Dataset(val_encodings, val_labels) . Fine-tune the BERT model . We can then fine-tune the model (only task specific weights). Below is a basic routine with out-of-the-box set of parameters. Care should be taken when chosing the parameters below, but this is out of the scope of this piece. . from transformers import BertForSequenceClassification, Trainer, TrainingArguments training_args = TrainingArguments( output_dir=&#39;./results&#39;, # output directory num_train_epochs=3, # total number of training epochs per_device_train_batch_size=16, # batch size per device during training per_device_eval_batch_size=64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay logging_dir=&#39;./logs&#39;, # directory for storing logs logging_steps=10, ) model = BertForSequenceClassification.from_pretrained(model_name) for param in model.base_model.parameters(): param.requires_grad = False trainer = Trainer( model=model, # the instantiated 🤗 Transformers model to be trained args=training_args, # training arguments, defined above train_dataset=train_dataset, # training dataset eval_dataset=val_dataset # evaluation dataset ) trainer.train() . Export the model to onnx . Once training is complete we can export the model using the ONNX format. . from torch.onnx import export from pathlib import Path model_onnx_path = Path(model_name + &quot;.onnx&quot;) dummy_input = ( train_dataset[0][&quot;input_ids&quot;].unsqueeze(0), train_dataset[0][&quot;token_type_ids&quot;].unsqueeze(0), train_dataset[0][&quot;attention_mask&quot;].unsqueeze(0) ) input_names = [&quot;input_ids&quot;, &quot;token_type_ids&quot;, &quot;attention_mask&quot;] output_names = [&quot;logits&quot;] export( model, dummy_input, model_onnx_path, input_names = input_names, output_names = output_names, verbose=False, opset_version=11 ) .",
            "url": "https://thigm85.github.io/blog/search/cord19/bert/transformers/2020/10/29/fine-tune-bert-search-basic.html",
            "relUrl": "/search/cord19/bert/transformers/2020/10/29/fine-tune-bert-search-basic.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "How to connect and interact with search applications from python",
            "content": "Vespa is the faster, more scalable and advanced search engine currently available, imho. It has a native tensor evaluation framework, can perform approximate nearest neighbor search and deploy the latest advancencements in NLP modeling, such as BERT models. This post will give you an overview of the Vespa python API available through the pyvespa library. The main goal of the library is to allow for faster prototyping and to facilitate Machine Learning experiments for Vespa applications. . We are going to connect to the CORD-19 search app and use it as an example here. You can later use your own application to replicate the following steps. Future posts will go deeper into each topic described in this overview tutorial. . You can also run the steps contained here from Google Colab. . Install . **Warning**: The library is under active development and backward incompatible changes may occur. The library is available at PyPI and therefore can be installed with pip. . !pip install pyvespa . Connect to a running Vespa application . We can connect to a running Vespa application by creating an instance of Vespa with the appropriate url. The resulting app will then be used to communicate with the application. . from vespa.application import Vespa app = Vespa(url = &quot;https://api.cord19.vespa.ai&quot;) . Define a Query model . Easily define matching and ranking criteria . When building a search application, we usually want to experiment with different query models. A Query model consists of a match phase and a ranking phase. The matching phase will define how to match documents based on the query sent and the ranking phase will define how to rank the matched documents. Both phases can get quite complex and being able to easily express and experiment with them is very valuable. . In the example below we define the match phase to be the Union of the WeakAnd and the ANN operators. The WeakAnd will match documents based on query terms while the Approximate Nearest Neighbor (ANN) operator will match documents based on the distance between the query and document embeddings. This is an illustration of how easy it is to combine term and semantic matching in Vespa. . from vespa.query import Union, WeakAnd, ANN from random import random match_phase = Union( WeakAnd(hits = 10), ANN( doc_vector=&quot;title_embedding&quot;, query_vector=&quot;title_vector&quot;, embedding_model=lambda x: [random() for x in range(768)], hits = 10, label=&quot;title&quot; ) ) . We then define the ranking to be done by the bm25 rank-profile that is already defined in the application schema. We set list_features=True to be able to collect ranking-features later in this tutorial. After defining the match_phase and the rank_profile we can instantiate the Query model. . from vespa.query import Query, RankProfile rank_profile = RankProfile(name=&quot;bm25&quot;, list_features=True) query_model = Query(match_phase=match_phase, rank_profile=rank_profile) . Query the vespa app . Send queries via the query API. See the query page for more examples. . We can use the query_model that we just defined to issue queries to the application via the query method. . query_result = app.query( query=&quot;Is remdesivir an effective treatment for COVID-19?&quot;, query_model=query_model ) . We can see the number of documents that were retrieved by Vespa: . query_result.number_documents_retrieved . 1121 . And the number of documents that were returned to us: . len(query_result.hits) . 10 . Labelled data . How to structure labelled data . We often need to either evaluate query models or to collect data to improve query models through ML. In both cases we usually need labelled data. Let&#39;s create some labelled data to illustrate their expected format and their usage in the library. . Each data point contains a query_id, a query and relevant_docs associated with the query. . labelled_data = [ { &quot;query_id&quot;: 0, &quot;query&quot;: &quot;Intrauterine virus infections and congenital heart disease&quot;, &quot;relevant_docs&quot;: [{&quot;id&quot;: 0, &quot;score&quot;: 1}, {&quot;id&quot;: 3, &quot;score&quot;: 1}] }, { &quot;query_id&quot;: 1, &quot;query&quot;: &quot;Clinical and immunologic studies in identical twins discordant for systemic lupus erythematosus&quot;, &quot;relevant_docs&quot;: [{&quot;id&quot;: 1, &quot;score&quot;: 1}, {&quot;id&quot;: 5, &quot;score&quot;: 1}] } ] . Non-relevant documents are assigned &quot;score&quot;: 0 by default. Relevant documents will be assigned &quot;score&quot;: 1 by default if the field is missing from the labelled data. The defaults for both relevant and non-relevant documents can be modified on the appropriate methods. . Collect training data . Collect training data to analyse and/or improve ranking functions. See the collect training data page for more examples. . We can collect training data with the collect_training_data method according to a specific Query model. Below we will collect two documents for each query in addition to the relevant ones. . training_data_batch = app.collect_training_data( labelled_data = labelled_data, id_field = &quot;id&quot;, query_model = query_model, number_additional_docs = 2, fields = [&quot;rankfeatures&quot;] ) . Many rank features are returned by default. We can select some of them to inspect: . training_data_batch[ [ &quot;document_id&quot;, &quot;query_id&quot;, &quot;label&quot;, &quot;textSimilarity(title).proximity&quot;, &quot;textSimilarity(title).queryCoverage&quot;, &quot;textSimilarity(title).score&quot; ] ] . document_id query_id label textSimilarity(title).proximity textSimilarity(title).queryCoverage textSimilarity(title).score . 0 0 | 0 | 1 | 0.000000 | 0.142857 | 0.055357 | . 1 255164 | 0 | 0 | 1.000000 | 1.000000 | 1.000000 | . 2 145189 | 0 | 0 | 0.739583 | 0.571429 | 0.587426 | . 3 3 | 0 | 1 | 0.437500 | 0.142857 | 0.224554 | . 4 255164 | 0 | 0 | 1.000000 | 1.000000 | 1.000000 | . 5 145189 | 0 | 0 | 0.739583 | 0.571429 | 0.587426 | . 6 1 | 1 | 1 | 0.000000 | 0.083333 | 0.047222 | . 7 232555 | 1 | 0 | 1.000000 | 1.000000 | 1.000000 | . 8 13944 | 1 | 0 | 1.000000 | 0.250000 | 0.612500 | . 9 5 | 1 | 1 | 0.000000 | 0.083333 | 0.041667 | . 10 232555 | 1 | 0 | 1.000000 | 1.000000 | 1.000000 | . 11 13944 | 1 | 0 | 1.000000 | 0.250000 | 0.612500 | . Evaluating a query model . Define metrics and evaluate query models. See the evaluation page for more examples. . We will define the following evaluation metrics: . % of documents retrieved per query . | recall @ 10 per query . | MRR @ 10 per query . | . from vespa.evaluation import MatchRatio, Recall, ReciprocalRank eval_metrics = [MatchRatio(), Recall(at=10), ReciprocalRank(at=10)] . Evaluate: . evaluation = app.evaluate( labelled_data = labelled_data, eval_metrics = eval_metrics, query_model = query_model, id_field = &quot;id&quot;, ) evaluation . query_id match_ratio_retrieved_docs match_ratio_docs_available match_ratio_value recall_10_value reciprocal_rank_10_value . 0 0 | 1192 | 309201 | 0.003855 | 0.0 | 0 | . 1 1 | 1144 | 309201 | 0.003700 | 0.0 | 0 | .",
            "url": "https://thigm85.github.io/blog/vespa/pyvespa/search/cord19/2020/10/28/connect-to-vespa-instance.html",
            "relUrl": "/vespa/pyvespa/search/cord19/2020/10/28/connect-to-vespa-instance.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Sequential feature selection applied to ranking features",
            "content": "Load data collected from Vespa . The dataset used here were created by collecting ranking features from Vespa associated with the labelled data released by the round 3 of the TREC-CORD competition. . vespa_cord19.head(2) . topic_id iteration cord_uid relevancy query query-rewrite query-vector question narrative fieldMatch(abstract) ... fieldLength(abstract) fieldLength(body_text) fieldLength(title) freshness(timestamp) nativeRank(abstract) nativeRank(abstract_t5) nativeRank(title) rawScore(specter_embedding) rawScore(abstract_embedding) rawScore(title_embedding) . 0 1 | 0.5 | 010vptx3 | 2 | coronavirus origin | coronavirus origin origin COVID-19 information... | (0.28812721371650696, 1.558979868888855, 0.481... | what is the origin of COVID-19 | seeking range of information about the SARS-Co... | 0.111406 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 1 | 2.0 | p0kv1pht | 1 | coronavirus origin | coronavirus origin origin COVID-19 information... | (0.28812721371650696, 1.558979868888855, 0.481... | what is the origin of COVID-19 | seeking range of information about the SARS-Co... | 0.094629 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 rows × 172 columns . Ranking features available . There are 163 ranking features available. . features = [ x for x in list(vespa_cord19.columns) if x not in [ &#39;topic_id&#39;, &#39;iteration&#39;, &#39;cord_uid&#39;, &#39;relevancy&#39;, &#39;binary_relevance&#39;, &#39;query&#39;, &#39;query-rewrite&#39;, &#39;query-vector&#39;, &#39;question&#39;, &#39;narrative&#39; ] ] print(len(features)) . 163 . Simplify target label . The original labelled data has three types of label: 0, 1 and 2. To simplify we will consider just two labels here. The document is either relevant (label = 1) or irrelevant (label = 0) . vespa_cord19[&quot;binary_relevance&quot;] = vespa_cord19.apply(lambda row: 1 if row[&quot;relevancy&quot;] &gt; 0 else 0, axis=1) vespa_cord19[[&#39;relevancy&#39;, &#39;binary_relevance&#39;]].head() . relevancy binary_relevance . 0 2 | 1 | . 1 1 | 1 | . 2 2 | 1 | . 3 0 | 0 | . 4 0 | 0 | . Define the model used . from sklearn.linear_model import LogisticRegression estimator = LogisticRegression(penalty=&#39;none&#39;, fit_intercept=True) . Create custom score function . The function needs to have the following format: score_func(y, y_pred, **kwargs). . from math import log from sklearn.metrics import make_scorer def compute_log_prob(y, y_pred): return sum([target*log(prob) + (1-target)*log(1-prob) for prob, target in zip(y_pred, y)]) scorer = make_scorer(score_func=compute_log_prob, needs_proba=True) . Data used . y = vespa_cord19.binary_relevance X = vespa_cord19[features] . Feature Selector . The code below was used to generate the sequential feature selection results that will be analysed in this report. It takes some hours to run on a machine with 32 CPUs. . import os import pandas as pd from mlxtend.feature_selection import SequentialFeatureSelector as SFS columns_to_write = [ &quot;avg_score&quot;, &quot;ci_bound&quot;, &quot;cv_scores&quot;, &quot;feature_idx&quot;, &quot;feature_names&quot;, &quot;std_dev&quot;, &quot;std_err&quot;, &quot;forward&quot;, &quot;floating&quot; ] output_file_name = &quot;sequential_feature.csv&quot; for forward in [True, False]: for floating in [False, True]: sfs = None sfs_df = None sfs = SFS( estimator=estimator, k_features=(1,len(features)), forward=forward, floating=floating, scoring=scorer, cv=4, n_jobs=-1, verbose=1 ) sfs = sfs.fit(X, y) sfs_df = pd.DataFrame.from_dict(sfs.get_metric_dict()).T sfs_df[&quot;forward&quot;] = forward sfs_df[&quot;floating&quot;] = floating if not os.path.isfile(output_file_name): sfs_df.to_csv(output_file_name, index = False, header = True, columns=columns_to_write) columns = list(sfs_df.columns) else: sfs_df.to_csv(output_file_name, mode = &quot;a&quot;, header=False, index = False, columns=columns_to_write) results.append(sfs) . Loading the pre-computed results . from pandas import read_csv data = read_csv(&quot;sequential_feature.csv&quot;) . data.head(2) . avg_score ci_bound cv_scores feature_idx feature_names std_dev std_err forward floating . 0 -2781.720568 | 82.261812 | [-2755.82180826 -2795.84549409 -2856.86996371 ... | (23,) | (&#39;fieldMatch(abstract).significance&#39;,) | 51.318010 | 29.628467 | True | False | . 1 -2728.851119 | 68.207668 | [-2778.41736416 -2698.51375201 -2762.13271275 ... | (10, 23) | (&#39;fieldMatch(abstract).importance&#39;, &#39;fieldMatc... | 42.550507 | 24.566547 | True | False | . Utility functions . from pandas import DataFrame . def create_dfs(df): cv_scores, number_features, feature_names = [], [], [] for idx, row in df.iterrows(): scores = [float(x) for x in row[&quot;cv_scores&quot;].lstrip(&quot;[&quot;).rstrip(&quot;]&quot;).split()] cv_scores.extend(scores) number_features.extend([len(row[&quot;feature_idx&quot;].lstrip(&quot;(&quot;).rstrip(&quot;)&quot;).split(&quot;,&quot;))] * len(scores)) feature_names.extend([row[&quot;feature_names&quot;]] * len(scores)) sequential_selection = DataFrame( data={ &quot;cv_scores&quot;: cv_scores, &quot;number_features&quot;: number_features, &quot;feature_names&quot;: feature_names } ) features = sequential_selection[[&quot;number_features&quot;, &quot;feature_names&quot;]].sort_values(&#39;number_features&#39;).drop_duplicates([&#39;number_features&#39;]) return sequential_selection, features . def display_feature_names(df, number_features): return [x.strip().strip(&quot;&#39;&quot;) for x in df[df.number_features == number_features].iloc[0][&quot;feature_names&quot;].lstrip(&quot;(&quot;).rstrip(&quot;)&quot;).split(&quot;,&quot;)] . Forward sequential selection . forward_df = data[(data[&quot;forward&quot;] == True) &amp; (data[&quot;floating&quot;] == False)] . forward_sequential_selection, forward_features = create_dfs(forward_df) . import plotly.express as px fig = px.box(forward_sequential_selection, x = &quot;number_features&quot;, y=&quot;cv_scores&quot;) fig.show() . display_feature_names(forward_features, 7) . [&#39;fieldMatch(abstract)&#39;, &#39;fieldMatch(abstract).importance&#39;, &#39;fieldMatch(abstract).significance&#39;, &#39;fieldMatch(body_text).absoluteProximity&#39;, &#39;fieldMatch(body_text).fieldCompleteness&#39;, &#39;textSimilarity(body_text).queryCoverage&#39;, &#39;bm25(title)&#39;] . Forward selection with floating . forward_floating_df = data[(data[&quot;forward&quot;] == True) &amp; (data[&quot;floating&quot;] == True)] . forward_floating_sequential_selection, forward_floating_features = create_dfs(forward_floating_df) . import plotly.express as px fig = px.box(forward_floating_sequential_selection, x = &quot;number_features&quot;, y=&quot;cv_scores&quot;) fig.show() . display_feature_names(forward_floating_features, 7) . [&#39;fieldMatch(abstract)&#39;, &#39;fieldMatch(abstract).importance&#39;, &#39;fieldMatch(abstract).significance&#39;, &#39;fieldMatch(body_text).absoluteProximity&#39;, &#39;fieldMatch(body_text).fieldCompleteness&#39;, &#39;textSimilarity(body_text).queryCoverage&#39;, &#39;bm25(title)&#39;] . Backward sequential selection . backward_df = data[(data[&quot;forward&quot;] == False) &amp; (data[&quot;floating&quot;] == False)] . backward_sequential_selection, backward_features = create_dfs(backward_df) . import plotly.express as px fig = px.box(backward_sequential_selection, x = &quot;number_features&quot;, y=&quot;cv_scores&quot;) fig.show() . display_feature_names(backward_features, 7) . [&#39;fieldMatch(abstract).longestSequence&#39;, &#39;fieldMatch(body_text).proximity&#39;, &#39;fieldMatch(body_text).weight&#39;, &#39;fieldMatch(title).matches&#39;, &#39;textSimilarity(title).queryCoverage&#39;, &#39;bm25(abstract)&#39;, &#39;bm25(title)&#39;] . Backward selection with floating . backward_floating_df = data[(data[&quot;forward&quot;] == False) &amp; (data[&quot;floating&quot;] == True)] . backward_floating_sequential_selection, backward_floating_features = create_dfs(backward_floating_df) . import plotly.express as px fig = px.box(backward_floating_sequential_selection, x = &quot;number_features&quot;, y=&quot;cv_scores&quot;) fig.show() . display_feature_names(backward_floating_features, 7) . [&#39;fieldMatch(abstract).absoluteOccurrence&#39;, &#39;fieldMatch(body_text).proximity&#39;, &#39;fieldMatch(body_text).significance&#39;, &#39;fieldMatch(body_text).weight&#39;, &#39;textSimilarity(body_text).fieldCoverage&#39;, &#39;textSimilarity(title).queryCoverage&#39;, &#39;bm25(abstract)&#39;] . Summary . DataFrame(data = { &quot;forward&quot;: display_feature_names(forward_features, 7), &quot;forward_floating&quot;: display_feature_names(forward_floating_features, 7), &quot;backward&quot;: display_feature_names(backward_features, 7), &quot;backward_floating&quot;: display_feature_names(backward_floating_features, 7) }) . forward forward_floating backward backward_floating . 0 fieldMatch(abstract) | fieldMatch(abstract) | fieldMatch(abstract).longestSequence | fieldMatch(abstract).absoluteOccurrence | . 1 fieldMatch(abstract).importance | fieldMatch(abstract).importance | fieldMatch(body_text).proximity | fieldMatch(body_text).proximity | . 2 fieldMatch(abstract).significance | fieldMatch(abstract).significance | fieldMatch(body_text).weight | fieldMatch(body_text).significance | . 3 fieldMatch(body_text).absoluteProximity | fieldMatch(body_text).absoluteProximity | fieldMatch(title).matches | fieldMatch(body_text).weight | . 4 fieldMatch(body_text).fieldCompleteness | fieldMatch(body_text).fieldCompleteness | textSimilarity(title).queryCoverage | textSimilarity(body_text).fieldCoverage | . 5 textSimilarity(body_text).queryCoverage | textSimilarity(body_text).queryCoverage | bm25(abstract) | textSimilarity(title).queryCoverage | . 6 bm25(title) | bm25(title) | bm25(title) | bm25(abstract) | .",
            "url": "https://thigm85.github.io/blog/ranking%20features/vespa/feature%20selection/model%20selection/2020/06/19/sequential-feature-selection-applied-to-ranking-features.html",
            "relUrl": "/ranking%20features/vespa/feature%20selection/model%20selection/2020/06/19/sequential-feature-selection-applied-to-ranking-features.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Reducing the number of initial features to select",
            "content": "Load data collected from Vespa . The dataset used here were created by collecting ranking features from Vespa associated with the labelled data released by the round 3 of the TREC-CORD competition. . vespa_cord19.to_csv(&quot;data/2020-05-27-subset-selection/training_features.csv&quot;, index=False) . vespa_cord19.head(2) . topic_id iteration cord_uid relevancy query query-rewrite query-vector question narrative fieldMatch(abstract) ... fieldLength(abstract) fieldLength(body_text) fieldLength(title) freshness(timestamp) nativeRank(abstract) nativeRank(abstract_t5) nativeRank(title) rawScore(specter_embedding) rawScore(abstract_embedding) rawScore(title_embedding) . 0 1 | 0.5 | 010vptx3 | 2 | coronavirus origin | coronavirus origin origin COVID-19 information... | (0.28812721371650696, 1.558979868888855, 0.481... | what is the origin of COVID-19 | seeking range of information about the SARS-Co... | 0.111406 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 1 | 2.0 | p0kv1pht | 1 | coronavirus origin | coronavirus origin origin COVID-19 information... | (0.28812721371650696, 1.558979868888855, 0.481... | what is the origin of COVID-19 | seeking range of information about the SARS-Co... | 0.094629 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 rows × 172 columns . Ranking features available . There are 163 ranking features available. . features = [ x for x in list(vespa_cord19.columns) if x not in [ &#39;topic_id&#39;, &#39;iteration&#39;, &#39;cord_uid&#39;, &#39;relevancy&#39;, &#39;binary_relevance&#39;, &#39;query&#39;, &#39;query-rewrite&#39;, &#39;query-vector&#39;, &#39;question&#39;, &#39;narrative&#39; ] ] print(len(features)) . 163 . features . [&#39;fieldMatch(abstract)&#39;, &#39;fieldMatch(abstract).absoluteOccurrence&#39;, &#39;fieldMatch(abstract).absoluteProximity&#39;, &#39;fieldMatch(abstract).completeness&#39;, &#39;fieldMatch(abstract).degradedMatches&#39;, &#39;fieldMatch(abstract).earliness&#39;, &#39;fieldMatch(abstract).fieldCompleteness&#39;, &#39;fieldMatch(abstract).gapLength&#39;, &#39;fieldMatch(abstract).gaps&#39;, &#39;fieldMatch(abstract).head&#39;, &#39;fieldMatch(abstract).importance&#39;, &#39;fieldMatch(abstract).longestSequence&#39;, &#39;fieldMatch(abstract).longestSequenceRatio&#39;, &#39;fieldMatch(abstract).matches&#39;, &#39;fieldMatch(abstract).occurrence&#39;, &#39;fieldMatch(abstract).orderness&#39;, &#39;fieldMatch(abstract).outOfOrder&#39;, &#39;fieldMatch(abstract).proximity&#39;, &#39;fieldMatch(abstract).queryCompleteness&#39;, &#39;fieldMatch(abstract).relatedness&#39;, &#39;fieldMatch(abstract).segmentDistance&#39;, &#39;fieldMatch(abstract).segmentProximity&#39;, &#39;fieldMatch(abstract).segments&#39;, &#39;fieldMatch(abstract).significance&#39;, &#39;fieldMatch(abstract).significantOccurrence&#39;, &#39;fieldMatch(abstract).tail&#39;, &#39;fieldMatch(abstract).unweightedProximity&#39;, &#39;fieldMatch(abstract).weight&#39;, &#39;fieldMatch(abstract).weightedAbsoluteOccurrence&#39;, &#39;fieldMatch(abstract).weightedOccurrence&#39;, &#39;fieldMatch(abstract_t5)&#39;, &#39;fieldMatch(abstract_t5).absoluteOccurrence&#39;, &#39;fieldMatch(abstract_t5).absoluteProximity&#39;, &#39;fieldMatch(abstract_t5).completeness&#39;, &#39;fieldMatch(abstract_t5).degradedMatches&#39;, &#39;fieldMatch(abstract_t5).earliness&#39;, &#39;fieldMatch(abstract_t5).fieldCompleteness&#39;, &#39;fieldMatch(abstract_t5).gapLength&#39;, &#39;fieldMatch(abstract_t5).gaps&#39;, &#39;fieldMatch(abstract_t5).head&#39;, &#39;fieldMatch(abstract_t5).importance&#39;, &#39;fieldMatch(abstract_t5).longestSequence&#39;, &#39;fieldMatch(abstract_t5).longestSequenceRatio&#39;, &#39;fieldMatch(abstract_t5).matches&#39;, &#39;fieldMatch(abstract_t5).occurrence&#39;, &#39;fieldMatch(abstract_t5).orderness&#39;, &#39;fieldMatch(abstract_t5).outOfOrder&#39;, &#39;fieldMatch(abstract_t5).proximity&#39;, &#39;fieldMatch(abstract_t5).queryCompleteness&#39;, &#39;fieldMatch(abstract_t5).relatedness&#39;, &#39;fieldMatch(abstract_t5).segmentDistance&#39;, &#39;fieldMatch(abstract_t5).segmentProximity&#39;, &#39;fieldMatch(abstract_t5).segments&#39;, &#39;fieldMatch(abstract_t5).significance&#39;, &#39;fieldMatch(abstract_t5).significantOccurrence&#39;, &#39;fieldMatch(abstract_t5).tail&#39;, &#39;fieldMatch(abstract_t5).unweightedProximity&#39;, &#39;fieldMatch(abstract_t5).weight&#39;, &#39;fieldMatch(abstract_t5).weightedAbsoluteOccurrence&#39;, &#39;fieldMatch(abstract_t5).weightedOccurrence&#39;, &#39;fieldMatch(body_text)&#39;, &#39;fieldMatch(body_text).absoluteOccurrence&#39;, &#39;fieldMatch(body_text).absoluteProximity&#39;, &#39;fieldMatch(body_text).completeness&#39;, &#39;fieldMatch(body_text).degradedMatches&#39;, &#39;fieldMatch(body_text).earliness&#39;, &#39;fieldMatch(body_text).fieldCompleteness&#39;, &#39;fieldMatch(body_text).gapLength&#39;, &#39;fieldMatch(body_text).gaps&#39;, &#39;fieldMatch(body_text).head&#39;, &#39;fieldMatch(body_text).importance&#39;, &#39;fieldMatch(body_text).longestSequence&#39;, &#39;fieldMatch(body_text).longestSequenceRatio&#39;, &#39;fieldMatch(body_text).matches&#39;, &#39;fieldMatch(body_text).occurrence&#39;, &#39;fieldMatch(body_text).orderness&#39;, &#39;fieldMatch(body_text).outOfOrder&#39;, &#39;fieldMatch(body_text).proximity&#39;, &#39;fieldMatch(body_text).queryCompleteness&#39;, &#39;fieldMatch(body_text).relatedness&#39;, &#39;fieldMatch(body_text).segmentDistance&#39;, &#39;fieldMatch(body_text).segmentProximity&#39;, &#39;fieldMatch(body_text).segments&#39;, &#39;fieldMatch(body_text).significance&#39;, &#39;fieldMatch(body_text).significantOccurrence&#39;, &#39;fieldMatch(body_text).tail&#39;, &#39;fieldMatch(body_text).unweightedProximity&#39;, &#39;fieldMatch(body_text).weight&#39;, &#39;fieldMatch(body_text).weightedAbsoluteOccurrence&#39;, &#39;fieldMatch(body_text).weightedOccurrence&#39;, &#39;fieldMatch(title)&#39;, &#39;fieldMatch(title).absoluteOccurrence&#39;, &#39;fieldMatch(title).absoluteProximity&#39;, &#39;fieldMatch(title).completeness&#39;, &#39;fieldMatch(title).degradedMatches&#39;, &#39;fieldMatch(title).earliness&#39;, &#39;fieldMatch(title).fieldCompleteness&#39;, &#39;fieldMatch(title).gapLength&#39;, &#39;fieldMatch(title).gaps&#39;, &#39;fieldMatch(title).head&#39;, &#39;fieldMatch(title).importance&#39;, &#39;fieldMatch(title).longestSequence&#39;, &#39;fieldMatch(title).longestSequenceRatio&#39;, &#39;fieldMatch(title).matches&#39;, &#39;fieldMatch(title).occurrence&#39;, &#39;fieldMatch(title).orderness&#39;, &#39;fieldMatch(title).outOfOrder&#39;, &#39;fieldMatch(title).proximity&#39;, &#39;fieldMatch(title).queryCompleteness&#39;, &#39;fieldMatch(title).relatedness&#39;, &#39;fieldMatch(title).segmentDistance&#39;, &#39;fieldMatch(title).segmentProximity&#39;, &#39;fieldMatch(title).segments&#39;, &#39;fieldMatch(title).significance&#39;, &#39;fieldMatch(title).significantOccurrence&#39;, &#39;fieldMatch(title).tail&#39;, &#39;fieldMatch(title).unweightedProximity&#39;, &#39;fieldMatch(title).weight&#39;, &#39;fieldMatch(title).weightedAbsoluteOccurrence&#39;, &#39;fieldMatch(title).weightedOccurrence&#39;, &#39;nativeFieldMatch&#39;, &#39;nativeProximity&#39;, &#39;nativeRank&#39;, &#39;textSimilarity(abstract).fieldCoverage&#39;, &#39;textSimilarity(abstract).order&#39;, &#39;textSimilarity(abstract).proximity&#39;, &#39;textSimilarity(abstract).queryCoverage&#39;, &#39;textSimilarity(abstract).score&#39;, &#39;textSimilarity(abstract_t5).fieldCoverage&#39;, &#39;textSimilarity(abstract_t5).order&#39;, &#39;textSimilarity(abstract_t5).proximity&#39;, &#39;textSimilarity(abstract_t5).queryCoverage&#39;, &#39;textSimilarity(abstract_t5).score&#39;, &#39;textSimilarity(body_text).fieldCoverage&#39;, &#39;textSimilarity(body_text).order&#39;, &#39;textSimilarity(body_text).proximity&#39;, &#39;textSimilarity(body_text).queryCoverage&#39;, &#39;textSimilarity(body_text).score&#39;, &#39;textSimilarity(body_text_t5).fieldCoverage&#39;, &#39;textSimilarity(body_text_t5).order&#39;, &#39;textSimilarity(body_text_t5).proximity&#39;, &#39;textSimilarity(body_text_t5).queryCoverage&#39;, &#39;textSimilarity(body_text_t5).score&#39;, &#39;textSimilarity(title).fieldCoverage&#39;, &#39;textSimilarity(title).order&#39;, &#39;textSimilarity(title).proximity&#39;, &#39;textSimilarity(title).queryCoverage&#39;, &#39;textSimilarity(title).score&#39;, &#39;attribute(has_full_text)&#39;, &#39;bm25(abstract)&#39;, &#39;bm25(abstract_t5)&#39;, &#39;bm25(body_text)&#39;, &#39;bm25(title)&#39;, &#39;fieldLength(abstract)&#39;, &#39;fieldLength(body_text)&#39;, &#39;fieldLength(title)&#39;, &#39;freshness(timestamp)&#39;, &#39;nativeRank(abstract)&#39;, &#39;nativeRank(abstract_t5)&#39;, &#39;nativeRank(title)&#39;, &#39;rawScore(specter_embedding)&#39;, &#39;rawScore(abstract_embedding)&#39;, &#39;rawScore(title_embedding)&#39;] . Simplify target label . The original labelled data has three types of label: 0, 1 and 2. To simplify we will consider just two labels here. The document is either relevant (label = 1) or irrelevant (label = 0) . vespa_cord19[&quot;binary_relevance&quot;] = vespa_cord19.apply(lambda row: 1 if row[&quot;relevancy&quot;] &gt; 0 else 0, axis=1) vespa_cord19[[&#39;relevancy&#39;, &#39;binary_relevance&#39;]].head() . relevancy binary_relevance . 0 2 | 1 | . 1 1 | 1 | . 2 2 | 1 | . 3 0 | 0 | . 4 0 | 0 | . Model . We are going to fit logistic regressions with the objective of maximizing the log probability of the observed outcome. . from sklearn.linear_model import LogisticRegression from statistics import mean def compute_mean_realize_log_prob(model, X, Y): return mean([x[int(y)] for x, y in zip(model.predict_log_proba(X), Y)]) def fit_logistic_reg(X, Y): model = LogisticRegression(penalty=&#39;none&#39;, fit_intercept=True) model.fit(X, Y) realized_log_prob = compute_mean_realize_log_prob(model, X, Y) return realized_log_prob . Subset selection routine . Below we run the subset selection algorithm with only one feature. . import itertools import pandas as pd from tqdm import tnrange, tqdm_notebook #Importing tqdm for the progress bar from tqdm.notebook import trange log_probs, feature_list = [], [] numb_features = [] max_number_features = min(1, len(features)) data = vespa_cord19 Y = data.binary_relevance X = data[features] for k in range(1,max_number_features + 1): for combo in itertools.combinations(X.columns,k): tmp_result = fit_logistic_reg(X[list(combo)],Y) log_probs.append(tmp_result) feature_list.append(combo) numb_features.append(len(combo)) #Store in DataFrame df = pd.DataFrame( { &#39;numb_features&#39;: numb_features, &#39;log_probs&#39;: log_probs, &#39;features&#39;:feature_list } ) . Analyze results . fine-grained results . df . numb_features log_probs features . 0 1 | -0.553448 | (fieldMatch(abstract),) | . 1 1 | -0.568826 | (fieldMatch(abstract).absoluteOccurrence,) | . 2 1 | -0.570800 | (fieldMatch(abstract).absoluteProximity,) | . 3 1 | -0.536799 | (fieldMatch(abstract).completeness,) | . 4 1 | -0.571269 | (fieldMatch(abstract).degradedMatches,) | . ... ... | ... | ... | . 158 1 | -0.571269 | (nativeRank(abstract_t5),) | . 159 1 | -0.571269 | (nativeRank(title),) | . 160 1 | -0.571269 | (rawScore(specter_embedding),) | . 161 1 | -0.571269 | (rawScore(abstract_embedding),) | . 162 1 | -0.571269 | (rawScore(title_embedding),) | . 163 rows × 3 columns . Plot average results across data samples . df[&#39;max_log_probs&#39;] = df.groupby(&#39;numb_features&#39;)[&#39;log_probs&#39;].transform(max) df . numb_features log_probs features max_log_probs . 0 1 | -0.553448 | (fieldMatch(abstract),) | -0.535476 | . 1 1 | -0.568826 | (fieldMatch(abstract).absoluteOccurrence,) | -0.535476 | . 2 1 | -0.570800 | (fieldMatch(abstract).absoluteProximity,) | -0.535476 | . 3 1 | -0.536799 | (fieldMatch(abstract).completeness,) | -0.535476 | . 4 1 | -0.571269 | (fieldMatch(abstract).degradedMatches,) | -0.535476 | . ... ... | ... | ... | ... | . 158 1 | -0.571269 | (nativeRank(abstract_t5),) | -0.535476 | . 159 1 | -0.571269 | (nativeRank(title),) | -0.535476 | . 160 1 | -0.571269 | (rawScore(specter_embedding),) | -0.535476 | . 161 1 | -0.571269 | (rawScore(abstract_embedding),) | -0.535476 | . 162 1 | -0.571269 | (rawScore(title_embedding),) | -0.535476 | . 163 rows × 4 columns . import matplotlib.pyplot as plt %matplotlib inline plt.style.use(&#39;ggplot&#39;) plt.scatter(df.numb_features,df.log_probs, alpha = .2, color = &#39;darkblue&#39;) plt.xlabel(&#39;# Features&#39;) plt.ylabel(&#39;log_probs&#39;) plt.title(&#39;Best subset selection&#39;) plt.plot(df.numb_features,df.max_log_probs, color = &#39;r&#39;, label = &#39;Best subset&#39;) plt.show() . Display the best features for each model size . df_max = df.sort_values(&#39;log_probs&#39;, ascending=False) . for f in df_max.features: print(f) . (&#39;fieldMatch(abstract).significance&#39;,) (&#39;fieldMatch(abstract).importance&#39;,) (&#39;fieldMatch(abstract).completeness&#39;,) (&#39;fieldMatch(abstract).queryCompleteness&#39;,) (&#39;fieldMatch(abstract).weight&#39;,) (&#39;bm25(abstract)&#39;,) (&#39;textSimilarity(abstract).queryCoverage&#39;,) (&#39;bm25(title)&#39;,) (&#39;fieldMatch(title).significance&#39;,) (&#39;fieldMatch(title).importance&#39;,) (&#39;fieldMatch(title).completeness&#39;,) (&#39;fieldMatch(title).weight&#39;,) (&#39;fieldMatch(title).queryCompleteness&#39;,) (&#39;textSimilarity(title).queryCoverage&#39;,) (&#39;fieldMatch(abstract).matches&#39;,) (&#39;fieldMatch(title).matches&#39;,) (&#39;fieldMatch(title)&#39;,) (&#39;nativeRank&#39;,) (&#39;nativeProximity&#39;,) (&#39;fieldMatch(abstract_t5).significance&#39;,) (&#39;nativeFieldMatch&#39;,) (&#39;fieldMatch(abstract_t5).importance&#39;,) (&#39;fieldMatch(abstract).segments&#39;,) (&#39;bm25(abstract_t5)&#39;,) (&#39;fieldMatch(abstract_t5).completeness&#39;,) (&#39;fieldMatch(abstract_t5).queryCompleteness&#39;,) (&#39;fieldMatch(abstract_t5).weight&#39;,) (&#39;textSimilarity(title).fieldCoverage&#39;,) (&#39;fieldMatch(title).occurrence&#39;,) (&#39;fieldMatch(title).fieldCompleteness&#39;,) (&#39;fieldMatch(abstract_t5).matches&#39;,) (&#39;textSimilarity(abstract_t5).queryCoverage&#39;,) (&#39;fieldMatch(title).longestSequence&#39;,) (&#39;fieldMatch(abstract_t5).fieldCompleteness&#39;,) (&#39;fieldMatch(abstract)&#39;,) (&#39;fieldMatch(title).gaps&#39;,) (&#39;fieldMatch(title).earliness&#39;,) (&#39;fieldMatch(abstract_t5).segments&#39;,) (&#39;textSimilarity(abstract_t5).fieldCoverage&#39;,) (&#39;textSimilarity(title).score&#39;,) (&#39;textSimilarity(abstract).score&#39;,) (&#39;fieldMatch(title).segments&#39;,) (&#39;textSimilarity(abstract).fieldCoverage&#39;,) (&#39;fieldMatch(abstract_t5).occurrence&#39;,) (&#39;fieldMatch(title).gapLength&#39;,) (&#39;fieldMatch(title).absoluteOccurrence&#39;,) (&#39;fieldMatch(title).weightedAbsoluteOccurrence&#39;,) (&#39;fieldMatch(abstract).occurrence&#39;,) (&#39;fieldMatch(abstract).fieldCompleteness&#39;,) (&#39;textSimilarity(title).proximity&#39;,) (&#39;fieldMatch(abstract_t5)&#39;,) (&#39;fieldMatch(abstract).longestSequence&#39;,) (&#39;fieldMatch(abstract_t5).longestSequence&#39;,) (&#39;textSimilarity(abstract_t5).score&#39;,) (&#39;fieldMatch(title).outOfOrder&#39;,) (&#39;fieldMatch(abstract).gapLength&#39;,) (&#39;textSimilarity(title).order&#39;,) (&#39;fieldMatch(abstract_t5).segmentDistance&#39;,) (&#39;fieldMatch(abstract).gaps&#39;,) (&#39;fieldMatch(title).relatedness&#39;,) (&#39;fieldMatch(abstract_t5).earliness&#39;,) (&#39;textSimilarity(abstract_t5).order&#39;,) (&#39;fieldMatch(title).segmentProximity&#39;,) (&#39;textSimilarity(abstract_t5).proximity&#39;,) (&#39;fieldMatch(title).unweightedProximity&#39;,) (&#39;fieldMatch(title).proximity&#39;,) (&#39;fieldMatch(title).absoluteProximity&#39;,) (&#39;fieldMatch(title).segmentDistance&#39;,) (&#39;textSimilarity(abstract).proximity&#39;,) (&#39;fieldMatch(title).orderness&#39;,) (&#39;fieldMatch(abstract_t5).absoluteOccurrence&#39;,) (&#39;fieldMatch(abstract_t5).weightedAbsoluteOccurrence&#39;,) (&#39;fieldMatch(abstract_t5).weightedOccurrence&#39;,) (&#39;fieldMatch(abstract_t5).significantOccurrence&#39;,) (&#39;fieldMatch(abstract).earliness&#39;,) (&#39;fieldMatch(abstract_t5).gapLength&#39;,) (&#39;fieldMatch(body_text).absoluteProximity&#39;,) (&#39;fieldMatch(body_text).proximity&#39;,) (&#39;fieldMatch(body_text).unweightedProximity&#39;,) (&#39;fieldMatch(abstract_t5).orderness&#39;,) (&#39;fieldMatch(abstract_t5).gaps&#39;,) (&#39;fieldMatch(body_text).segmentProximity&#39;,) (&#39;textSimilarity(abstract).order&#39;,) (&#39;fieldMatch(title).significantOccurrence&#39;,) (&#39;fieldMatch(title).weightedOccurrence&#39;,) (&#39;fieldMatch(body_text).head&#39;,) (&#39;fieldMatch(body_text).tail&#39;,) (&#39;fieldMatch(abstract_t5).unweightedProximity&#39;,) (&#39;fieldMatch(abstract_t5).absoluteProximity&#39;,) (&#39;fieldMatch(abstract_t5).proximity&#39;,) (&#39;fieldMatch(abstract).outOfOrder&#39;,) (&#39;fieldMatch(body_text).longestSequenceRatio&#39;,) (&#39;fieldMatch(body_text).gaps&#39;,) (&#39;fieldMatch(abstract).longestSequenceRatio&#39;,) (&#39;fieldMatch(body_text).orderness&#39;,) (&#39;fieldMatch(abstract).segmentDistance&#39;,) (&#39;fieldMatch(body_text).gapLength&#39;,) (&#39;fieldMatch(body_text).segmentDistance&#39;,) (&#39;fieldMatch(abstract_t5).outOfOrder&#39;,) (&#39;fieldMatch(abstract).segmentProximity&#39;,) (&#39;fieldMatch(abstract).absoluteOccurrence&#39;,) (&#39;fieldMatch(abstract).weightedAbsoluteOccurrence&#39;,) (&#39;fieldMatch(body_text).outOfOrder&#39;,) (&#39;fieldMatch(title).tail&#39;,) (&#39;fieldMatch(body_text).earliness&#39;,) (&#39;fieldMatch(abstract).orderness&#39;,) (&#39;fieldMatch(title).longestSequenceRatio&#39;,) (&#39;textSimilarity(body_text).order&#39;,) (&#39;fieldMatch(abstract).significantOccurrence&#39;,) (&#39;fieldMatch(abstract).weightedOccurrence&#39;,) (&#39;bm25(body_text)&#39;,) (&#39;textSimilarity(body_text).fieldCoverage&#39;,) (&#39;fieldMatch(abstract).tail&#39;,) (&#39;fieldMatch(body_text).fieldCompleteness&#39;,) (&#39;fieldMatch(body_text)&#39;,) (&#39;fieldMatch(title).head&#39;,) (&#39;fieldMatch(abstract_t5).relatedness&#39;,) (&#39;fieldMatch(abstract_t5).tail&#39;,) (&#39;fieldMatch(abstract_t5).segmentProximity&#39;,) (&#39;fieldMatch(body_text).relatedness&#39;,) (&#39;fieldMatch(body_text).weightedAbsoluteOccurrence&#39;,) (&#39;fieldMatch(body_text).absoluteOccurrence&#39;,) (&#39;fieldMatch(body_text).weightedOccurrence&#39;,) (&#39;fieldMatch(abstract).head&#39;,) (&#39;fieldMatch(abstract).unweightedProximity&#39;,) (&#39;fieldMatch(abstract).absoluteProximity&#39;,) (&#39;fieldMatch(abstract).proximity&#39;,) (&#39;fieldMatch(body_text).significantOccurrence&#39;,) (&#39;fieldMatch(body_text).occurrence&#39;,) (&#39;fieldMatch(body_text).segments&#39;,) (&#39;fieldMatch(abstract_t5).head&#39;,) (&#39;textSimilarity(body_text).queryCoverage&#39;,) (&#39;fieldMatch(body_text).longestSequence&#39;,) (&#39;fieldMatch(body_text).significance&#39;,) (&#39;fieldMatch(body_text).completeness&#39;,) (&#39;fieldMatch(body_text).queryCompleteness&#39;,) (&#39;fieldMatch(body_text).importance&#39;,) (&#39;textSimilarity(body_text).proximity&#39;,) (&#39;fieldMatch(body_text).weight&#39;,) (&#39;fieldMatch(abstract_t5).longestSequenceRatio&#39;,) (&#39;fieldMatch(body_text).matches&#39;,) (&#39;fieldMatch(abstract).relatedness&#39;,) (&#39;textSimilarity(body_text).score&#39;,) (&#39;nativeRank(title)&#39;,) (&#39;nativeRank(abstract)&#39;,) (&#39;freshness(timestamp)&#39;,) (&#39;rawScore(specter_embedding)&#39;,) (&#39;rawScore(abstract_embedding)&#39;,) (&#39;fieldLength(title)&#39;,) (&#39;fieldLength(body_text)&#39;,) (&#39;fieldLength(abstract)&#39;,) (&#39;nativeRank(abstract_t5)&#39;,) (&#39;fieldMatch(abstract_t5).degradedMatches&#39;,) (&#39;fieldMatch(abstract).degradedMatches&#39;,) (&#39;fieldMatch(title).degradedMatches&#39;,) (&#39;attribute(has_full_text)&#39;,) (&#39;fieldMatch(body_text).degradedMatches&#39;,) (&#39;textSimilarity(body_text_t5).score&#39;,) (&#39;textSimilarity(body_text_t5).queryCoverage&#39;,) (&#39;textSimilarity(body_text_t5).proximity&#39;,) (&#39;textSimilarity(body_text_t5).order&#39;,) (&#39;textSimilarity(body_text_t5).fieldCoverage&#39;,) (&#39;rawScore(title_embedding)&#39;,) . Conclusion . Using the predicting performance of individual features does not seem a good approach to eliminate features from a grid search by greedy algorithms. The reason is that many features that perform poorly when considered in isolation would shine when combined with other complementary features. .",
            "url": "https://thigm85.github.io/blog/ranking%20features/vespa/feature%20selection/model%20selection/2020/06/11/select-promissing-ranking-features.html",
            "relUrl": "/ranking%20features/vespa/feature%20selection/model%20selection/2020/06/11/select-promissing-ranking-features.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "Best subset selection of Vespa ranking features",
            "content": "Load data collected from Vespa . The dataset used here were created by collecting ranking features from Vespa associated with the labelled data released by the round 3 of the TREC-CORD competition. . vespa_cord19.head(2) . topic_id iteration cord_uid relevancy query query-rewrite query-vector question narrative fieldMatch(abstract) ... fieldLength(body_text) fieldLength(title) freshness(timestamp) nativeRank(abstract) nativeRank(abstract_t5) nativeRank(title) rawScore(specter_embedding) rawScore(abstract_embedding) rawScore(title_embedding) binary_relevance . 0 1 | 0.5 | 010vptx3 | 2 | coronavirus origin | coronavirus origin origin COVID-19 information... | (0.28812721371650696, 1.558979868888855, 0.481... | what is the origin of COVID-19 | seeking range of information about the SARS-Co... | 0.111406 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 1 1 | 2.0 | p0kv1pht | 1 | coronavirus origin | coronavirus origin origin COVID-19 information... | (0.28812721371650696, 1.558979868888855, 0.481... | what is the origin of COVID-19 | seeking range of information about the SARS-Co... | 0.094629 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 2 rows × 173 columns . There are 163 ranking features available. Below we print the first three as a sample. . features = [ x for x in list(vespa_cord19.columns) if x not in [ &#39;topic_id&#39;, &#39;iteration&#39;, &#39;cord_uid&#39;, &#39;relevancy&#39;, &#39;binary_relevance&#39;, &#39;query&#39;, &#39;query-rewrite&#39;, &#39;query-vector&#39;, &#39;question&#39;, &#39;narrative&#39; ] ] print(len(features)) print(features[:3]) . 163 [&#39;fieldMatch(abstract)&#39;, &#39;fieldMatch(abstract).absoluteOccurrence&#39;, &#39;fieldMatch(abstract).absoluteProximity&#39;] . The original labelled data has three types of label: 0, 1 and 2. To simplify we will consider just two labels here. The document is either relevant (label = 1) or irrelevant (label = 0) . vespa_cord19[&quot;binary_relevance&quot;] = vespa_cord19.apply(lambda row: 1 if row[&quot;relevancy&quot;] &gt; 0 else 0, axis=1) vespa_cord19[[&#39;relevancy&#39;, &#39;binary_relevance&#39;]].head() . relevancy binary_relevance . 0 2 | 1 | . 1 1 | 1 | . 2 2 | 1 | . 3 0 | 0 | . 4 0 | 0 | . Model . We are going to fit logistic regressions with the objective of maximizing the log probability of the observed outcome. . from sklearn.linear_model import LogisticRegression from statistics import mean def compute_mean_realize_log_prob(model, X, Y): return mean([x[int(y)] for x, y in zip(model.predict_log_proba(X), Y)]) def fit_logistic_reg(X, Y): model = LogisticRegression(penalty=&#39;none&#39;, fit_intercept=True) model.fit(X, Y) realized_log_prob = compute_mean_realize_log_prob(model, X, Y) return realized_log_prob . Subset selection routine . Give the high number of features, we can only apply the best subset selection algorithm up to three features. Instead of running one logistic regression for each feature combination, we will run 10 replications with smaller sampled datasets with 1.000 data points. . The code below was adapted from this blog post written by Xavier Bourret Sicotte. . import itertools import pandas as pd from tqdm import tnrange, tqdm_notebook #Importing tqdm for the progress bar from tqdm.notebook import trange #Initialization variables log_probs, feature_list = [], [] numb_features = [] data_sample = [] number_sample = 10 number_points_per_sample = 1000 max_number_features = min(3, len(features)) for i in range(number_sample): data = vespa_cord19.sample(n=number_points_per_sample, random_state=456) Y = data.binary_relevance X = data[features] #Looping over k = 1 to k = 11 features in X for k in trange(1,max_number_features + 1, desc = &#39;Loop...&#39;): print(k) #Looping over all possible combinations: from 11 choose k for combo in itertools.combinations(X.columns,k): tmp_result = fit_logistic_reg(X[list(combo)],Y) #Store temp result log_probs.append(tmp_result) #Append lists feature_list.append(combo) numb_features.append(len(combo)) data_sample.append(i) #Store in DataFrame df = pd.DataFrame( { &#39;data_sample&#39;: data_sample, &#39;numb_features&#39;: numb_features, &#39;log_probs&#39;: log_probs, &#39;features&#39;:feature_list } ) . Analyze results . fine-grained results . Even with the limitation of checking at most 3 features we end up running more than 7 million logistic regressions to obtain the results presented here. . df . data_sample numb_features log_probs features . 0 0 | 1 | -0.577166 | (fieldMatch(abstract),) | . 1 0 | 1 | -0.584612 | (fieldMatch(abstract).absoluteOccurrence,) | . 2 0 | 1 | -0.589084 | (fieldMatch(abstract).absoluteProximity,) | . 3 0 | 1 | -0.563372 | (fieldMatch(abstract).completeness,) | . 4 0 | 1 | -0.589136 | (fieldMatch(abstract).degradedMatches,) | . ... ... | ... | ... | ... | . 7219265 9 | 3 | -0.589136 | (nativeRank(abstract_t5), rawScore(abstract_em... | . 7219266 9 | 3 | -0.589136 | (nativeRank(title), rawScore(specter_embedding... | . 7219267 9 | 3 | -0.589136 | (nativeRank(title), rawScore(specter_embedding... | . 7219268 9 | 3 | -0.589136 | (nativeRank(title), rawScore(abstract_embeddin... | . 7219269 9 | 3 | -0.589136 | (rawScore(specter_embedding), rawScore(abstrac... | . 7219270 rows × 4 columns . Average across data samples . We can now average the results across the 10 replications we runned for each combination. . average_df = df.groupby([&#39;numb_features&#39;, &#39;features&#39;], as_index=False).mean() average_df . numb_features features data_sample log_probs . 0 1 | (attribute(has_full_text),) | 4.5 | -0.589136 | . 1 1 | (bm25(abstract),) | 4.5 | -0.560556 | . 2 1 | (bm25(abstract_t5),) | 4.5 | -0.574475 | . 3 1 | (bm25(body_text),) | 4.5 | -0.587452 | . 4 1 | (bm25(title),) | 4.5 | -0.558449 | . ... ... | ... | ... | ... | . 721922 3 | (textSimilarity(title).score, nativeRank(title... | 4.5 | -0.576275 | . 721923 3 | (textSimilarity(title).score, nativeRank(title... | 4.5 | -0.576275 | . 721924 3 | (textSimilarity(title).score, rawScore(abstrac... | 4.5 | -0.576275 | . 721925 3 | (textSimilarity(title).score, rawScore(specter... | 4.5 | -0.576275 | . 721926 3 | (textSimilarity(title).score, rawScore(specter... | 4.5 | -0.576275 | . 721927 rows × 4 columns . Plot average results across data samples . average_df[&#39;max_log_probs&#39;] = df.groupby(&#39;numb_features&#39;)[&#39;log_probs&#39;].transform(max) average_df . numb_features features data_sample log_probs max_log_probs . 0 1 | (attribute(has_full_text),) | 4.5 | -0.589136 | -0.558449 | . 1 1 | (bm25(abstract),) | 4.5 | -0.560556 | -0.558449 | . 2 1 | (bm25(abstract_t5),) | 4.5 | -0.574475 | -0.558449 | . 3 1 | (bm25(body_text),) | 4.5 | -0.587452 | -0.558449 | . 4 1 | (bm25(title),) | 4.5 | -0.558449 | -0.558449 | . ... ... | ... | ... | ... | ... | . 721922 3 | (textSimilarity(title).score, nativeRank(title... | 4.5 | -0.576275 | -0.534266 | . 721923 3 | (textSimilarity(title).score, nativeRank(title... | 4.5 | -0.576275 | -0.534266 | . 721924 3 | (textSimilarity(title).score, rawScore(abstrac... | 4.5 | -0.576275 | -0.534266 | . 721925 3 | (textSimilarity(title).score, rawScore(specter... | 4.5 | -0.576275 | -0.534266 | . 721926 3 | (textSimilarity(title).score, rawScore(specter... | 4.5 | -0.576275 | -0.534266 | . 721927 rows × 5 columns . import matplotlib.pyplot as plt %matplotlib inline plt.style.use(&#39;ggplot&#39;) plt.scatter(average_df.numb_features,average_df.log_probs, alpha = .2, color = &#39;darkblue&#39;) plt.xlabel(&#39;# Features&#39;) plt.ylabel(&#39;log_probs&#39;) plt.title(&#39;Best subset selection&#39;) plt.plot(average_df.numb_features,average_df.max_log_probs, color = &#39;r&#39;, label = &#39;Best subset&#39;) plt.show() . Display the best features for each model size . average_df_max = average_df.sort_values(&#39;log_probs&#39;, ascending=False).drop_duplicates([&#39;numb_features&#39;]).sort_values(&#39;numb_features&#39;) . for f in average_df_max.features: print(f) . (&#39;bm25(title)&#39;,) (&#39;textSimilarity(title).queryCoverage&#39;, &#39;bm25(abstract)&#39;) (&#39;textSimilarity(abstract).proximity&#39;, &#39;textSimilarity(title).queryCoverage&#39;, &#39;bm25(abstract)&#39;) .",
            "url": "https://thigm85.github.io/blog/ranking%20features/vespa/feature%20selection/model%20selection/2020/05/27/subset-selection.html",
            "relUrl": "/ranking%20features/vespa/feature%20selection/model%20selection/2020/05/27/subset-selection.html",
            "date": " • May 27, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "Semantic text search with Vespa python API",
            "content": "Connect to a running Vespa application . It is assumed that you have deployed the MS MARCO full-text vespa sample app in your local machine. . from vespa.application import Vespa app = Vespa(url=&quot;http://localhost&quot;, port=8080) . Load the BERT model . Loading one of the many models available. . from sentence_transformers import SentenceTransformer bert_model = SentenceTransformer(&quot;distilbert-base-nli-stsb-mean-tokens&quot;) . Define a function that take a text as input and return a vector of floats as output. . import numpy as np def normalized_bert_encoder(text): vector = bert_model.encode([text])[0].tolist() norm = np.linalg.norm(vector) if norm &gt; 0.0: vector = vector / norm return vector.tolist() . Define a query model . from vespa.query import Query, Union, WeakAnd, ANN, RankProfile query_model = Query( match_phase=Union( WeakAnd(hits=1000), ANN( doc_vector=&quot;title_bert&quot;, query_vector=&quot;tensor_bert&quot;, embedding_model=normalized_bert_encoder, hits=1000, label=&quot;ann_title&quot; ), ANN( doc_vector=&quot;body_bert&quot;, query_vector=&quot;tensor_bert&quot;, embedding_model=normalized_bert_encoder, hits=1000, label=&quot;ann_body&quot; ) ), rank_profile=RankProfile(name=&quot;bert_title_body_all&quot;) ) . At this point we can query our application: . query_results = app.query(query=&quot;this is a test&quot;, query_model=query_model) . Load labelled data . I will load a sample of the test set that I used in my experiments. . import requests import json labelled_data = json.loads( requests.get(&quot;https://thigm85.github.io/data/msmarco/labelled_data_msmarco_test_set.json&quot;).text ) . Here it is the first two queries with their respective relevant documents. . labelled_data[0:2] . [{&#39;query_id&#39;: &#39;848370&#39;, &#39;query&#39;: &#39;what is the state bird of rhode island&#39;, &#39;relevant_docs&#39;: [{&#39;id&#39;: &#39;D2533084&#39;}]}, {&#39;query_id&#39;: &#39;230835&#39;, &#39;query&#39;: &#39;how far is dallas airport from downtown&#39;, &#39;relevant_docs&#39;: [{&#39;id&#39;: &#39;D1327232&#39;}]}] . Evaluate the query model . from vespa.evaluation import MatchRatio, Recall, ReciprocalRank eval_metrics = [MatchRatio(), Recall(at = 100), ReciprocalRank(at = 100)] . evaluation = app.evaluate( labelled_data=labelled_data, eval_metrics=eval_metrics, query_model=query_model, id_field=&quot;id&quot;, timeout=5 ) . evaluation.head() . query_id match_ratio_retrieved_docs match_ratio_docs_available match_ratio_value recall_100_value reciprocal_rank_100_value . 0 848370 | 12738 | 99577 | 0.127921 | 0 | 0.000000 | . 1 230835 | 12533 | 99577 | 0.125862 | 1 | 1.000000 | . 2 915794 | 11523 | 99577 | 0.115719 | 0 | 0.000000 | . 3 733084 | 20047 | 99577 | 0.201322 | 0 | 0.000000 | . 4 23313 | 17423 | 99577 | 0.174970 | 1 | 0.111111 | . Baseline query model . baseline_query_model = Query( match_phase=WeakAnd(hits=1000), rank_profile=RankProfile(name=&quot;bm25&quot;) ) . baseline_evaluation = app.evaluate( labelled_data=labelled_data, eval_metrics=eval_metrics, query_model=baseline_query_model, id_field=&quot;id&quot;, timeout=5 ) . baseline_evaluation.head() . query_id match_ratio_retrieved_docs match_ratio_docs_available match_ratio_value recall_100_value reciprocal_rank_100_value . 0 848370 | 7160 | 99577 | 0.071904 | 1 | 1.000000 | . 1 230835 | 7581 | 99577 | 0.076132 | 1 | 1.000000 | . 2 915794 | 6067 | 99577 | 0.060928 | 1 | 0.142857 | . 3 733084 | 15283 | 99577 | 0.153479 | 1 | 1.000000 | . 4 23313 | 13424 | 99577 | 0.134810 | 1 | 0.333333 | . Compare query model with baseline . from pandas import merge eval_comparison = merge(left=baseline_evaluation, right=evaluation, on=&quot;query_id&quot;, suffixes=(&#39;_baseline&#39;, &#39;&#39;)) . Match Ratio . eval_comparison[[&quot;match_ratio_value_baseline&quot;, &quot;match_ratio_value&quot;]].describe() . match_ratio_value_baseline match_ratio_value . count 500.000000 | 500.000000 | . mean 0.124775 | 0.179036 | . std 0.116852 | 0.116558 | . min 0.000000 | 0.000000 | . 25% 0.067169 | 0.122315 | . 50% 0.083393 | 0.138973 | . 75% 0.121597 | 0.180619 | . max 0.873666 | 0.887836 | . Recall . eval_comparison[[&quot;recall_100_value_baseline&quot;, &quot;recall_100_value&quot;]].describe() . recall_100_value_baseline recall_100_value . count 500.000000 | 500.000000 | . mean 0.842000 | 0.586000 | . std 0.365106 | 0.493042 | . min 0.000000 | 0.000000 | . 25% 1.000000 | 0.000000 | . 50% 1.000000 | 1.000000 | . 75% 1.000000 | 1.000000 | . max 1.000000 | 1.000000 | . Reciprocal rank comparison . eval_comparison[[&quot;reciprocal_rank_100_value_baseline&quot;, &quot;reciprocal_rank_100_value&quot;]].describe() . reciprocal_rank_100_value_baseline reciprocal_rank_100_value . count 500.000000 | 500.000000 | . mean 0.675333 | 0.449417 | . std 0.413238 | 0.453405 | . min 0.000000 | 0.000000 | . 25% 0.237500 | 0.000000 | . 50% 1.000000 | 0.333333 | . 75% 1.000000 | 1.000000 | . max 1.000000 | 1.000000 | . Sometimes it is helpful to visually see the difference between the two query models. . from pandas import concat from plotnine import ggplot, geom_boxplot, aes baseline_evaluation[&quot;query_model_name&quot;] = &quot;bm25&quot; evaluation[&quot;query_model_name&quot;] = &quot;bert&quot; data_plot = concat([evaluation, baseline_evaluation]) ggplot(data_plot) + geom_boxplot(aes(x=&#39;query_model_name&#39;, y=&#39;reciprocal_rank_100_value&#39;)) . &lt;ggplot: (349088749)&gt; . Collect training data . When collecting training data, it is important to use a rank profile that applied a random ordering of the matched documents. . data_collection_query_model = Query( match_phase=Union( WeakAnd(hits=1000), ANN( doc_vector=&quot;title_bert&quot;, query_vector=&quot;tensor_bert&quot;, embedding_model=normalized_bert_encoder, hits=1000, label=&quot;ann_title&quot; ), ANN( doc_vector=&quot;body_bert&quot;, query_vector=&quot;tensor_bert&quot;, embedding_model=normalized_bert_encoder, hits=1000, label=&quot;ann_body&quot; ) ), rank_profile=RankProfile( name=&quot;collect_rank_features_embeddings&quot;, list_features=True) ) . Once we have defined the data_collection_query_model, we can collect data containing both relevant and random documents. . training_data = app.collect_training_data( labelled_data=labelled_data, id_field=&quot;id&quot;, query_model=data_collection_query_model, number_random_docs=99 ) . training_data.head() . bm25(body) bm25(title) nativeRank(body) nativeRank(title) rankingExpression(dot_product_body_bert) rankingExpression(dot_product_body_gse) rankingExpression(dot_product_body_word2vec) rankingExpression(dot_product_title_bert) rankingExpression(dot_product_title_gse) rankingExpression(dot_product_title_word2vec) document_id query_id relevant . 0 26.474014 | 9.612229 | 0.314493 | 8.557759e-02 | 0.247331 | 0.0 | 0.0 | 0.562989 | 0.0 | 0.0 | D2533084 | 848370 | 1 | . 1 3.247162 | 0.000000 | 0.117618 | 1.351221e-38 | 0.080011 | 0.0 | 0.0 | 0.205397 | 0.0 | 0.0 | D864574 | 848370 | 0 | . 2 3.259604 | 0.000000 | 0.156522 | 1.351221e-38 | 0.011655 | 0.0 | 0.0 | 0.171949 | 0.0 | 0.0 | D3246601 | 848370 | 0 | . 3 1.853070 | 0.835559 | 0.121469 | 6.774715e-02 | 0.274763 | 0.0 | 0.0 | 0.292467 | 0.0 | 0.0 | D3028705 | 848370 | 0 | . 4 7.327304 | 0.000000 | 0.153262 | 1.351221e-38 | 0.045287 | 0.0 | 0.0 | 0.048524 | 0.0 | 0.0 | D289677 | 848370 | 0 | . We can now create a figure that is similar to the one displayed in semantic text search tutorial that shows the MS MARCO bias toward term-matching signals like BM25. . import plotly.graph_objects as go relevant_training_data = training_data[training_data[&quot;relevant&quot;] == 1] fig = go.Figure() fig.add_trace( go.Histogram( x=training_data[&quot;bm25(body)&quot;] + training_data[&quot;bm25(title)&quot;], histnorm=&#39;probability density&#39;, name = &quot;relevant + random&quot; ) ) fig.add_trace( go.Histogram( x=relevant_training_data[&quot;bm25(body)&quot;] + relevant_training_data[&quot;bm25(title)&quot;], histnorm=&#39;probability density&#39;, name = &quot;relevant&quot; ) ) fig.update_layout(barmode=&#39;overlay&#39;) fig.update_traces(opacity=0.5) fig.show() .",
            "url": "https://thigm85.github.io/blog/ms%20marco/vespa/semantic%20search/text%20search/2020/05/15/msmarco-evaluation.html",
            "relUrl": "/ms%20marco/vespa/semantic%20search/text%20search/2020/05/15/msmarco-evaluation.html",
            "date": " • May 15, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "Vespa submission to round 1 of TREC-COVID",
            "content": "This piece will reproduce a possible round 1 TREC-COVID submission generated with the cord19.vespa.ai application. . . Vespa . Connect to the CORD-19 Vespa API. . from vespa.application import Vespa app = Vespa(url = &quot;https://api.cord19.vespa.ai&quot;) . Define the query model used for the submission. . from vespa.query import Query, OR, RankProfile query_model = Query( match_phase = OR(), rank_profile = RankProfile(name=&quot;bm25t5&quot;) ) . Submission . Load the topics provided by the organizers. . import requests import json topics = json.loads(requests.get(&quot;https://thigm85.github.io/data/covid19/topics-annotated.json&quot;).text) . Generate the submissions by querying the Vespa application, and organizing the results according to the TREC output format. We only return 2 hits for each request as an example. Feel free to change that to 1000 when generating your own submmission. . from pandas import DataFrame submission = [] for t in topics: id = t[&#39;id&#39;] question = t[&#39;question&#39;] query = t[&#39;query&#39;] narrative = t[&#39;narrative&#39;] query = question + &#39; &#39; + query + &#39; &#39; + narrative result = app.query( query=query, query_model=query_model, hits = 2, model = {&#39;defaultIndex&#39;: &#39;allt5&#39;}, summary = &#39;default&#39;, timeout = &#39;15s&#39;, collapsefield = &#39;cord_uid&#39;, bolding = &#39;false&#39; ) i = 0 for h in result[&#39;root&#39;][&#39;children&#39;]: i+=1 submission.append( {&quot;topicid&quot;: id, &quot;Q0&quot;: &quot;Q0&quot;, &quot;docid&quot;: h[&quot;fields&quot;].get(&#39;cord_uid&#39;), &quot;rank&quot;: i, &quot;score&quot;: h[&#39;relevance&#39;], &quot;run-tag&quot;: query_model.rank_profile.name }) submission = DataFrame.from_records(submission) . submission . topicid Q0 docid rank score run-tag . 0 1 | Q0 | lfndq85x | 1 | 70.993937 | bm25t5 | . 1 1 | Q0 | z14rf85c | 2 | 70.613335 | bm25t5 | . 2 2 | Q0 | exqza1kg | 1 | 92.731966 | bm25t5 | . 3 2 | Q0 | r9scxa76 | 2 | 88.443326 | bm25t5 | . 4 3 | Q0 | rq5nqm92 | 1 | 80.897894 | bm25t5 | . ... ... | ... | ... | ... | ... | ... | . 65 33 | Q0 | 79yna07e | 2 | 96.310245 | bm25t5 | . 66 34 | Q0 | gd5btv69 | 1 | 81.606707 | bm25t5 | . 67 34 | Q0 | 8p9d1c9k | 2 | 76.193660 | bm25t5 | . 68 35 | Q0 | 6xkm2j0f | 1 | 109.850018 | bm25t5 | . 69 35 | Q0 | vaeyoxv7 | 2 | 107.736848 | bm25t5 | . 70 rows × 6 columns .",
            "url": "https://thigm85.github.io/blog/covid-19/vespa/2020/05/08/trec-covid-round-1-submission.html",
            "relUrl": "/covid-19/vespa/2020/05/08/trec-covid-round-1-submission.html",
            "date": " • May 8, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://thigm85.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://thigm85.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Principal Data Scientist at Verizon Media working on vespa.ai. .",
          "url": "https://thigm85.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://thigm85.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}