<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Trees, bagging, random forest, boosting and their implementations | Thiago G. Martins</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Trees, bagging, random forest, boosting and their implementations" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal blog." />
<meta property="og:description" content="Personal blog." />
<link rel="canonical" href="https://thigm85.github.io/blog/decision%20trees/bagging/random%20forest/boosting/2022/06/28/trees-bagging-random-forest-boosting-and-their-implementations.html" />
<meta property="og:url" content="https://thigm85.github.io/blog/decision%20trees/bagging/random%20forest/boosting/2022/06/28/trees-bagging-random-forest-boosting-and-their-implementations.html" />
<meta property="og:site_name" content="Thiago G. Martins" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-28T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Personal blog.","headline":"Trees, bagging, random forest, boosting and their implementations","dateModified":"2022-06-28T00:00:00-05:00","datePublished":"2022-06-28T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://thigm85.github.io/blog/decision%20trees/bagging/random%20forest/boosting/2022/06/28/trees-bagging-random-forest-boosting-and-their-implementations.html"},"url":"https://thigm85.github.io/blog/decision%20trees/bagging/random%20forest/boosting/2022/06/28/trees-bagging-random-forest-boosting-and-their-implementations.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://thigm85.github.io/blog/feed.xml" title="Thiago G. Martins" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-28943273-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Trees, bagging, random forest, boosting and their implementations | Thiago G. Martins</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Trees, bagging, random forest, boosting and their implementations" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal blog." />
<meta property="og:description" content="Personal blog." />
<link rel="canonical" href="https://thigm85.github.io/blog/decision%20trees/bagging/random%20forest/boosting/2022/06/28/trees-bagging-random-forest-boosting-and-their-implementations.html" />
<meta property="og:url" content="https://thigm85.github.io/blog/decision%20trees/bagging/random%20forest/boosting/2022/06/28/trees-bagging-random-forest-boosting-and-their-implementations.html" />
<meta property="og:site_name" content="Thiago G. Martins" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-28T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Personal blog.","headline":"Trees, bagging, random forest, boosting and their implementations","dateModified":"2022-06-28T00:00:00-05:00","datePublished":"2022-06-28T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://thigm85.github.io/blog/decision%20trees/bagging/random%20forest/boosting/2022/06/28/trees-bagging-random-forest-boosting-and-their-implementations.html"},"url":"https://thigm85.github.io/blog/decision%20trees/bagging/random%20forest/boosting/2022/06/28/trees-bagging-random-forest-boosting-and-their-implementations.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://thigm85.github.io/blog/feed.xml" title="Thiago G. Martins" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-28943273-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
<script type="text/javascript">
require.config({
  paths: {
    jquery: 'https://code.jquery.com/jquery-3.5.0.min',
    plotly: 'https://cdn.plot.ly/plotly-latest.min'
  },

  shim: {
    plotly: {
      deps: ['jquery'],
      exports: 'plotly'
    }
  }
});
</script>

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Thiago G. Martins</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Trees, bagging, random forest, boosting and their implementations</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-06-28T00:00:00-05:00" itemprop="datePublished">
        Jun 28, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#decision trees">decision trees</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#bagging">bagging</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#random forest">random forest</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#boosting">boosting</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/thigm85/blog/tree/master/_notebooks/2022-06-28-trees-bagging-random-forest-boosting-and-their-implementations.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/thigm85/blog/blob/master/_notebooks/2022-06-28-trees-bagging-random-forest-boosting-and-their-implementations.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#The-basics-of-decision-trees">The basics of decision trees </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Tree-based-model-in-the-light-of-a-linear-model">Tree-based model in the light of a linear model </a></li>
<li class="toc-entry toc-h3"><a href="#Regression-trees">Regression trees </a></li>
<li class="toc-entry toc-h3"><a href="#Classification-trees">Classification trees </a></li>
<li class="toc-entry toc-h3"><a href="#Advantages-and-disadvantages">Advantages and disadvantages </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Bagging,-Random-Forests,-Boosting">Bagging, Random Forests, Boosting </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Bootstrapping">Bootstrapping </a></li>
<li class="toc-entry toc-h3"><a href="#Bagging">Bagging </a></li>
<li class="toc-entry toc-h3"><a href="#Random-Forests">Random Forests </a></li>
<li class="toc-entry toc-h3"><a href="#Boosting">Boosting </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#LightGBM">LightGBM </a></li>
<li class="toc-entry toc-h2"><a href="#References">References </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-06-28-trees-bagging-random-forest-boosting-and-their-implementations.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-basics-of-decision-trees">
<a class="anchor" href="#The-basics-of-decision-trees" aria-hidden="true"><span class="octicon octicon-link"></span></a>The basics of decision trees<a class="anchor-link" href="#The-basics-of-decision-trees"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tree-based-model-in-the-light-of-a-linear-model">
<a class="anchor" href="#Tree-based-model-in-the-light-of-a-linear-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tree-based model in the light of a linear model<a class="anchor-link" href="#Tree-based-model-in-the-light-of-a-linear-model"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A tree based model can be written as $f(x) = \sum_{m=1}^M c_m 1_{(x \in R_m)}$, where $c_m$ is the leaf node value of the region $R_m$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure below show two cases, one where a linear model is better than a tree-based model (top row) and one where a tree-based model is better than a linear model (botton row).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/tree_vs_linear.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Regression-trees">
<a class="anchor" href="#Regression-trees" aria-hidden="true"><span class="octicon octicon-link"></span></a>Regression trees<a class="anchor-link" href="#Regression-trees"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/regression_tree_algo.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Step 1<ul>
<li>The goal is to find boxes $R_1, ... R_j$ that minimizes the $RSS=\sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})$, where $\hat{y}_{R_j}$ is the mean response for the training observations within $R_j$.</li>
<li>It is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.</li>
<li>For this reason, we take a top-down, greedy approach that is known as recursive binary splitting.<ul>
<li>The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space.</li>
<li>It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</li>
<li>In greater detail, for any $j$ and $s$, we define the pair of half-planes $R_1(j, s) = \{X|X_j &lt; s\}$ and $R_2(j, s) = \{X|Xj ≥ s\}$, and we seek the value of $j$ and $s$ that minimize the equation 
$$\sum_{i: x_i \in R_1(j,s)}(y_i − \hat{y}_{R_1})^2 + \sum_{i: x_i \in R_2(j,s)}(y_i − \hat{y}_{R_2})^2$$</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Step 2<ul>
<li>Step 1 tends to overfit the training data.</li>
<li>Computing the test error at each partition on step 1 is expensive. We use cost complexity pruning instead.</li>
<li>For each value of $\alpha$ there corresponds a subtree $T \subset T_0$ such that 
$$ \sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} (y_i − \hat{y}_{R_m})^2 + \alpha |T|$$
  is as small as possible, where $|T|$ is the number of terminal nodes of the tree $T$ and $\alpha$ is a tunning parameter.</li>
<li>It turns out that as we increase $\alpha$ from zero, branches get pruned from the tree in a nested and predictable fashion, so obtaining the whole sequence of subtrees as a function of $\alpha$ is easy.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Classification-trees">
<a class="anchor" href="#Classification-trees" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classification trees<a class="anchor-link" href="#Classification-trees"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Uses recursive binary splitting to build the tree.</li>
<li>Task can be to either return the most common class or to return the proportion of each class.</li>
<li>There are a few possibilities to use instead of the RSS:<ul>
<li>classification error rate: $E = 1 - max_k(\hat{p}_{mk})$, where $\hat{p}_{mk}$ is the proportion that the $m$-th region are of class $k$.</li>
<li>Gini index: $G = \sum_{k=1}^{K} \hat{p}_{mk} (1-\hat{p}_{mk})$</li>
<li>Cross-entropy: $D = -\sum_{k=1}^{K} \hat{p}_{mk} \log \hat{p}_{mk}$</li>
</ul>
</li>
<li>When building a classification tree, either the Gini index or the crossentropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Advantages-and-disadvantages">
<a class="anchor" href="#Advantages-and-disadvantages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Advantages and disadvantages<a class="anchor-link" href="#Advantages-and-disadvantages"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Advantages<ul>
<li>Tree-based methods are more interpretable</li>
<li>Can handle qualitative predictors without the need to create dummy variables</li>
</ul>
</li>
<li>Disadvantages<ul>
<li>Decision trees by itself are in general not as accurate as other linear models</li>
<li>Decision trees are not robust in the sense that an small change in the data can lead to a very different estimated tree. That is, decision trees have high variance.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bagging,-Random-Forests,-Boosting">
<a class="anchor" href="#Bagging,-Random-Forests,-Boosting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bagging, Random Forests, Boosting<a class="anchor-link" href="#Bagging,-Random-Forests,-Boosting"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Those are generic approaches that can be applied to many statistical learning methods.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bootstrapping">
<a class="anchor" href="#Bootstrapping" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bootstrapping<a class="anchor-link" href="#Bootstrapping"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Assume we have $n$ observations of training data, $Z$. Bootstrapping works by sampling with replacement B datasets with $n$ observations each $Z^{*1}$, ..., $Z^{*B}$. Those sampled datasets can then be used either to compate uncertainty measures or by averaging different model estimates to reduce the variance of the original model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Ideally we would like to have B independent datasets, but when collecting independent datasets are not possible, bootstrapping can be used.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What are the bias introduced by bootstraping instead of using independent datasets?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bagging">
<a class="anchor" href="#Bagging" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bagging<a class="anchor-link" href="#Bagging"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>For regression, fit tree-based models on B bootstraped datasets and average the results. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
$$\hat{f}_{\text{avg}}(x) = \frac{1}{B}\sum_{b=1}^{B} \hat{f}^b (x)$$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>For a classification tree we can predict the class of a data point by taking the majority vote between the B predictions. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Each tree is grown deep, and are not pruned. Hence, each individual tree has high variance but low bias. The boostrap technique helps by reducing the variance.  </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Out-of-bag test error estimation: <ul>
<li>On average, each bagged tree uses 2/3 of the original data. </li>
<li>For each observation there are on average B/3 model that were not trained on it. </li>
<li>If we average for regression and take the majority for classification of the predictions of those B/3 models, we have an out-of-bag (OOB) prediction for the i-th observation.</li>
<li>If we do the above for all the observation we have the overall OOB MSE for regression or classification error for classification.</li>
<li>The resulting OOB error is a valid estimate of the test error for the bagged model.</li>
<li>It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out CV error.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Interpretability and variable importance<ul>
<li>We lose the easy of interpretation when bagging tree-based models</li>
<li>We can compute feature importance by computing the average decrease in the error metric obtained by splits of the feature.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Random-Forests">
<a class="anchor" href="#Random-Forests" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Forests<a class="anchor-link" href="#Random-Forests"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Similar to bagging, except that when choosing which feature to split we randomly select $m$ out of the $p$ predictor to be considered.</li>
<li>Usual choice $m = \sqrt p$</li>
<li>$m = p$ means that random forest equal to bagging.</li>
<li>The logic behind chosing a subset of the predictors before each split is to create more uncorrelated trees. Say there is a strong predictor that will be picked for all the trees when bagging, randomly selecting m predictor will give a change for other predictors to be considered, leading to more uncorrelated trees.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Boosting">
<a class="anchor" href="#Boosting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Boosting<a class="anchor-link" href="#Boosting"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Unlike bagging, boosting does not involve bootstrap sampling, instead the trees grows sequentially, where each tree is fit on a modified version of the original data set.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/boosting_regression_trees.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Boosting has three tuning parameters:</p>
<ol>
<li>The number of trees $B$. Unlike boosting and random forest we can overfit the data with high $B$, although it happens slowly if at all. We use CV to select $B$.</li>
<li>The shrinkage parameter $\lambda$, a small positive number that controls the rate at which boosting learns. Typical values are $0.01$ or $0.001$ but the value is problem dependent. Smaller $\lambda$ can require larger $B$.</li>
<li>The number $d$ of splits in each tree, which controls the complexity of the boosted ensemble. $d=1$ often works well and means that we are fitting an additive model since each tree involves only one predictor. More generally, $d$ is the interaction depth and controls the interaction order of the boosted model, since $d$ splits can involve at most $d$ variables.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="LightGBM">
<a class="anchor" href="#LightGBM" aria-hidden="true"><span class="octicon octicon-link"></span></a>LightGBM<a class="anchor-link" href="#LightGBM"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<a href="https://www.kaggle.com/code?searchQuery=lightgbm">Kaggle lightGBM notebooks</a><ul>
<li>
<a href="https://www.kaggle.com/competitions/home-credit-default-risk">Home Credit Default Risk dataset</a><ul>
<li>
<a href="https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features/script">LightGBM with Simple Features</a><ul>
<li>Most of the script is feature pre-processing</li>
<li>Uses LGBMClassifier with hyperparameter already tuned </li>
<li>Uses sklearn CV</li>
<li>Display feature importance</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>LightGBM<ul>
<li>
<a href="https://lightgbm.readthedocs.io/en/latest/index.html">LightGBM Home page</a> homepage</li>
<li>
<a href="https://neptune.ai/blog/lightgbm-parameters-guide">Understanding LightGBM Parameters (and How to Tune Them)</a>   </li>
<li>Optune LightGBM tuner<ul>
<li><a href="https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258">Blog post</a></li>
<li><a href="https://github.com/optuna/optuna-examples/blob/main/lightgbm/lightgbm_tuner_simple.py">Code</a></li>
</ul>
</li>
<li><a href="https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5">Kaggler’s Guide to LightGBM Hyperparameter Tuning with Optuna in 2021</a></li>
<li><a href="https://stackoverflow.com/a/67627169">LightGBM stackoverflow example</a></li>
</ul>
</li>
<li>XGBoost<ul>
<li><a href="https://xgboost.readthedocs.io/en/latest/index.html">XGBoost Home page</a></li>
</ul>
</li>
<li>LightGBM/XGBoost comparison<ul>
<li><a href="https://neptune.ai/blog/xgboost-vs-lightgbm">XGBoost vs LightGBM: How Are They Different</a></li>
<li>Set of <a href="https://github.com/sophwats/learning-to-rank">LTR notebooks</a> involving boosting from sophwatts    </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">
<a class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a class="anchor-link" href="#References"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Chapter 8 of Introduction to Statistical Learning<ul>
<li>Covered section 8.1 and 8.2</li>
<li>Need to cover section 8.3</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="thigm85/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/decision%20trees/bagging/random%20forest/boosting/2022/06/28/trees-bagging-random-forest-boosting-and-their-implementations.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Personal blog.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/thigm85" target="_blank" title="thigm85"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Thiagogm" target="_blank" title="Thiagogm"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
