{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "democratic-advice",
   "metadata": {},
   "source": [
    "# Evaluate text-image search app with Flickr 8k dataset\n",
    "> Create labeled data, text processor and evaluate with Vespa python API\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [text_image_search, clip_model, vespa, flicker8k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-numbers",
   "metadata": {},
   "source": [
    "This post creates a labeled dataset out of the Flicker 8k image-caption dataset, builds a text processor that uses a CLIP model to map a text query into the same 512-dimensional space used to represent images and evaluate different query models using the Vespa python API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-learning",
   "metadata": {},
   "source": [
    "Check the previous three posts for context:\n",
    "\n",
    "* [Flicker 8k dataset first exploration](https://thigm85.github.io/blog/flicker8k/dataset/image/nlp/2021/10/21/flicker8k-dataset-first-exploration.html)\n",
    "* [Understanding CLIP image pipeline](https://thigm85.github.io/blog/image%20processing/clip%20model/dual%20encoder/pil/2021/10/22/understanding-clip-image-pipeline.html)\n",
    "* [Vespa image search with PyTorch feeder](https://thigm85.github.io/blog/image%20processing/clip%20model/vespa/pytorch/pytorch%20dataset/2021/10/25/vespa-image-search-flicker8k.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5df31a-0d8d-4de6-9d4e-e38428e43406",
   "metadata": {},
   "source": [
    "## Create labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-sleep",
   "metadata": {},
   "source": [
    "An (image, caption) pair will be considered relevant for our purposes if all three experts agreed on a relevance score equal to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c3556-fe1b-43fa-9f89-dd36b47d3954",
   "metadata": {},
   "source": [
    "### Load and check the expert judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653bdb0d-ab27-4f70-991b-518382b70307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "experts = read_csv(\n",
    "    os.path.join(os.environ[\"DATA_FOLDER\"], \"ExpertAnnotations.txt\"), \n",
    "    sep = \"\\t\", \n",
    "    header=None, \n",
    "    names=[\"image_file_name\", \"caption_id\", \"expert_1\", \"expert_2\", \"expert_3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f111fb-f30b-4447-a420-1f84e155eb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_name</th>\n",
       "      <th>caption_id</th>\n",
       "      <th>expert_1</th>\n",
       "      <th>expert_2</th>\n",
       "      <th>expert_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1056338697_4f7d7ce270.jpg</td>\n",
       "      <td>2549968784_39bfbe44f9.jpg#2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1056338697_4f7d7ce270.jpg</td>\n",
       "      <td>2718495608_d8533e3ac5.jpg#2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1056338697_4f7d7ce270.jpg</td>\n",
       "      <td>3181701312_70a379ab6e.jpg#2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1056338697_4f7d7ce270.jpg</td>\n",
       "      <td>3207358897_bfa61fa3c6.jpg#2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1056338697_4f7d7ce270.jpg</td>\n",
       "      <td>3286822339_5535af6b93.jpg#2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_file_name                   caption_id  expert_1  expert_2  \\\n",
       "0  1056338697_4f7d7ce270.jpg  2549968784_39bfbe44f9.jpg#2         1         1   \n",
       "1  1056338697_4f7d7ce270.jpg  2718495608_d8533e3ac5.jpg#2         1         1   \n",
       "2  1056338697_4f7d7ce270.jpg  3181701312_70a379ab6e.jpg#2         1         1   \n",
       "3  1056338697_4f7d7ce270.jpg  3207358897_bfa61fa3c6.jpg#2         1         2   \n",
       "4  1056338697_4f7d7ce270.jpg  3286822339_5535af6b93.jpg#2         1         1   \n",
       "\n",
       "   expert_3  \n",
       "0         1  \n",
       "1         2  \n",
       "2         2  \n",
       "3         2  \n",
       "4         2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328b522-db13-47c0-8e52-e6a59e35bf55",
   "metadata": {},
   "source": [
    "### Check cases where all experts agrees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6352cef1-c96f-44d5-9c4d-0651eecb22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experts_agreement_bool = experts.apply(\n",
    "    lambda x: x[\"expert_1\"] == x[\"expert_2\"] and x[\"expert_2\"] == x[\"expert_3\"], \n",
    "    axis=1\n",
    ")\n",
    "experts_agreement = experts[experts_agreement_bool][\n",
    "    [\"image_file_name\", \"caption_id\", \"expert_1\"]\n",
    "].rename(columns={\"expert_1\":\"expert\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3daad3a3-69f4-4a6c-88b9-f8ef29691fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_name</th>\n",
       "      <th>caption_id</th>\n",
       "      <th>expert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1056338697_4f7d7ce270.jpg</td>\n",
       "      <td>2549968784_39bfbe44f9.jpg#2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1056338697_4f7d7ce270.jpg</td>\n",
       "      <td>3360930596_1e75164ce6.jpg#2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1056338697_4f7d7ce270.jpg</td>\n",
       "      <td>3545652636_0746537307.jpg#2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
       "      <td>1425069308_488e5fcf9d.jpg#2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
       "      <td>1714316707_8bbaa2a2ba.jpg#2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_file_name                   caption_id  expert\n",
       "0  1056338697_4f7d7ce270.jpg  2549968784_39bfbe44f9.jpg#2       1\n",
       "5  1056338697_4f7d7ce270.jpg  3360930596_1e75164ce6.jpg#2       1\n",
       "6  1056338697_4f7d7ce270.jpg  3545652636_0746537307.jpg#2       1\n",
       "8   106490881_5a2dd9b7bd.jpg  1425069308_488e5fcf9d.jpg#2       1\n",
       "9   106490881_5a2dd9b7bd.jpg  1714316707_8bbaa2a2ba.jpg#2       2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experts_agreement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95223172-a660-428b-8ce8-a0f4e2fcdbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2350\n",
       "2     580\n",
       "3     214\n",
       "4     247\n",
       "Name: expert, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experts_agreement[\"expert\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee029c2d-89af-4ea1-a362-4c403d885597",
   "metadata": {},
   "source": [
    "### Load captions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "consecutive-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = read_csv(\n",
    "    os.path.join(os.environ[\"DATA_FOLDER\"], \"Flickr8k.token.txt\"), \n",
    "    sep=\"\\t\", \n",
    "    header=None, \n",
    "    names=[\"caption_id\", \"caption\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "north-socket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#0</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#1</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#2</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#3</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#4</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    caption_id  \\\n",
       "0  1000268201_693b08cb0e.jpg#0   \n",
       "1  1000268201_693b08cb0e.jpg#1   \n",
       "2  1000268201_693b08cb0e.jpg#2   \n",
       "3  1000268201_693b08cb0e.jpg#3   \n",
       "4  1000268201_693b08cb0e.jpg#4   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wound-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption(caption_id, captions):\n",
    "    return captions[captions[\"caption_id\"] == caption_id][\"caption\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288384cf-4fd8-4d02-9ad7-6b3dbb10d69e",
   "metadata": {},
   "source": [
    "### Relevant (image, text) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15a01e59-e9bf-4502-bf4e-558911941766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_name</th>\n",
       "      <th>caption_id</th>\n",
       "      <th>expert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1119015538_e8e796281e.jpg</td>\n",
       "      <td>416106657_cab2a107a5.jpg#2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1131932671_c8d17751b3.jpg</td>\n",
       "      <td>1131932671_c8d17751b3.jpg#2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>115684808_cb01227802.jpg</td>\n",
       "      <td>115684808_cb01227802.jpg#2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_file_name                   caption_id  expert\n",
       "43  1119015538_e8e796281e.jpg   416106657_cab2a107a5.jpg#2       4\n",
       "53  1131932671_c8d17751b3.jpg  1131932671_c8d17751b3.jpg#2       4\n",
       "66   115684808_cb01227802.jpg   115684808_cb01227802.jpg#2       4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_data = experts_agreement[experts_agreement[\"expert\"] == 4]\n",
    "relevant_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-athletics",
   "metadata": {},
   "source": [
    "### Create labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "12299769-7b76-498a-ab16-337bee2a4850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A white dog runs in the grass</td>\n",
       "      <td>1119015538_e8e796281e.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A boy jumps from one bed to another</td>\n",
       "      <td>1131932671_c8d17751b3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Three people and a sled</td>\n",
       "      <td>115684808_cb01227802.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A group of people walking a city street in war...</td>\n",
       "      <td>1174629344_a2e1a2bdbf.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Two children one of which is holding a stick a...</td>\n",
       "      <td>1322323208_c7ecb742c6.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid                                              query  \\\n",
       "0    0                      A white dog runs in the grass   \n",
       "1    1                A boy jumps from one bed to another   \n",
       "2    2                            Three people and a sled   \n",
       "3    3  A group of people walking a city street in war...   \n",
       "4    4  Two children one of which is holding a stick a...   \n",
       "\n",
       "                      doc_id  relevance  \n",
       "0  1119015538_e8e796281e.jpg          1  \n",
       "1  1131932671_c8d17751b3.jpg          1  \n",
       "2   115684808_cb01227802.jpg          1  \n",
       "3  1174629344_a2e1a2bdbf.jpg          1  \n",
       "4  1322323208_c7ecb742c6.jpg          1  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ntpath import basename\n",
    "from pandas import DataFrame\n",
    "\n",
    "labeled_data = DataFrame(\n",
    "    data={\n",
    "        \"qid\": list(range(relevant_data.shape[0])),\n",
    "        \"query\": [get_caption(\n",
    "            caption_id=x, \n",
    "            captions=captions\n",
    "        ).replace(\" ,\", \"\").replace(\" .\", \"\") for x in list(relevant_data.caption_id)],\n",
    "        \"doc_id\": [basename(x) for x in list(relevant_data.image_file_name)],\n",
    "        \"relevance\": 1}\n",
    ")\n",
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-compression",
   "metadata": {},
   "source": [
    "## From text to embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-victim",
   "metadata": {},
   "source": [
    "Create a text processor to map a text string into the same 512-dimensional space used to embed the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "relative-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "class TextProcessor(object):\n",
    "    def __init__(self, model_name):\n",
    "        self.model, _ = clip.load(model_name)\n",
    "        \n",
    "    def embed(self, text):\n",
    "        text_tokens = clip.tokenize(text)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_tokens).float()\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-breeding",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-lithuania",
   "metadata": {},
   "source": [
    "Define search evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "identified-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.evaluation import MatchRatio, Recall, ReciprocalRank\n",
    "\n",
    "eval_metrics = [\n",
    "    MatchRatio(), \n",
    "    Recall(at=5), \n",
    "    Recall(at=100), \n",
    "    ReciprocalRank(at=5), \n",
    "    ReciprocalRank(at=100)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-sussex",
   "metadata": {},
   "source": [
    "Instantiate `TextProcessor` with a specific CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "inside-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextProcessor(model_name=\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-forum",
   "metadata": {},
   "source": [
    "Create a `QueryModel`'s to be evaluated. In this case we create two query models based on the `ViT-B/32` CLIP model, one that sends the `query` as it is and another that prepends the prompt \"A photo of \" to the query before sending it, as suggest in the original CLIP paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "supported-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import QueryModel\n",
    "\n",
    "def create_vespa_query(query, prompt = False):\n",
    "    if prompt:\n",
    "        query = \"A photo of \" + query.lower()\n",
    "    return {\n",
    "        'yql': 'select * from sources * where ([{\"targetNumHits\":100}]nearestNeighbor(vit_b_32_image,vit_b_32_text));',\n",
    "        'hits': 100,\n",
    "        'ranking.features.query(vit_b_32_text)': text_processor.embed(query),\n",
    "        'ranking.profile': 'vit-b-32-similarity',\n",
    "        'timeout': 10\n",
    "    }\n",
    "\n",
    "query_model_1 = QueryModel(name=\"vit_b_32\", body_function=create_vespa_query)\n",
    "query_model_2 = QueryModel(name=\"vit_b_32_prompt\", body_function=lambda x: create_vespa_query(x, prompt=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-controversy",
   "metadata": {},
   "source": [
    "Create a connection to the Vespa instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "iraqi-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Vespa(\n",
    "    url=os.environ[\"VESPA_END_POINT\"],\n",
    "    cert = os.environ[\"PRIVATE_CERTIFICATE_PATH\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-prior",
   "metadata": {},
   "source": [
    "Evaluate the query models using the labeled data and metrics defined earlier. The labeled data uses the `image_file_name` and doc id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dominant-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.application import Vespa\n",
    "\n",
    "result = app.evaluate(\n",
    "    labeled_data=labeled_data, \n",
    "    eval_metrics=eval_metrics, \n",
    "    query_model=[query_model_1, query_model_2], \n",
    "    id_field=\"image_file_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-investment",
   "metadata": {},
   "source": [
    "The results shows that there is a lot of improvements to be made on the pre-trained `ViT-B/32` CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "composed-granny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>vit_b_32</th>\n",
       "      <th>vit_b_32_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">match_ratio</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">recall_5</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.417004</td>\n",
       "      <td>0.412955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.494065</td>\n",
       "      <td>0.493365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">recall_100</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.870445</td>\n",
       "      <td>0.870445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.336495</td>\n",
       "      <td>0.336495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">reciprocal_rank_5</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.279285</td>\n",
       "      <td>0.268084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.394814</td>\n",
       "      <td>0.386606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">reciprocal_rank_100</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.304849</td>\n",
       "      <td>0.293651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.378595</td>\n",
       "      <td>0.370633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                       vit_b_32  vit_b_32_prompt\n",
       "match_ratio         mean    0.012359         0.012359\n",
       "                    median  0.012359         0.012359\n",
       "                    std     0.000000         0.000000\n",
       "recall_5            mean    0.417004         0.412955\n",
       "                    median  0.000000         0.000000\n",
       "                    std     0.494065         0.493365\n",
       "recall_100          mean    0.870445         0.870445\n",
       "                    median  1.000000         1.000000\n",
       "                    std     0.336495         0.336495\n",
       "reciprocal_rank_5   mean    0.279285         0.268084\n",
       "                    median  0.000000         0.000000\n",
       "                    std     0.394814         0.386606\n",
       "reciprocal_rank_100 mean    0.304849         0.293651\n",
       "                    median  0.111111         0.100000\n",
       "                    std     0.378595         0.370633"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
