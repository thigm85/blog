{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "individual-liverpool",
   "metadata": {},
   "source": [
    "# Vespa text-image search with PyTorch feeder\n",
    "> Create, deploy and feed a text-image search app\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [image processing, clip model, vespa, pytorch, pytorch dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-climate",
   "metadata": {},
   "source": [
    "This post describe how to define and deploy a Vespa image search app through Vespa python API. In addition, we create a Vespa feeder based on PyTorch Dataset/Dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-arena",
   "metadata": {},
   "source": [
    "Check the following posts for context about the data and the model used here:\n",
    "\n",
    "* [Flicker 8k dataset first exploration](https://thigm85.github.io/blog/flicker8k/dataset/image/nlp/2021/10/21/flicker8k-dataset-first-exploration.html)\n",
    "* [Understanding CLIP image pipeline](https://thigm85.github.io/blog/image%20processing/clip%20model/dual%20encoder/pil/2021/10/22/understanding-clip-image-pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-suspension",
   "metadata": {},
   "source": [
    "## Vespa image search app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-passion",
   "metadata": {},
   "source": [
    "Check [pyvespa documentation](https://pyvespa.readthedocs.io/en/latest/index.html) for more info about Vespa python API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-beverage",
   "metadata": {},
   "source": [
    "### Create application package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greatest-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage, Field, HNSW, RankProfile, QueryTypeField\n",
    "\n",
    "app_package = ApplicationPackage(name=\"image_search\")\n",
    "\n",
    "app_package.schema.add_fields(\n",
    "    Field(\n",
    "        name=\"image_file_name\", \n",
    "        type=\"string\", \n",
    "        indexing=[\"summary\", \"attribute\"]\n",
    "    ),\n",
    "    Field(\n",
    "        name=\"vit_b_32_image\", \n",
    "        type=\"tensor<float>(x[512])\", \n",
    "        indexing=[\"attribute\", \"index\"], \n",
    "        ann=HNSW(\n",
    "            distance_metric=\"euclidean\", \n",
    "            max_links_per_node=16, \n",
    "            neighbors_to_explore_at_insert=500\n",
    "        )\n",
    "    )    \n",
    ")\n",
    "\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(\n",
    "        name=\"vit-b-32-similarity\", \n",
    "        inherits=\"default\", \n",
    "        first_phase=\"closeness(vit_b_32_image)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "app_package.query_profile_type.add_fields(\n",
    "    QueryTypeField(\n",
    "        name=\"ranking.features.query(vit_b_32_text)\", \n",
    "        type=\"tensor<float>(x[512])\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-module",
   "metadata": {},
   "source": [
    "### Deploy application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quiet-lexington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for application status.\n",
      "Waiting for application status.\n",
      "Waiting for application status.\n",
      "Finished deployment.\n"
     ]
    }
   ],
   "source": [
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker(disk_folder=os.environ[\"DISK_FOLDER\"])\n",
    "app = vespa_docker.deploy(application_package=app_package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-exclusive",
   "metadata": {},
   "source": [
    "## Feeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db66134-f9db-445d-ae7e-5452242d4665",
   "metadata": {},
   "source": [
    "### Create pytorch ImageFeedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00c56f-5576-4403-9cc4-dd1d5f98fb75",
   "metadata": {},
   "source": [
    "Create a custom Dataset that loads an image, transform it into a 512-dimension vector and return data into a pyvespa-compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cea80a-b1e0-4f86-974a-a1e145c4fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import ntpath\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import clip\n",
    "\n",
    "\n",
    "class ImageFeedDataset(Dataset):\n",
    "    def __init__(self, img_dir, image_embedding_name, model_name):\n",
    "        self.model, self.preprocess = clip.load(model_name)        \n",
    "        self.img_dir = img_dir\n",
    "        self.image_file_names = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "        self.image_embedding_name = image_embedding_name\n",
    "\n",
    "    def _from_image_to_vector(self, x):\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(self.preprocess(x).unsqueeze(0)).float()\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        return image_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file_name = self.image_file_names[idx]\n",
    "        image = Image.open(image_file_name)\n",
    "        image = self._from_image_to_vector(image)\n",
    "        image_base_name = ntpath.basename(image_file_name)\n",
    "        return {\n",
    "            \"id\": image_base_name.split(\".jpg\")[0], \n",
    "            \"fields\": {\n",
    "                \"image_file_name\": image_base_name, \n",
    "                self.image_embedding_name: {\"values\": image.tolist()[0]}\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce302d-0af9-42c4-8ef1-ce556c135a8c",
   "metadata": {},
   "source": [
    "### Instantiate Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf85375-bef6-49f0-8a29-d57650d56e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = ImageFeedDataset(\n",
    "    img_dir=os.environ[\"IMG_DIR\"],  # Folder containing image files     \n",
    "    image_embedding_name=\"vit_b_32_image\",  # name of the Vespa field that will hold image embedding\n",
    "    model_name=\"ViT-B/32\" # CLIP model name used to convert image into vector\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-soundtrack",
   "metadata": {},
   "source": [
    "`dataloader` will make it possible for us to loop through the dataset `batch_size` data points at a time. Since the objective is to feed the data to the vespa `app`, we can set shuffle to `False`. We also specify a custom `collate_fn` function so that pyvespa-compatible format is preserved when batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "connected-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(image_dataset, batch_size=128, shuffle=False, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-adventure",
   "metadata": {},
   "source": [
    "### Feed the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-somalia",
   "metadata": {},
   "source": [
    "Note that most of the time is spent creating the image embedding. So, pre-computing the embedding will provide a significant speed-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "printable-courtesy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0/64\n",
      "Iteration: 1/64\n",
      "Iteration: 2/64\n",
      "Iteration: 3/64\n",
      "Iteration: 4/64\n",
      "Iteration: 5/64\n",
      "Iteration: 6/64\n",
      "Iteration: 7/64\n",
      "Iteration: 8/64\n",
      "Iteration: 9/64\n",
      "Iteration: 10/64\n",
      "Iteration: 11/64\n",
      "Iteration: 12/64\n",
      "Iteration: 13/64\n",
      "Iteration: 14/64\n",
      "Iteration: 15/64\n",
      "Iteration: 16/64\n",
      "Iteration: 17/64\n",
      "Iteration: 18/64\n",
      "Iteration: 19/64\n",
      "Iteration: 20/64\n",
      "Iteration: 21/64\n",
      "Iteration: 22/64\n",
      "Iteration: 23/64\n",
      "Iteration: 24/64\n",
      "Iteration: 25/64\n",
      "Iteration: 26/64\n",
      "Iteration: 27/64\n",
      "Iteration: 28/64\n",
      "Iteration: 29/64\n",
      "Iteration: 30/64\n",
      "Iteration: 31/64\n",
      "Iteration: 32/64\n",
      "Iteration: 33/64\n",
      "Iteration: 34/64\n",
      "Iteration: 35/64\n",
      "Iteration: 36/64\n",
      "Iteration: 37/64\n",
      "Iteration: 38/64\n",
      "Iteration: 39/64\n",
      "Iteration: 40/64\n",
      "Iteration: 41/64\n",
      "Iteration: 42/64\n",
      "Iteration: 43/64\n",
      "Iteration: 44/64\n",
      "Iteration: 45/64\n",
      "Iteration: 46/64\n",
      "Iteration: 47/64\n",
      "Iteration: 48/64\n",
      "Iteration: 49/64\n",
      "Iteration: 50/64\n",
      "Iteration: 51/64\n",
      "Iteration: 52/64\n",
      "Iteration: 53/64\n",
      "Iteration: 54/64\n",
      "Iteration: 55/64\n",
      "Iteration: 56/64\n",
      "Iteration: 57/64\n",
      "Iteration: 58/64\n",
      "Iteration: 59/64\n",
      "Iteration: 60/64\n",
      "Iteration: 61/64\n",
      "Iteration: 62/64\n",
      "Iteration: 63/64\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(dataloader):\n",
    "    print(\"Iteration: {}/{}\".format(idx, len(dataloader)))\n",
    "    app.feed_batch(batch=batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
