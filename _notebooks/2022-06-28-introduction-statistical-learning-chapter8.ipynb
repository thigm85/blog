{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d394cd",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 8 Tree-based methods\n",
    "> Search and recommendation\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [book, tree-based methods]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d4d5d",
   "metadata": {},
   "source": [
    "## The basics of decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4db281",
   "metadata": {},
   "source": [
    "### Regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ed134",
   "metadata": {},
   "source": [
    "<img src=\"images/regression_tree_algo.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede146e",
   "metadata": {},
   "source": [
    "* Step 1\n",
    "    * The goal is to find boxes $R_1, ... R_j$ that minimizes the $RSS=\\sum_{j=1}^J \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})$, where $\\hat{y}_{R_j}$ is the mean response for the training observations within $R_j$.\n",
    "    * It is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.\n",
    "    * For this reason, we take a top-down, greedy approach that is known as recursive binary splitting.\n",
    "        * The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space.\n",
    "        * It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n",
    "        * In greater detail, for any $j$ and $s$, we define the pair of half-planes $R_1(j, s) = \\{X|X_j < s\\}$ and $R_2(j, s) = \\{X|Xj ≥ s\\}$, and we seek the value of $j$ and $s$ that minimize the equation \n",
    "        $$\\sum_{i: x_i \\in R_1(j,s)}(y_i − \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j,s)}(y_i − \\hat{y}_{R_2})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df72736",
   "metadata": {},
   "source": [
    "* Step 2\n",
    "    * Step 1 tends to overfit the training data.\n",
    "    * Computing the test error at each partition on step 1 is expensive. We use cost complexity pruning instead.\n",
    "    * For each value of $\\alpha$ there corresponds a subtree $T \\subset T_0$ such that \n",
    "    $$ \\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} (y_i − \\hat{y}_{R_m})^2 + \\alpha |T|$$\n",
    "        is as small as possible, where $|T|$ is the number of terminal nodes of the tree $T$ and $\\alpha$ is a tunning parameter.\n",
    "    * It turns out that as we increase $\\alpha$ from zero, branches get pruned from the tree in a nested and predictable fashion, so obtaining the whole sequence of subtrees as a function of $\\alpha$ is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0ed49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
