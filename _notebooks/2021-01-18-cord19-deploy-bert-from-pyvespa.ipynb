{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic app based on bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the application package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage, Field, FieldSet, RankProfile\n",
    "\n",
    "app_package = ApplicationPackage(name=\"cord19\")\n",
    "app_package.schema.add_fields(\n",
    "    Field(name = \"cord_uid\", type = \"string\", indexing = [\"attribute\", \"summary\"]),\n",
    "    Field(name = \"title\", type = \"string\", indexing = [\"index\", \"summary\"], index = \"enable-bm25\")\n",
    ")\n",
    "app_package.schema.add_field_set(\n",
    "    FieldSet(name = \"default\", fields = [\"title\"])\n",
    ")\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(name = \"bm25\", first_phase = \"bm25(title)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the absolute disk path to store the application files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WORK_DIR\"] = \"/Users/tmartins\"\n",
    "disk_folder = os.path.join(os.getenv(\"WORK_DIR\"), \"sample_application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy to a docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker(port=8089)\n",
    "\n",
    "app = vespa_docker.deploy(\n",
    "    application_package = app_package,\n",
    "    disk_folder=disk_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed data to the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "parsed_feed = read_csv(\"/Users/tmartins/projects/sw/blog/_notebooks/data/2021-01-18-cord19-deploy-bert-from-pyvespa/parsed_feed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_feed = parsed_feed.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in parsed_feed.iterrows():\n",
    "    response = app.feed_data_point(\n",
    "        schema = \"cord19\",\n",
    "        data_id = str(row[\"cord_uid\"]),\n",
    "        fields = {\n",
    "            \"cord_uid\": str(row[\"cord_uid\"]),\n",
    "            \"title\": str(row[\"title\"]),\n",
    "        }\n",
    "    )\n",
    "    #print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import QueryModel, RankProfile as Ranking, OR\n",
    "\n",
    "result = app.query(\n",
    "    query=\"this is a test\", \n",
    "    query_model=QueryModel(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bm25\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.number_documents_retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT model through ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\")  # This could be any pytorch BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.ml import BertModelConfig\n",
    "\n",
    "bert_config = BertModelConfig(\n",
    "    model_id=\"pretrained_bert_tiny\",\n",
    "    tokenizer=tokenizer,\n",
    "    query_input_size=32,\n",
    "    doc_input_size=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import SecondPhaseRanking\n",
    "\n",
    "app_package.add_bert_ranking(\n",
    "    model_config=bert_config,\n",
    "    model=model,\n",
    "    inherits=\"default\",\n",
    "    first_phase=\"bm25(title)\",\n",
    "    second_phase=SecondPhaseRanking(\n",
    "        rerank_count=10, expression=\"logit0\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redeploy the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = vespa_docker.deploy(\n",
    "    application_package = app_package,\n",
    "    disk_folder=disk_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed data to the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_config.doc_tensor(text = str(row[\"title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in parsed_feed.iterrows():\n",
    "    fields = {\n",
    "        \"cord_uid\": str(row[\"cord_uid\"]),\n",
    "        \"title\": str(row[\"title\"]),\n",
    "    }\n",
    "    fields.update(bert_config.doc_tensor(text = str(row[\"title\"])))\n",
    "\n",
    "    response = app.feed_data_point(\n",
    "        schema = \"cord19\",\n",
    "        data_id = str(row[\"cord_uid\"]),\n",
    "        fields=fields\n",
    "    )\n",
    "    #print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import QueryModel, RankProfile as Ranking, OR, QueryRankingFeature\n",
    "\n",
    "result = app.query(\n",
    "    query=\"this is a test\", \n",
    "    query_model=QueryModel(\n",
    "        query_properties=[\n",
    "            QueryRankingFeature(\n",
    "                name=bert_config.query_token_ids_name, \n",
    "                mapping=bert_config.query_tensor_mapping)\n",
    "        ],\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"pretrained_bert_tiny\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.number_documents_retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to encode queries sent to the Vespa application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config.query_input_ids(\n",
    "    queries=[\"this is a query\", \"here is another query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to encode document text to be fed to the Vespa application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config.doc_input_ids(\n",
    "    docs=[\"this is a text\", \"another text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to generate data to train model that is compatible to the data used to serve the model on Vespa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = bert_config.create_encodings(\n",
    "    queries=[\"this is a query\", \"here is another query\"],\n",
    "    docs=[\"this is a text\", \"another text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much easier to export a pytorch model (no need to specify input and output names, generate dummy data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\") # This could be any pytorch BERT model \n",
    "bert_config.export_to_onnx(model=model, output_path=\"bert_tiny.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behind the scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_encodings(queries, docs, tokenizer, query_input_size, doc_input_size):\n",
    "    queries_encodings = tokenizer(\n",
    "        queries, truncation=True, max_length=query_input_size-2, add_special_tokens=False\n",
    "    )\n",
    "    docs_encodings = tokenizer(\n",
    "        docs, truncation=True, max_length=doc_input_size-1, add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    TOKEN_NONE=0\n",
    "    TOKEN_CLS=101\n",
    "    TOKEN_SEP=102\n",
    "\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    for query_input_ids, doc_input_ids in zip(queries_encodings[\"input_ids\"], docs_encodings[\"input_ids\"]):\n",
    "        # create input id\n",
    "        input_id = [TOKEN_CLS] + query_input_ids + [TOKEN_SEP] + doc_input_ids + [TOKEN_SEP]\n",
    "        number_tokens = len(input_id)\n",
    "        padding_length = max(128 - number_tokens, 0)\n",
    "        input_id = input_id + [TOKEN_NONE] * padding_length\n",
    "        input_ids.append(input_id)\n",
    "        # create token id\n",
    "        token_type_id = [0] * len([TOKEN_CLS] + query_input_ids + [TOKEN_SEP]) + [1] * len(doc_input_ids + [TOKEN_SEP]) + [TOKEN_NONE] * padding_length\n",
    "        token_type_ids.append(token_type_id)\n",
    "        # create attention_mask\n",
    "        attention_mask.append([1] * number_tokens + [TOKEN_NONE] * padding_length)\n",
    "\n",
    "    encodings = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = create_bert_encodings(\n",
    "    queries=[\"dummy query 1\"],\n",
    "    docs=[\"dummy document 1\"],\n",
    "    tokenizer=tokenizer,\n",
    "    query_input_size=32,\n",
    "    doc_input_size=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export\n",
    "\n",
    "model_onnx_path = \"bert_tiny.onnx\"\n",
    "dummy_input = (\n",
    "    tensor(encodings[\"input_ids\"]), \n",
    "    tensor(encodings[\"token_type_ids\"]), \n",
    "    tensor(encodings[\"attention_mask\"]), \n",
    ")\n",
    "input_names = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "output_names = [\"logits\"]\n",
    "export(\n",
    "    model, dummy_input, model_onnx_path, input_names = input_names, \n",
    "    output_names = output_names, verbose=False, opset_version=11\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the application to deploy interaction BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document token ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need a field for the document token ids. \n",
    "\n",
    "* Ideally we should name this field according to a BERT model id in case we want to deploy multiple models on the same application. \n",
    "\n",
    "* We also need to specify the maximum size of the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_package.schema.add_fields(\n",
    "    Field(name = \"doc_token_ids\", type = \"tensor<float>(d0[96])\", indexing = [\"attribute\", \"summary\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query vector type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each model deployed should use its own query vector to send the token ids.\n",
    "\n",
    "* Similar to the document vector the name of the query vector should be influenced by the model used.\n",
    "\n",
    "* We also need to specify the maximum size of the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import QueryTypeField\n",
    "\n",
    "    app_package.query_profile_type.add_fields(\n",
    "        QueryTypeField(name=\"ranking.features.query(query_token_ids)\", type = \"tensor<float>(d0[32])\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model name here should be the string used to add to the name of the document and query vectors to ensure uniqueness. \n",
    "\n",
    "* Somehow we need to make sure the model `bert.onnx` ends up in the write location when deploying the app.\n",
    "\n",
    "* Need to coordinate input and output names to make sure there is no clash between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import OnnxModel\n",
    "\n",
    "app_package.schema.add_model(\n",
    "    OnnxModel(\n",
    "        model_name=\"bert\",\n",
    "        file_path=\"files/bert.onnx\",\n",
    "        inputs={\n",
    "            \"input_ids\": \"input_ids\",\n",
    "            \"token_type_ids\": \"token_type_ids\",\n",
    "            \"attention_mask\": \"attention_mask\",\n",
    "        },\n",
    "        outputs={\"logits\": \"logits\"},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT rank profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import RankProfile, Function, SecondPhaseRanking\n",
    "\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(\n",
    "        name=\"bert\",\n",
    "        inherits=\"default\",\n",
    "        constants={\"TOKEN_NONE\": 0, \"TOKEN_CLS\": 101, \"TOKEN_SEP\": 102}, \n",
    "        functions=[\n",
    "            Function(\n",
    "                name=\"question_length\",\n",
    "                expression=\"sum(map(query(query_token_ids), f(a)(a > 0)))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"doc_length\",\n",
    "                expression=\"sum(map(attribute(doc_token_ids), f(a)(a > 0)))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"input_ids\",\n",
    "                expression=\"tensor<float>(d0[1],d1[128])(\\n\"\n",
    "                \"    if (d1 == 0,\\n\"\n",
    "                \"        TOKEN_CLS,\\n\"\n",
    "                \"    if (d1 < question_length + 1,\\n\"\n",
    "                \"        query(query_token_ids){d0:(d1-1)},\\n\"\n",
    "                \"    if (d1 == question_length + 1,\\n\"\n",
    "                \"        TOKEN_SEP,\\n\"\n",
    "                \"    if (d1 < question_length + doc_length + 2,\\n\"\n",
    "                \"        attribute(doc_token_ids){d0:(d1-question_length-2)},\\n\"\n",
    "                \"    if (d1 == question_length + doc_length + 2,\\n\"\n",
    "                \"        TOKEN_SEP,\\n\"\n",
    "                \"        TOKEN_NONE\\n\"\n",
    "                \"    ))))))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"attention_mask\",\n",
    "                expression=\"map(input_ids, f(a)(a > 0))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"token_type_ids\",\n",
    "                expression=\"tensor<float>(d0[1],d1[128])(\\n\"\n",
    "                \"    if (d1 < question_length,\\n\"\n",
    "                \"        0,\\n\"\n",
    "                \"    if (d1 < question_length + doc_length,\\n\"\n",
    "                \"        1,\\n\"\n",
    "                \"        TOKEN_NONE\\n\"\n",
    "                \"    )))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"eval\",\n",
    "                expression=\"tensor(x{}):{x1:onnxModel(bert).logits{d0:0,d1:0}}\",\n",
    "            ),\n",
    "            \n",
    "        ],     \n",
    "        first_phase=\"bm25(title) + bm25(body)\",\n",
    "        second_phase=SecondPhaseRanking(\n",
    "            rerank_count=10, expression=\"sum(eval)\"\n",
    "        ),\n",
    "        summary_features=[\n",
    "            \"onnxModel(bert).logits\",\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"token_type_ids\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redeploy the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = vespa_docker.deploy(\n",
    "    application_package = app_package,\n",
    "    disk_folder=disk_folder\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
