{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic app based on bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage, Field, FieldSet, RankProfile\n",
    "\n",
    "app_package = ApplicationPackage(name=\"cord19\")\n",
    "app_package.schema.add_fields(\n",
    "    Field(name = \"cord_uid\", type = \"string\", indexing = [\"attribute\", \"summary\"]),\n",
    "    Field(name = \"title\", type = \"string\", indexing = [\"index\", \"summary\"], index = \"enable-bm25\")\n",
    ")\n",
    "app_package.schema.add_field_set(\n",
    "    FieldSet(name = \"default\", fields = [\"title\"])\n",
    ")\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(name = \"bm25\", first_phase = \"bm25(title)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the absolute disk path to store the application files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WORK_DIR\"] = \"/Users/tmartins\"\n",
    "disk_folder = os.path.join(os.getenv(\"WORK_DIR\"), \"sample_application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy to a docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for application status.\n",
      "Waiting for application status.\n"
     ]
    }
   ],
   "source": [
    "from vespa.package import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker(port=8089)\n",
    "\n",
    "app = vespa_docker.deploy(\n",
    "    application_package = app_package,\n",
    "    disk_folder=disk_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps required to deploy BERT:\n",
    "\n",
    "* Add onnx-model in the sd.\n",
    "* Create a models folder in the same level of the schemas folder\n",
    "* Create a rank-profile that define inputs and use the model to rank\n",
    "* Add the input field tensors related to the docs\n",
    "* Add query profile with the relevant tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy application from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_docker.container = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_docker.container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = vespa_docker.deploy_from_disk(\n",
    "    application_name=\"cord19\", \n",
    "    disk_folder=\"/Users/tmartins/projects/vespa/pyvespa/docs/sphinx/source/use_cases/cord19/sample_application\", \n",
    "    container_memory=\"10G\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.deployment_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed data to the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_sets = json.load(open(\"cord19/test_sets.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_to_feed = []\n",
    "for test_set in test_sets:\n",
    "    for query_point in test_sets[test_set]:\n",
    "        query = query_point[\"query\"]\n",
    "        print(query)\n",
    "        result = app.query(\n",
    "            query=query, \n",
    "            query_model=Query(\n",
    "                match_phase = OR(),\n",
    "                rank_profile = Ranking(name=\"bm25\")\n",
    "            ),\n",
    "            timeout=\"20s\",    \n",
    "            hits = 100\n",
    "        )\n",
    "        assert len(result.hits) > 0\n",
    "        for hit in result.hits:\n",
    "            documents_to_feed.append(\n",
    "                {\"cord_uid\": hit[\"fields\"][\"cord_uid\"],\n",
    "                 \"title\": hit[\"fields\"][\"title-full\"]}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"cord19/documents_to_feed.json\", \"w\") as f:\n",
    "    f.write(json.dumps(documents_to_feed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"cord19/documents_to_feed.json\", \"r\") as f:\n",
    "    documents_to_feed = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_to_feed[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_batch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents_to_feed:\n",
    "    response = app.feed_data_point(\n",
    "        schema = \"cord19\",\n",
    "        data_id = str(document[\"cord_uid\"]),\n",
    "        fields = {\n",
    "            \"cord_uid\": str(document[\"cord_uid\"]),\n",
    "            \"title\": str(document[\"title\"]),\n",
    "            \"title_token_ids\": {\"values\": tokenizer(\n",
    "                str(document[\"title\"]), \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "                max_length=63, \n",
    "                add_special_tokens=False\n",
    "            )[\"input_ids\"]}\n",
    "        }\n",
    "    )\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.application import Vespa\n",
    "\n",
    "app = Vespa(url = \"http://localhost\", port = 8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import Query, OR, RankProfile as Ranking\n",
    "\n",
    "query = 'coronavirus origin'\n",
    "result = app.query(\n",
    "    query=query, \n",
    "    query_model=Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"default\")),\n",
    "    timeout=\"20s\",    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import RankProfile as Ranking\n",
    "\n",
    "query = 'coronavirus origin'\n",
    "result = app.query(\n",
    "    query=query, \n",
    "    query_model=Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bert_index_1\")),\n",
    "    timeout=\"20s\",\n",
    "    debug_request=False,\n",
    "    **{\"ranking.features.query(query_token_ids)\": str(tokenizer(\n",
    "                str(query), \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "                max_length=64, \n",
    "                add_special_tokens=False\n",
    "            )[\"input_ids\"])}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[hit[\"relevance\"] for hit in result.hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.request_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define query models that we want to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import Query, RankProfile, OR\n",
    "\n",
    "query_models = {\n",
    "    \"or_bm25\": Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bm25\")\n",
    "    ),\n",
    "    \"or_bm25_bert\": Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bert\")\n",
    "    ),\n",
    "    \"or_bm25_bert_index_1\": Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bert_index_1\")\n",
    "    )\n",
    "    \n",
    "}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.evaluation import MatchRatio, Recall, ReciprocalRank, NormalizedDiscountedCumulativeGain\n",
    "\n",
    "eval_metrics = [MatchRatio(), Recall(at=10), ReciprocalRank(at=10), NormalizedDiscountedCumulativeGain(at=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        evaluation = []\n",
    "        for query_data in labelled_data:\n",
    "            evaluation_query = self.evaluate_query(\n",
    "                eval_metrics=eval_metrics,\n",
    "                query_model=query_model,\n",
    "                query_id=query_data[\"query_id\"],\n",
    "                query=query_data[\"query\"],\n",
    "                id_field=id_field,\n",
    "                relevant_docs=query_data[\"relevant_docs\"],\n",
    "                default_score=default_score,\n",
    "                **kwargs\n",
    "            )\n",
    "            evaluation.append(evaluation_query)\n",
    "        evaluation = DataFrame.from_records(evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in test_sets:\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data[\"relevant_docs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "evaluations = {}\n",
    "for test_set in test_sets:\n",
    "    evaluations[test_set] = {}\n",
    "    for query_model in query_models:\n",
    "        evaluation = []\n",
    "        for query_data in test_sets[test_set]:\n",
    "            print(query_data[\"query_id\"])\n",
    "            evaluation_query = app.evaluate_query(\n",
    "                eval_metrics=eval_metrics,\n",
    "                query_model=query_models[query_model],\n",
    "                query_id=query_data[\"query_id\"],\n",
    "                query=query_data[\"query\"],\n",
    "                id_field = \"cord_uid\",\n",
    "                relevant_docs=query_data[\"relevant_docs\"],\n",
    "                hits = 10,\n",
    "                timeout=\"100s\",\n",
    "                **{\"ranking.features.query(query_token_ids)\": str(tokenizer(\n",
    "                            str(query_data[\"query\"]), \n",
    "                            truncation=True, \n",
    "                            padding=\"max_length\",\n",
    "                            max_length=64, \n",
    "                            add_special_tokens=False\n",
    "                        )[\"input_ids\"])}            \n",
    "            )\n",
    "            evaluation.append(evaluation_query)\n",
    "        evaluations[test_set][query_model] = DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metric_values = []\n",
    "for test_set in test_sets:\n",
    "    for query_model in query_models:\n",
    "        for metric in eval_metrics:\n",
    "            metric_values.append(\n",
    "                pd.DataFrame(\n",
    "                    data={\n",
    "                        \"test_set\": test_set, \n",
    "                        \"query_model\": query_model, \n",
    "                        \"metric\": metric.name, \n",
    "                        \"value\": evaluations[test_set][query_model][metric.name + \"_value\"].to_list()\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "metric_values = pd.concat(metric_values, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values.metric.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.box(metric_values[metric_values.metric == \"reciprocal_rank_10\"], x=\"query_model\", y=\"value\", title = \"RR @ 10\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values.groupby(['query_model', 'metric']).median()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
