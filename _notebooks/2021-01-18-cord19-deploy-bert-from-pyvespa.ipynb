{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic app based on bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the application package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage, Field, FieldSet, RankProfile\n",
    "\n",
    "app_package = ApplicationPackage(name=\"cord19\")\n",
    "app_package.schema.add_fields(\n",
    "    Field(name = \"cord_uid\", type = \"string\", indexing = [\"attribute\", \"summary\"]),\n",
    "    Field(name = \"title\", type = \"string\", indexing = [\"index\", \"summary\"], index = \"enable-bm25\")\n",
    ")\n",
    "app_package.schema.add_field_set(\n",
    "    FieldSet(name = \"default\", fields = [\"title\"])\n",
    ")\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(name = \"bm25\", first_phase = \"bm25(title)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the absolute disk path to store the application files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WORK_DIR\"] = \"/Users/tmartins\"\n",
    "disk_folder = os.path.join(os.getenv(\"WORK_DIR\"), \"sample_application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy to a docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker(port=8089)\n",
    "\n",
    "app = vespa_docker.deploy(\n",
    "    application_package = app_package,\n",
    "    disk_folder=disk_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed data to the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "parsed_feed = read_csv(\"/Users/tmartins/projects/sw/blog/_notebooks/data/2021-01-18-cord19-deploy-bert-from-pyvespa/parsed_feed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_feed = parsed_feed.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in parsed_feed.iterrows():\n",
    "    response = app.feed_data_point(\n",
    "        schema = \"cord19\",\n",
    "        data_id = str(row[\"cord_uid\"]),\n",
    "        fields = {\n",
    "            \"cord_uid\": str(row[\"cord_uid\"]),\n",
    "            \"title\": str(row[\"title\"]),\n",
    "        }\n",
    "    )\n",
    "    #print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import QueryModel, RankProfile as Ranking, OR\n",
    "\n",
    "result = app.query(\n",
    "    query=\"this is a test\", \n",
    "    query_model=QueryModel(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bm25\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.number_documents_retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a BERT model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a `from vespa.ml import BertModel` that contains all the information required to ensure training and serving compatibility. The instance can be used when including the model in the application package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_encodings(queries, docs, tokenizer, query_input_size, doc_input_size):\n",
    "    queries_encodings = tokenizer(\n",
    "        queries, truncation=True, max_length=query_input_size-2, add_special_tokens=False\n",
    "    )\n",
    "    docs_encodings = tokenizer(\n",
    "        docs, truncation=True, max_length=doc_input_size-1, add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    TOKEN_NONE=0\n",
    "    TOKEN_CLS=101\n",
    "    TOKEN_SEP=102\n",
    "\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    for query_input_ids, doc_input_ids in zip(queries_encodings[\"input_ids\"], docs_encodings[\"input_ids\"]):\n",
    "        # create input id\n",
    "        input_id = [TOKEN_CLS] + query_input_ids + [TOKEN_SEP] + doc_input_ids + [TOKEN_SEP]\n",
    "        number_tokens = len(input_id)\n",
    "        padding_length = max(128 - number_tokens, 0)\n",
    "        input_id = input_id + [TOKEN_NONE] * padding_length\n",
    "        input_ids.append(input_id)\n",
    "        # create token id\n",
    "        token_type_id = [0] * len([TOKEN_CLS] + query_input_ids + [TOKEN_SEP]) + [1] * len(doc_input_ids + [TOKEN_SEP]) + [TOKEN_NONE] * padding_length\n",
    "        token_type_ids.append(token_type_id)\n",
    "        # create attention_mask\n",
    "        attention_mask.append([1] * number_tokens + [TOKEN_NONE] * padding_length)\n",
    "\n",
    "    encodings = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "#model_name = \"google/bert_uncased_L-4_H-512_A-8\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = create_bert_encodings(\n",
    "    queries=[\"dummy query 1\"],\n",
    "    docs=[\"dummy document 1\"],\n",
    "    tokenizer=tokenizer,\n",
    "    query_input_size=32,\n",
    "    doc_input_size=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101,\n",
       "   24369,\n",
       "   23032,\n",
       "   1015,\n",
       "   102,\n",
       "   24369,\n",
       "   6254,\n",
       "   1015,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'token_type_ids': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "\n",
    "tensor(encodings[\"input_ids\"]).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(encodings[\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export\n",
    "\n",
    "model_onnx_path = \"bert_tiny.onnx\"\n",
    "dummy_input = (\n",
    "    tensor(encodings[\"input_ids\"]), \n",
    "    tensor(encodings[\"token_type_ids\"]), \n",
    "    tensor(encodings[\"attention_mask\"]), \n",
    ")\n",
    "input_names = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "output_names = [\"logits\"]\n",
    "export(\n",
    "    model, dummy_input, model_onnx_path, input_names = input_names, \n",
    "    output_names = output_names, verbose=False, opset_version=11\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the application to deploy interaction BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document token ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need a field for the document token ids. \n",
    "\n",
    "* Ideally we should name this field according to a BERT model id in case we want to deploy multiple models on the same application. \n",
    "\n",
    "* We also need to specify the maximum size of the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_package.schema.add_fields(\n",
    "    Field(name = \"doc_token_ids\", type = \"tensor<float>(d0[96])\", indexing = [\"attribute\", \"summary\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query vector type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each model deployed should use its own query vector to send the token ids.\n",
    "\n",
    "* Similar to the document vector the name of the query vector should be influenced by the model used.\n",
    "\n",
    "* We also need to specify the maximum size of the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import QueryTypeField\n",
    "\n",
    "    app_package.query_profile_type.add_fields(\n",
    "        QueryTypeField(name=\"ranking.features.query(query_token_ids)\", type = \"tensor<float>(d0[32])\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model name here should be the string used to add to the name of the document and query vectors to ensure uniqueness. \n",
    "\n",
    "* Somehow we need to make sure the model `bert.onnx` ends up in the write location when deploying the app.\n",
    "\n",
    "* Need to coordinate input and output names to make sure there is no clash between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import OnnxModel\n",
    "\n",
    "app_package.schema.add_model(\n",
    "    OnnxModel(\n",
    "        model_name=\"bert\",\n",
    "        file_path=\"files/bert.onnx\",\n",
    "        inputs={\n",
    "            \"input_ids\": \"input_ids\",\n",
    "            \"token_type_ids\": \"token_type_ids\",\n",
    "            \"attention_mask\": \"attention_mask\",\n",
    "        },\n",
    "        outputs={\"logits\": \"logits\"},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT rank profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import RankProfile, Function, SecondPhaseRanking\n",
    "\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(\n",
    "        name=\"bert\",\n",
    "        inherits=\"default\",\n",
    "        constants={\"TOKEN_NONE\": 0, \"TOKEN_CLS\": 101, \"TOKEN_SEP\": 102}, \n",
    "        functions=[\n",
    "            Function(\n",
    "                name=\"question_length\",\n",
    "                expression=\"sum(map(query(query_token_ids), f(a)(a > 0)))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"doc_length\",\n",
    "                expression=\"sum(map(attribute(doc_token_ids), f(a)(a > 0)))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"input_ids\",\n",
    "                expression=\"tensor<float>(d0[1],d1[128])(\\n\"\n",
    "                \"    if (d1 == 0,\\n\"\n",
    "                \"        TOKEN_CLS,\\n\"\n",
    "                \"    if (d1 < question_length + 1,\\n\"\n",
    "                \"        query(query_token_ids){d0:(d1-1)},\\n\"\n",
    "                \"    if (d1 == question_length + 1,\\n\"\n",
    "                \"        TOKEN_SEP,\\n\"\n",
    "                \"    if (d1 < question_length + doc_length + 2,\\n\"\n",
    "                \"        attribute(doc_token_ids){d0:(d1-question_length-2)},\\n\"\n",
    "                \"    if (d1 == question_length + doc_length + 2,\\n\"\n",
    "                \"        TOKEN_SEP,\\n\"\n",
    "                \"        TOKEN_NONE\\n\"\n",
    "                \"    ))))))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"attention_mask\",\n",
    "                expression=\"map(input_ids, f(a)(a > 0))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"token_type_ids\",\n",
    "                expression=\"tensor<float>(d0[1],d1[128])(\\n\"\n",
    "                \"    if (d1 < question_length,\\n\"\n",
    "                \"        0,\\n\"\n",
    "                \"    if (d1 < question_length + doc_length,\\n\"\n",
    "                \"        1,\\n\"\n",
    "                \"        TOKEN_NONE\\n\"\n",
    "                \"    )))\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"eval\",\n",
    "                expression=\"tensor(x{}):{x1:onnxModel(bert).logits{d0:0,d1:0}}\",\n",
    "            ),\n",
    "            \n",
    "        ],     \n",
    "        first_phase=\"bm25(title) + bm25(body)\",\n",
    "        second_phase=SecondPhaseRanking(\n",
    "            rerank_count=10, expression=\"sum(eval)\"\n",
    "        ),\n",
    "        summary_features=[\n",
    "            \"onnxModel(bert).logits\",\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"token_type_ids\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redeploy the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = vespa_docker.deploy(\n",
    "    application_package = app_package,\n",
    "    disk_folder=disk_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps required to deploy BERT:\n",
    "\n",
    "* Add onnx-model in the sd.\n",
    "* Create a models folder in the same level of the schemas folder\n",
    "* Create a rank-profile that define inputs and use the model to rank\n",
    "* Add the input field tensors related to the docs\n",
    "* Add query profile with the relevant tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy application from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_docker.container = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_docker.container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = vespa_docker.deploy_from_disk(\n",
    "    application_name=\"cord19\", \n",
    "    disk_folder=\"/Users/tmartins/projects/vespa/pyvespa/docs/sphinx/source/use_cases/cord19/sample_application\", \n",
    "    container_memory=\"10G\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.deployment_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_sets = json.load(open(\"cord19/test_sets.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_to_feed = []\n",
    "for test_set in test_sets:\n",
    "    for query_point in test_sets[test_set]:\n",
    "        query = query_point[\"query\"]\n",
    "        print(query)\n",
    "        result = app.query(\n",
    "            query=query, \n",
    "            query_model=Query(\n",
    "                match_phase = OR(),\n",
    "                rank_profile = Ranking(name=\"bm25\")\n",
    "            ),\n",
    "            timeout=\"20s\",    \n",
    "            hits = 100\n",
    "        )\n",
    "        assert len(result.hits) > 0\n",
    "        for hit in result.hits:\n",
    "            documents_to_feed.append(\n",
    "                {\"cord_uid\": hit[\"fields\"][\"cord_uid\"],\n",
    "                 \"title\": hit[\"fields\"][\"title-full\"]}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"cord19/documents_to_feed.json\", \"w\") as f:\n",
    "    f.write(json.dumps(documents_to_feed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"cord19/documents_to_feed.json\", \"r\") as f:\n",
    "    documents_to_feed = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_to_feed[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_batch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.application import Vespa\n",
    "\n",
    "app = Vespa(url = \"http://localhost\", port = 8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import Query, OR, RankProfile as Ranking\n",
    "\n",
    "query = 'coronavirus origin'\n",
    "result = app.query(\n",
    "    query=query, \n",
    "    query_model=Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"default\")),\n",
    "    timeout=\"20s\",    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import RankProfile as Ranking\n",
    "\n",
    "query = 'coronavirus origin'\n",
    "result = app.query(\n",
    "    query=query, \n",
    "    query_model=Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bert_index_1\")),\n",
    "    timeout=\"20s\",\n",
    "    debug_request=False,\n",
    "    **{\"ranking.features.query(query_token_ids)\": str(tokenizer(\n",
    "                str(query), \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "                max_length=64, \n",
    "                add_special_tokens=False\n",
    "            )[\"input_ids\"])}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[hit[\"relevance\"] for hit in result.hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.request_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define query models that we want to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import Query, RankProfile, OR\n",
    "\n",
    "query_models = {\n",
    "    \"or_bm25\": Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bm25\")\n",
    "    ),\n",
    "    \"or_bm25_bert\": Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bert\")\n",
    "    ),\n",
    "    \"or_bm25_bert_index_1\": Query(\n",
    "        match_phase = OR(),\n",
    "        rank_profile = Ranking(name=\"bert_index_1\")\n",
    "    )\n",
    "    \n",
    "}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.evaluation import MatchRatio, Recall, ReciprocalRank, NormalizedDiscountedCumulativeGain\n",
    "\n",
    "eval_metrics = [MatchRatio(), Recall(at=10), ReciprocalRank(at=10), NormalizedDiscountedCumulativeGain(at=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        evaluation = []\n",
    "        for query_data in labelled_data:\n",
    "            evaluation_query = self.evaluate_query(\n",
    "                eval_metrics=eval_metrics,\n",
    "                query_model=query_model,\n",
    "                query_id=query_data[\"query_id\"],\n",
    "                query=query_data[\"query\"],\n",
    "                id_field=id_field,\n",
    "                relevant_docs=query_data[\"relevant_docs\"],\n",
    "                default_score=default_score,\n",
    "                **kwargs\n",
    "            )\n",
    "            evaluation.append(evaluation_query)\n",
    "        evaluation = DataFrame.from_records(evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in test_sets:\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data[\"relevant_docs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "evaluations = {}\n",
    "for test_set in test_sets:\n",
    "    evaluations[test_set] = {}\n",
    "    for query_model in query_models:\n",
    "        evaluation = []\n",
    "        for query_data in test_sets[test_set]:\n",
    "            print(query_data[\"query_id\"])\n",
    "            evaluation_query = app.evaluate_query(\n",
    "                eval_metrics=eval_metrics,\n",
    "                query_model=query_models[query_model],\n",
    "                query_id=query_data[\"query_id\"],\n",
    "                query=query_data[\"query\"],\n",
    "                id_field = \"cord_uid\",\n",
    "                relevant_docs=query_data[\"relevant_docs\"],\n",
    "                hits = 10,\n",
    "                timeout=\"100s\",\n",
    "                **{\"ranking.features.query(query_token_ids)\": str(tokenizer(\n",
    "                            str(query_data[\"query\"]), \n",
    "                            truncation=True, \n",
    "                            padding=\"max_length\",\n",
    "                            max_length=64, \n",
    "                            add_special_tokens=False\n",
    "                        )[\"input_ids\"])}            \n",
    "            )\n",
    "            evaluation.append(evaluation_query)\n",
    "        evaluations[test_set][query_model] = DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metric_values = []\n",
    "for test_set in test_sets:\n",
    "    for query_model in query_models:\n",
    "        for metric in eval_metrics:\n",
    "            metric_values.append(\n",
    "                pd.DataFrame(\n",
    "                    data={\n",
    "                        \"test_set\": test_set, \n",
    "                        \"query_model\": query_model, \n",
    "                        \"metric\": metric.name, \n",
    "                        \"value\": evaluations[test_set][query_model][metric.name + \"_value\"].to_list()\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "metric_values = pd.concat(metric_values, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values.metric.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.box(metric_values[metric_values.metric == \"reciprocal_rank_10\"], x=\"query_model\", y=\"value\", title = \"RR @ 10\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values.groupby(['query_model', 'metric']).median()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
