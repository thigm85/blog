{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search with embedding representation from python\n",
    "> Build and interact with a MS MARCO search application\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [MS MARCO, vespa, pyvespa, semantic search, text search, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the release of version 0.2.0 for [pyvespa](https://pyvespa.readthedocs.io/en/latest/index.html), we can now create a semantic search application based on embedding representation from python with just a few lines of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage, Field, QueryTypeField, RankProfile\n",
    "\n",
    "app_package = ApplicationPackage(name = \"msmarco\")\n",
    "\n",
    "app_package.schema.add_fields(        \n",
    "    Field(\n",
    "        name = \"id\", type = \"string\", \n",
    "        indexing = [\"attribute\", \"summary\"]\n",
    "    ),\n",
    "    Field(\n",
    "        name = \"title\", type = \"string\", \n",
    "        indexing = [\"index\", \"summary\"], \n",
    "        index = \"enable-bm25\"\n",
    "    ),\n",
    "    Field(\n",
    "        name = \"title_bert\", type = \"tensor<float>(x[768])\", \n",
    "        indexing = [\"attribute\"]\n",
    "    )        \n",
    ")\n",
    "\n",
    "app_package.query_profile_type.add_fields(        \n",
    "    QueryTypeField(\n",
    "        name=\"ranking.features.query(title_bert)\",\n",
    "        type=\"tensor<float>(x[768])\"\n",
    "    )\n",
    ")\n",
    "\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(\n",
    "        name = \"bert_title\", \n",
    "        first_phase = \"sum(query(title_bert)*attribute(title_bert))\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover the step-by-step of the code block in the next sections. The goal of this post is to illustrate the pyvespa API that will enable you to create your own application rather than focusing on how to create an effective application or chosing the best transformers model to use for embedding generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pyvespa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use version 0.2.0 here, which can be installed via pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install pyvespa==0.2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the application "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create an application package with the [ApplicationPackage](https://pyvespa.readthedocs.io/en/latest/reference-api.html?highlight=ApplicationPackage#vespa.package.ApplicationPackage) class. We will name our application `msmarco` because we want to build a full document ranking application based on the MS MARCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage\n",
    "\n",
    "app_package = ApplicationPackage(name = \"msmarco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then add three fields to our application. Each document will have an unique `id`, a `title` and a 768 dimensional vector named `title_bert` that will store the title embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import Field\n",
    "\n",
    "app_package.schema.add_fields(        \n",
    "    Field(\n",
    "        name = \"id\", type = \"string\", \n",
    "        indexing = [\"attribute\", \"summary\"]\n",
    "    ),\n",
    "    Field(\n",
    "        name = \"title\", type = \"string\", \n",
    "        indexing = [\"index\", \"summary\"], \n",
    "        index = \"enable-bm25\"\n",
    "    ),\n",
    "    Field(\n",
    "        name = \"title_bert\", type = \"tensor<float>(x[768])\", \n",
    "        indexing = [\"attribute\"]\n",
    "    )        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a query ranking feature responsible to hold the query embedding at query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import QueryTypeField\n",
    "\n",
    "app_package.query_profile_type.add_fields(        \n",
    "    QueryTypeField(\n",
    "        name=\"ranking.features.query(title_bert)\",\n",
    "        type=\"tensor<float>(x[768])\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a document and a query embedding in the application, we can use them in a `RankProfile` to rank the documents matched by the query. In this case we will define a dot-product between the query embedding `query(title_bert)` and the document embedding `attribute(title_bert)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import RankProfile\n",
    "\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(\n",
    "        name = \"bert_title\", \n",
    "        first_phase = \"sum(query(title_bert)*attribute(title_bert))\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for configuration server.\n",
      "Waiting for application status.\n",
      "Waiting for application status.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vespa.package import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker(port=8089)\n",
    "\n",
    "os.environ[\"WORK_DIR\"] = \"/Users/tmartins\"\n",
    "disk_folder = os.path.join(os.getenv(\"WORK_DIR\"), \"sample_application\")\n",
    "app = vespa_docker.deploy(\n",
    "    application_package = app_package,\n",
    "    disk_folder=disk_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading one of the many models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bert_model = SentenceTransformer(\"distilbert-base-nli-stsb-mean-tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that take a text as input and return a vector of floats as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalized_bert_encoder(text):\n",
    "    vector = bert_model.encode([text])[0].tolist()\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm > 0.0:\n",
    "        vector = vector / norm\n",
    "    return vector.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "docs = read_csv(\"https://thigm85.github.io/data/msmarco/docs_100.tsv\", sep = \"\\t\")\n",
    "docs = docs[docs['title'].str.strip().astype(bool)] # remove empty titles\n",
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D2185715</td>\n",
       "      <td>What Is an Appropriate Gift for a Bris</td>\n",
       "      <td>Hub Pages   Religion and Philosophy   Judaism...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2819479</td>\n",
       "      <td>lunge</td>\n",
       "      <td>1lungenoun   ˈlənj  Popularity  Bottom 40  of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                    title  \\\n",
       "0  D2185715  What Is an Appropriate Gift for a Bris    \n",
       "1  D2819479                                    lunge   \n",
       "\n",
       "                                                body  \n",
       "0   Hub Pages   Religion and Philosophy   Judaism...  \n",
       "1   1lungenoun   ˈlənj  Popularity  Bottom 40  of...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in docs.iterrows():\n",
    "    response = app.feed_data_point(\n",
    "        schema = \"msmarco\",\n",
    "        data_id = row[\"id\"],\n",
    "        fields = {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"title\": row[\"title\"],\n",
    "            \"title_bert\": {\"values\": normalized_bert_encoder(row[\"title\"])}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a query model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import QueryModel, QueryRankingFeature, Union, WeakAnd, ANN, RankProfile\n",
    "\n",
    "query_model = QueryModel(\n",
    "    query_properties=[QueryRankingFeature(name=\"title_bert\", mapping=normalized_bert_encoder)],\n",
    "    match_phase=Union(\n",
    "        WeakAnd(field=\"title\", hits=10), \n",
    "        ANN(\n",
    "            doc_vector=\"title_bert\", \n",
    "            query_vector=\"title_bert\", \n",
    "            hits=10, \n",
    "            label=\"ann_title\"\n",
    "        )\n",
    "    ),\n",
    "    rank_profile=RankProfile(name=\"bert_title\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can query our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = app.query(query=\"What is science?\", query_model=query_model, debug_request=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'id:msmarco:msmarco::D2089371',\n",
       " 'relevance': 0.6983165740966797,\n",
       " 'source': 'msmarco_content',\n",
       " 'fields': {'sddocname': 'msmarco',\n",
       "  'documentid': 'id:msmarco:msmarco::D2089371',\n",
       "  'id': 'D2089371',\n",
       "  'title': 'Inquiry Science'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results.hits[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
