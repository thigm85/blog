{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a basic text search application from python with Vespa: part 2\n",
    "\n",
    "> Evaluate search engine experiments using python.\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [vespa, pyvespa, cord19, evaluation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to enable Vespa users to run their experiments from python. This tutorial illustrates how to define query models and evaluation metrics to perform search engine experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show how to use the [pyvespa](https://pyvespa.readthedocs.io/en/latest/index.html) API to run search engine experiments based on the text search app we built in [the first part](https://blog.vespa.ai/build-basic-text-search-app-from-python-with-vespa/) of this tutorial series. Specifically, we compare two different matching operators and show how to reduce the number of documents matched by the queries while keeping similar recall and precision metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that you have followed the first tutorial and have a variable `app` holding the `Vespa` connection instance that we established there. This connection should be pointing to a Docker container named `cord19` running the Vespa application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed additional data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to use the [CORD19 sample data](https://ir.nist.gov/covidSubmit/data.html) that fed the search app in the first tutorial. In addition, we are going to feed a few additional data points to make it possible to get relevant metrics from our experiments. We tried to minimize the amount of data required to make this tutorial easy to reproduce. You can download the additional 494 data points below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f047dvb4</td>\n",
       "      <td>Retrouver les origines du SARS-CoV-2 dans les ...</td>\n",
       "      <td>SARS-CoV-2 is a new human coronavirus (CoV), w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>db8j8wxx</td>\n",
       "      <td>SARS-CoV-2: Zoonotic origin of pandemic corona...</td>\n",
       "      <td>A novel disease, of unknown origin, causing a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is8tlxja</td>\n",
       "      <td>SARS-CoV-2: Zoonotic origin of pandemic corona...</td>\n",
       "      <td>A novel disease, of unknown origin, causing a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7nmmgi3v</td>\n",
       "      <td>Framing the Origins of COVID-19</td>\n",
       "      <td>Conspiracy theories have flourished about the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bc8taaw9</td>\n",
       "      <td>[Tracing the origins of SARS-COV-2 in coronavi...</td>\n",
       "      <td>SARS-CoV-2 is a new human coronavirus (CoV), w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                              title  \\\n",
       "0  f047dvb4  Retrouver les origines du SARS-CoV-2 dans les ...   \n",
       "1  db8j8wxx  SARS-CoV-2: Zoonotic origin of pandemic corona...   \n",
       "2  is8tlxja  SARS-CoV-2: Zoonotic origin of pandemic corona...   \n",
       "3  7nmmgi3v                    Framing the Origins of COVID-19   \n",
       "4  bc8taaw9  [Tracing the origins of SARS-COV-2 in coronavi...   \n",
       "\n",
       "                                            abstract  \n",
       "0  SARS-CoV-2 is a new human coronavirus (CoV), w...  \n",
       "1  A novel disease, of unknown origin, causing a ...  \n",
       "2  A novel disease, of unknown origin, causing a ...  \n",
       "3  Conspiracy theories have flourished about the ...  \n",
       "4  SARS-CoV-2 is a new human coronavirus (CoV), w...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "parsed_feed = read_csv(\"https://data.vespa.oath.cloud/blog/cord19/parsed_feed_additional.csv\")\n",
    "parsed_feed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then feed the data we just downloaded to the `app` via the `feed_data_point` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in parsed_feed.iterrows():\n",
    "    fields = {\n",
    "        \"cord_uid\": str(row[\"cord_uid\"]),\n",
    "        \"title\": str(row[\"title\"]),\n",
    "        \"abstract\": str(row[\"abstract\"])\n",
    "    }\n",
    "    response = app.feed_data_point(\n",
    "        schema = \"cord19\",\n",
    "        data_id = str(row[\"cord_uid\"]),\n",
    "        fields = fields,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define query models to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `QueryModel` is an abstraction that encapsulates all the relevant information controlling how your app matches and ranks documents. Since we are dealing with a simple text search app here, we will start by creating two query models that use BM25 to rank but differ on how they match documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import QueryModel, OR, WeakAnd, RankProfile as Ranking\n",
    "\n",
    "or_bm25 = QueryModel(\n",
    "    name=\"or_bm25\",\n",
    "    match_phase=OR(), \n",
    "    rank_profile=Ranking(name=\"bm25\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model is named `or_bm25` and will match all the documents that share at least one token with the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.query import WeakAnd\n",
    "\n",
    "wand_bm25 = QueryModel(\n",
    "    name=\"wand_bm25\", \n",
    "    match_phase=WeakAnd(hits=10), \n",
    "    rank_profile=Ranking(name=\"bm25\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model is named `wand_bm25` and uses the `WeakAnd` operator, considered an [accelerated OR operator](https://docs.vespa.ai/en/using-wand-with-vespa.html). The next section shows that the `WeakAnd` operator matches fewer documents without affecting the recall and precision metrics for the case considered here. We also analyze the optimal `hits` parameter to use for our specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define which metrics we want to compute when running our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.evaluation import MatchRatio, Recall, NormalizedDiscountedCumulativeGain\n",
    "\n",
    "eval_metrics = [\n",
    "    MatchRatio(), \n",
    "    Recall(at=10), \n",
    "    NormalizedDiscountedCumulativeGain(at=10)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MatchRatio` computes the fraction of the document corpus matched by the queries. This metric will be critical when comparing match phase operators such as the `OR` and the `WeakAnd`. In addition, we compute Recall and NDCG metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download labeled data to perform our experiments and compare query models. In our sample data, we have 50 queries, and each has a relevant document associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query_id': 1,\n",
       "  'relevant_docs': [{'id': 'kqqantwg', 'score': 2}],\n",
       "  'query': 'coronavirus origin'},\n",
       " {'query_id': 2,\n",
       "  'relevant_docs': [{'id': '526elsrf', 'score': 2}],\n",
       "  'query': 'coronavirus response to weather changes'},\n",
       " {'query_id': 3,\n",
       "  'relevant_docs': [{'id': '5jl6ltfj', 'score': 1}],\n",
       "  'query': 'coronavirus immunity'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, requests\n",
    "\n",
    "labeled_data = json.loads(\n",
    "    requests.get(\"https://data.vespa.oath.cloud/blog/cord19/labeled_data.json\").text\n",
    ")\n",
    "labeled_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have labeled data, the evaluation metrics to compute, and the query models we want to compare, we can run experiments with the `evaluate` method. The `cord_uid` field of the Vespa application should match the `id` of the relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>or_bm25</th>\n",
       "      <th>wand_bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">match_ratio</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.672670</td>\n",
       "      <td>0.211068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.597473</td>\n",
       "      <td>0.138989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.168444</td>\n",
       "      <td>0.160804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">recall_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.369989</td>\n",
       "      <td>0.369989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ndcg_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.407856</td>\n",
       "      <td>0.408342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.298673</td>\n",
       "      <td>0.298451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                or_bm25  wand_bm25\n",
       "match_ratio mean    0.672670   0.211068\n",
       "            median  0.597473   0.138989\n",
       "            std     0.168444   0.160804\n",
       "recall_10   mean    0.840909   0.840909\n",
       "            median  1.000000   1.000000\n",
       "            std     0.369989   0.369989\n",
       "ndcg_10     mean    0.407856   0.408342\n",
       "            median  0.333333   0.333333\n",
       "            std     0.298673   0.298451"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = app.evaluate(\n",
    "    labeled_data=labeled_data, \n",
    "    query_model=[or_bm25, wand_bm25], \n",
    "    eval_metrics=eval_metrics, \n",
    "    id_field=\"cord_uid\",\n",
    ")\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that, on average, we match 67% of our document corpus when using the `OR` operator and 21% when using the `WeakAnd` operator. The reduction in matched documents did not affect the recall and the NDCG metrics, which stayed at around 0.84 and 0.40, respectively. The Match Ratio will get even better when we experiment with the `hits` parameter of the `WeakAnd` further down in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different options available to configure the output of the `evaluate` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate` method returns the mean, the median, and the standard deviation of the metrics by default. We can customize this by specifying the desired `aggregators`. Below we choose the mean, the max, and the min as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>or_bm25</th>\n",
       "      <th>wand_bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">match_ratio</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.672670</td>\n",
       "      <td>0.211068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.362816</td>\n",
       "      <td>0.032491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.983755</td>\n",
       "      <td>0.738267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">recall_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ndcg_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.407856</td>\n",
       "      <td>0.408342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model              or_bm25  wand_bm25\n",
       "match_ratio mean  0.672670   0.211068\n",
       "            min   0.362816   0.032491\n",
       "            max   0.983755   0.738267\n",
       "recall_10   mean  0.840909   0.840909\n",
       "            min   0.000000   0.000000\n",
       "            max   1.000000   1.000000\n",
       "ndcg_10     mean  0.407856   0.408342\n",
       "            min   0.000000   0.000000\n",
       "            max   1.000000   1.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = app.evaluate(\n",
    "    labeled_data=labeled_data, \n",
    "    query_model=[or_bm25, wand_bm25], \n",
    "    eval_metrics=eval_metrics, \n",
    "    id_field=\"cord_uid\",\n",
    "    aggregators=[\"mean\", \"min\", \"max\"]\n",
    ")\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check detailed metrics output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the metrics have intermediate results that might be of interest. For example, the `MatchRatio` metric requires us to compute the number of matched documents (retrieved_docs) and the number of documents available to be retrieved (docs_available). We can output those intermediate steps by setting `detailed_metrics=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>or_bm25</th>\n",
       "      <th>wand_bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>match_ratio</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.672670</td>\n",
       "      <td>0.211068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_ratio_retrieved_docs</th>\n",
       "      <th>mean</th>\n",
       "      <td>372.659091</td>\n",
       "      <td>116.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_ratio_docs_available</th>\n",
       "      <th>mean</th>\n",
       "      <td>554.000000</td>\n",
       "      <td>554.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndcg_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.407856</td>\n",
       "      <td>0.408342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndcg_10_ideal_dcg</th>\n",
       "      <th>mean</th>\n",
       "      <td>1.704545</td>\n",
       "      <td>1.704545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndcg_10_dcg</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.707666</td>\n",
       "      <td>0.707632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                               or_bm25   wand_bm25\n",
       "match_ratio                mean    0.672670    0.211068\n",
       "match_ratio_retrieved_docs mean  372.659091  116.931818\n",
       "match_ratio_docs_available mean  554.000000  554.000000\n",
       "recall_10                  mean    0.840909    0.840909\n",
       "ndcg_10                    mean    0.407856    0.408342\n",
       "ndcg_10_ideal_dcg          mean    1.704545    1.704545\n",
       "ndcg_10_dcg                mean    0.707666    0.707632"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = app.evaluate(\n",
    "    labeled_data=labeled_data, \n",
    "    query_model=[or_bm25, wand_bm25], \n",
    "    eval_metrics=eval_metrics, \n",
    "    id_field=\"cord_uid\",\n",
    "    aggregators=[\"mean\"],\n",
    "    detailed_metrics=True\n",
    ")\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get per-query results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging the results, it is often helpful to look at the metrics on a per-query basis, which is available by setting `per_query=True.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>query_id</th>\n",
       "      <th>match_ratio</th>\n",
       "      <th>recall_10</th>\n",
       "      <th>ndcg_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>or_bm25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.557762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.315465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wand_bm25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.315465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or_bm25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.945848</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wand_bm25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.126354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>or_bm25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.592058</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  query_id  match_ratio  recall_10   ndcg_10\n",
       "0    or_bm25         1     0.557762        1.0  0.315465\n",
       "1  wand_bm25         1     0.032491        1.0  0.315465\n",
       "2    or_bm25         2     0.945848        1.0  0.430677\n",
       "3  wand_bm25         2     0.126354        1.0  0.430677\n",
       "4    or_bm25         3     0.592058        1.0  0.430677"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = app.evaluate(\n",
    "    labeled_data=labeled_data, \n",
    "    query_model=[or_bm25, wand_bm25], \n",
    "    eval_metrics=eval_metrics, \n",
    "    id_field=\"cord_uid\",\n",
    "    per_query=True\n",
    ")\n",
    "evaluation.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal WeakAnd parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same evaluation framework to find the optimal `hits` parameter of the `WeakAnd` operator for this specific application. To do that, we can define a list of query models that only differ by the `hits` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_models = [QueryModel(\n",
    "    name=\"wand_{}_bm25\".format(hits), \n",
    "    match_phase=WeakAnd(hits=hits), \n",
    "    rank_profile=Ranking(name=\"bm25\")\n",
    ") for hits in range(1, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then call `evaluate` as before and show the match ratio and recall for each of the options defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>wand_1_bm25</th>\n",
       "      <th>wand_2_bm25</th>\n",
       "      <th>wand_3_bm25</th>\n",
       "      <th>wand_4_bm25</th>\n",
       "      <th>wand_5_bm25</th>\n",
       "      <th>wand_6_bm25</th>\n",
       "      <th>wand_7_bm25</th>\n",
       "      <th>wand_8_bm25</th>\n",
       "      <th>wand_9_bm25</th>\n",
       "      <th>wand_10_bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>match_ratio</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.088530</td>\n",
       "      <td>0.109739</td>\n",
       "      <td>0.130374</td>\n",
       "      <td>0.142722</td>\n",
       "      <td>0.151378</td>\n",
       "      <td>0.165778</td>\n",
       "      <td>0.177018</td>\n",
       "      <td>0.186208</td>\n",
       "      <td>0.190556</td>\n",
       "      <td>0.211068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model             wand_1_bm25  wand_2_bm25  wand_3_bm25  wand_4_bm25  \\\n",
       "match_ratio mean     0.088530     0.109739     0.130374     0.142722   \n",
       "recall_10   mean     0.727273     0.750000     0.795455     0.840909   \n",
       "\n",
       "model             wand_5_bm25  wand_6_bm25  wand_7_bm25  wand_8_bm25  \\\n",
       "match_ratio mean     0.151378     0.165778     0.177018     0.186208   \n",
       "recall_10   mean     0.840909     0.840909     0.840909     0.840909   \n",
       "\n",
       "model             wand_9_bm25  wand_10_bm25  \n",
       "match_ratio mean     0.190556      0.211068  \n",
       "recall_10   mean     0.840909      0.840909  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = app.evaluate(\n",
    "    labeled_data=labeled_data, \n",
    "    query_model=wand_models, \n",
    "    eval_metrics=eval_metrics, \n",
    "    id_field=\"cord_uid\",\n",
    "    aggregators=[\"mean\"],\n",
    ")\n",
    "evaluation.loc[[\"match_ratio\", \"recall_10\"], [\"wand_{}_bm25\".format(hits) for hits in range(1, 11)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we can see that a higher `hits` parameter implies a higher match ratio. But the recall metric remains the same as long as we pick `hits > 3`. So, using `WeakAnd` with `hits = 4` is enough for this specific application and dataset, leading to a further reduction in the number of documents matched on average by our queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to enable Vespa users to run their experiments from python. This tutorial illustrates how to define query models and evaluation metrics to run search engine experiments via the evaluate method. We used a simple example that compares two different match operators and another that optimizes the parameter of one of those operators. Our key finding is that we can reduce the size of the retrieved set of hits without losing recall and precision by using the `WeakAnd` instead of the `OR` match operator.\n",
    "\n",
    "The following Vespa resources are related to the topics explored by the experiments presented here:\n",
    "\n",
    "* [Getting started with retrieval and ranking](https://docs.vespa.ai/en/getting-started-ranking.html#retrieval-and-ranking)\n",
    "\n",
    "* [Phased Ranking](https://docs.vespa.ai/en/phased-ranking.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
