{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Probabilistic Relevance Framework:\n",
    "\n",
    "> TBD\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [search, term-based matching, bm25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assumptions behind the Probabilistic Relevance Framework\n",
    "    - Relevance is a property of a individual document\n",
    "    - Relevance is binary, a document is either relevant or not.\n",
    "\n",
    "# Basic model\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. conditional independence between documents term given relevance info\n",
    "    - debatable but shown to be robust in practice\n",
    "2. Only query terms are relevant to rank the documents\n",
    "    - Reasonable assumption in the absence of other information\n",
    "\n",
    "## Model\n",
    "\n",
    "$$P(rel|d, q) \\approx  \\sum_q U_i(tf_i)\\\\ \\propto_q \\sum_{q, tf_i>0} w_i$$\n",
    "\n",
    "$$U_i(x) = \\log \\frac{P(TF_i=x|rel)}{P(TF_i=x|\\sim rel)}, \\\\ w_i = U_i(tf_i) - U_i(0) = \\log \\frac{P(TF_i=x|rel)P(TF_i=0|\\sim rel )}{P(TF_i=x|\\sim rel )P(TF_i=0|rel)}$$\n",
    "\n",
    "## Comments\n",
    "\n",
    "- The advantage of using $w_i$ instead of U_i is that we only need to compute the score for documents that contain at least one query term.\n",
    "- The model is not restricted to terms and term-frequencies, any attribute of the document of query-document pair can be included.\n",
    "    - Any discrete property with a natural zero can be dealt with using the W_i form of the weight.\n",
    "    - If we want to include a property without such a natural zero, we need to revert to the U_i form.\n",
    "- The approximations above work if we are only interested in the ranking problem. They do not provide accurate probabilities due to the approximations and transformations performed to arrive at the simplified equation.\n",
    "\n",
    "# The Binary Independence Model\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. TF_i is a binary variable. That is the term is either present or absent.\n",
    "    - Define t_i as the event that the term is present in the document\n",
    "\n",
    "## Model\n",
    "\n",
    "- Under the binary assumption w_i becomes\n",
    "\n",
    "    $$w_i^{BIM} = \\log \\frac{P(t_i|rel)(1-P(t_i|\\sim rel))}{(1-P(t_i|rel))P(t_i|\\sim rel)}$$\n",
    "\n",
    "- We can estimate the probabilities below using the following quantities:\n",
    "    - N: Size of the whole collection\n",
    "    - n_i: Number of documents in the collection containing t_i\n",
    "    - R: Relevant set size (i.e., number of documents judged relevant)\n",
    "    - r_i: Number of judged relevant docs containing t_i\n",
    "\n",
    "    $$P(t_i|rel)=r_i/R, \\quad P(t_i|\\sim rel)=(n_i - r_i)/(N-R)$$\n",
    "\n",
    "- Using the estimates above and a small 0.5 correction for robustness, we have\n",
    "\n",
    "    $$w_i^{BIM} = \\log \\frac{(r_i + 0.5)(N-R-n_i+r_i+0.5)}{(n_i-r_i+0.5)(R-r_i+0.5)}$$\n",
    "\n",
    "    - The resulting formula is the well-known Robertson/Sprck Jones weight, also denoted as w_i^{RSJ}\n",
    "\n",
    "## Absence of relevant information\n",
    "\n",
    "- If we assume that the entire collection is non-relevant, we can use r_i = R = 0 in the formula above leading to a close approximation to the classical idf\n",
    "\n",
    "    $$w_i^{IDF} = \\log \\frac{N - n_i+0.5}{n_i+0.5}$$\n",
    "\n",
    "## Comments\n",
    "\n",
    "- We assumed above that non-judged document is non-relevant\n",
    "- The absence of relevant information assumption is equivalent of saying that P(t_i|rel) = 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
