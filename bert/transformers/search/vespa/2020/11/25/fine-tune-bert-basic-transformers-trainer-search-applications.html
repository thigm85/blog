<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fine-tuning a BERT model for search applications | Thiago G. Martins</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Fine-tuning a BERT model for search applications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to ensure training and serving encoding compatibility" />
<meta property="og:description" content="How to ensure training and serving encoding compatibility" />
<link rel="canonical" href="https://thigm85.github.io/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html" />
<meta property="og:url" content="https://thigm85.github.io/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html" />
<meta property="og:site_name" content="Thiago G. Martins" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-25T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2020-11-25T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://thigm85.github.io/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html"},"description":"How to ensure training and serving encoding compatibility","@type":"BlogPosting","url":"https://thigm85.github.io/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html","headline":"Fine-tuning a BERT model for search applications","dateModified":"2020-11-25T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://thigm85.github.io/blog/feed.xml" title="Thiago G. Martins" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-28943273-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fine-tuning a BERT model for search applications | Thiago G. Martins</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Fine-tuning a BERT model for search applications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to ensure training and serving encoding compatibility" />
<meta property="og:description" content="How to ensure training and serving encoding compatibility" />
<link rel="canonical" href="https://thigm85.github.io/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html" />
<meta property="og:url" content="https://thigm85.github.io/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html" />
<meta property="og:site_name" content="Thiago G. Martins" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-25T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2020-11-25T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://thigm85.github.io/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html"},"description":"How to ensure training and serving encoding compatibility","@type":"BlogPosting","url":"https://thigm85.github.io/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html","headline":"Fine-tuning a BERT model for search applications","dateModified":"2020-11-25T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://thigm85.github.io/blog/feed.xml" title="Thiago G. Martins" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-28943273-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
<script type="text/javascript">
require.config({
  paths: {
    jquery: 'https://code.jquery.com/jquery-3.5.0.min',
    plotly: 'https://cdn.plot.ly/plotly-latest.min'
  },

  shim: {
    plotly: {
      deps: ['jquery'],
      exports: 'plotly'
    }
  }
});
</script>

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Thiago G. Martins</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Fine-tuning a BERT model for search applications</h1><p class="page-description">How to ensure training and serving encoding compatibility</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-11-25T00:00:00-06:00" itemprop="datePublished">
        Nov 25, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#BERT">BERT</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#transformers">transformers</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#search">search</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Vespa">Vespa</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/thigm85/blog/tree/master/_notebooks/2020-11-25-fine-tune-bert-basic-transformers-trainer-search-applications.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/thigm85/blog/blob/master/_notebooks/2020-11-25-fine-tune-bert-basic-transformers-trainer-search-applications.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#The-search-use-case">The search use case </a></li>
<li class="toc-entry toc-h2"><a href="#Training-and-serving-compatibility">Training and serving compatibility </a></li>
<li class="toc-entry toc-h2"><a href="#Create-independent-BERT-encodings">Create independent BERT encodings </a></li>
<li class="toc-entry toc-h2"><a href="#Conclusion-and-future-work">Conclusion and future work </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-11-25-fine-tune-bert-basic-transformers-trainer-search-applications.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are cases where the inputs to your Transformer model are pairs of sentences, but you want to process each sentence of the pair at different times due to your application’s nature.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-search-use-case">
<a class="anchor" href="#The-search-use-case" aria-hidden="true"><span class="octicon octicon-link"></span></a>The search use case<a class="anchor-link" href="#The-search-use-case"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Search applications are one example. They involve a large collection of documents that can be pre-processed and stored before a search action is required. On the other hand, a query triggers a search action, and we can only process it in real-time. Search apps’ goal is to return the most relevant documents to the query as quickly as possible. By applying the tokenizer to the documents as soon as we feed them to the application, we only need to tokenize the query when a search action is required, saving us precious time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In addition to applying the tokenizer at different times, you also want to retain adequate control about encoding your pair of sentences. For search, you might want to have a joint input vector of length 128 where the query, which is usually smaller than the document, contributes with 32 tokens while the document can take up to 96 tokens.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-and-serving-compatibility">
<a class="anchor" href="#Training-and-serving-compatibility" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training and serving compatibility<a class="anchor-link" href="#Training-and-serving-compatibility"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When training a Transformer model for search, you want to ensure that the training data will follow the same pattern used by the search engine serving the final model. I have written <a href="https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b">a blog post</a> on how to get started with BERT model fine-tuning using the <code>transformer</code> library. This piece will adapt the training routine with a custom encoding based on two separate tokenizers to reproduce how a <a href="https://vespa.ai/">Vespa</a> application would serve the model once deployed.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Create-independent-BERT-encodings">
<a class="anchor" href="#Create-independent-BERT-encodings" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create independent BERT encodings<a class="anchor-link" href="#Create-independent-BERT-encodings"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The only change required is simple but essential. In my <a href="https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b">previous post</a>, we discussed the vanilla case where we simply applied the <code>tokenizer</code> directly to the pairs of queries and documents.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizerFast</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"google/bert_uncased_L-4_H-512_A-8"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">train_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_queries</span><span class="p">,</span> <span class="n">train_docs</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'max_length'</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">val_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">val_queries</span><span class="p">,</span> <span class="n">val_docs</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'max_length'</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the search case, we create the <code>create_bert_encodings</code> function that will apply two different tokenizers, one for the query and the other for the document. In addition to allowing for different query and document <code>max_length</code>, we also need to set <code>add_special_tokens=False</code> and not use <code>padding</code> as those need to be included by our custom code when joining the tokens generated by the <code>tokenizer</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_bert_encodings</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">query_input_size</span><span class="p">,</span> <span class="n">doc_input_size</span><span class="p">):</span>
    <span class="n">queries_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">queries</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">query_input_size</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">docs_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">docs</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">doc_input_size</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    
    <span class="n">TOKEN_NONE</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">TOKEN_CLS</span><span class="o">=</span><span class="mi">101</span>
    <span class="n">TOKEN_SEP</span><span class="o">=</span><span class="mi">102</span>

    <span class="n">input_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">query_input_ids</span><span class="p">,</span> <span class="n">doc_input_ids</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">queries_encodings</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">docs_encodings</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]):</span>
        <span class="c1"># create input id</span>
        <span class="n">input_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">TOKEN_CLS</span><span class="p">]</span> <span class="o">+</span> <span class="n">query_input_ids</span> <span class="o">+</span> <span class="p">[</span><span class="n">TOKEN_SEP</span><span class="p">]</span> <span class="o">+</span> <span class="n">doc_input_ids</span> <span class="o">+</span> <span class="p">[</span><span class="n">TOKEN_SEP</span><span class="p">]</span>
        <span class="n">number_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span>
        <span class="n">padding_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">128</span> <span class="o">-</span> <span class="n">number_tokens</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">input_id</span> <span class="o">=</span> <span class="n">input_id</span> <span class="o">+</span> <span class="p">[</span><span class="n">TOKEN_NONE</span><span class="p">]</span> <span class="o">*</span> <span class="n">padding_length</span>
        <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span>
        <span class="c1"># create token id</span>
        <span class="n">token_type_id</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">([</span><span class="n">TOKEN_CLS</span><span class="p">]</span> <span class="o">+</span> <span class="n">query_input_ids</span> <span class="o">+</span> <span class="p">[</span><span class="n">TOKEN_SEP</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc_input_ids</span> <span class="o">+</span> <span class="p">[</span><span class="n">TOKEN_SEP</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="n">TOKEN_NONE</span><span class="p">]</span> <span class="o">*</span> <span class="n">padding_length</span>
        <span class="n">token_type_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_type_id</span><span class="p">)</span>
        <span class="c1"># create attention_mask</span>
        <span class="n">attention_mask</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">number_tokens</span> <span class="o">+</span> <span class="p">[</span><span class="n">TOKEN_NONE</span><span class="p">]</span> <span class="o">*</span> <span class="n">padding_length</span><span class="p">)</span>

    <span class="n">encodings</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
        <span class="s2">"token_type_ids"</span><span class="p">:</span> <span class="n">token_type_ids</span><span class="p">,</span>
        <span class="s2">"attention_mask"</span><span class="p">:</span> <span class="n">attention_mask</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">encodings</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We then create the <code>train_encodings</code> and <code>val_encodings</code> required by the <a href="https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b">training routine</a>. Everything else on the training routine works just the same.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizerFast</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"google/bert_uncased_L-4_H-512_A-8"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">train_encodings</span> <span class="o">=</span> <span class="n">create_bert_encodings</span><span class="p">(</span>
    <span class="n">queries</span><span class="o">=</span><span class="n">train_queries</span><span class="p">,</span> 
    <span class="n">docs</span><span class="o">=</span><span class="n">train_docs</span><span class="p">,</span> 
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> 
    <span class="n">query_input_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> 
    <span class="n">doc_input_size</span><span class="o">=</span><span class="mi">96</span>
<span class="p">)</span>

<span class="n">val_encodings</span> <span class="o">=</span> <span class="n">create_bert_encodings</span><span class="p">(</span>
    <span class="n">queries</span><span class="o">=</span><span class="n">val_queries</span><span class="p">,</span> 
    <span class="n">docs</span><span class="o">=</span><span class="n">val_docs</span><span class="p">,</span> 
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> 
    <span class="n">query_input_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> 
    <span class="n">doc_input_size</span><span class="o">=</span><span class="mi">96</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion-and-future-work">
<a class="anchor" href="#Conclusion-and-future-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion and future work<a class="anchor-link" href="#Conclusion-and-future-work"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Training a model to deploy in a search application require us to ensure that the training encodings are compatible with encodings used at serving time. We generate document encodings offline when feeding the documents to the search engine while creating query encoding at run-time upon arrival of the query. It is often relevant to use different maximum lengths for queries and documents, and other possible configurations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We showed how to customize BERT model encodings to ensure this training and serving compatibility. However, a better approach is to build tools that bridge the gap between training and serving by allowing users to request training data that respects by default the encodings used when serving the model. <a href="https://pyvespa.readthedocs.io/en/latest/index.html">pyvespa</a> will include such integration to make it easier for <a href="https://vespa.ai/">Vespa</a> users to train BERT models without having to adjust the encoding generation manually as we did above.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="thigm85/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/bert/transformers/search/vespa/2020/11/25/fine-tune-bert-basic-transformers-trainer-search-applications.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Personal blog.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/thigm85" title="thigm85"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Thiagogm" title="Thiagogm"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
